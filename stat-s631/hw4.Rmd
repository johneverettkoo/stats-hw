---
title: "STAT-S631"
subtitle: 'Assignment 4'
author: "John Koo"
output: pdf_document
# output: html_document
header-includes:
- \usepackage{float}
---
  
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      comment = NA, 
                      fig.pos = 'H', 
                      fig.align = 'center', 
                      dpi = 300, 
                      fig.height = 3, 
                      fig.width = 5, 
                      fig.retina = 1, 
                      cache = FALSE)
options(xtable.comment = FALSE, 
        xtable.table.placement = 'H')
```

```{r packages}
dp <- loadNamespace('dplyr')
import::from(magrittr, `%>%`, `%<>%`)
library(ggplot2)
theme_set(theme_bw())
```
# Question 1

[From ALR 2.16]

```{r q1_setup}
un.df <- alr4::UN11 %>% 
  dp$mutate(country = rownames(.))
```

## Part 1

```{r q1_p1}
fertility.mod <- lm(log(fertility) ~ log(ppgdp), data = un.df)

summary(fertility.mod)
```

## Part 2

```{r q1_p2}
fert.vs.gdp.plot <- un.df %>% 
  dp$mutate(fert.hat = 
              exp(fertility.mod$coefficients['(Intercept)']) * 
              ppgdp ** fertility.mod$coefficients['log(ppgdp)']) %>% 
  ggplot() + 
  geom_point(aes(x = ppgdp, y = fertility)) + 
  geom_line(aes(x = ppgdp, y = fert.hat), colour = 'red') + 
  labs(x = 'GDP per capita [USD]', y = 'children per woman')

fert.vs.gdp.plot
fert.vs.gdp.plot + scale_x_log10() + scale_y_log10()
```

## Part 3

Using $\alpha = 0.05$:

```{r q1_p3}
# alternatively, we can just look at summary(fertility.mod)

b1.mean <- fertility.mod$coefficients['log(ppgdp)']
b1.sterr <- summary(fertility.mod)$coefficients['log(ppgdp)', 'Std. Error']
b1.tval <- unname(-b1.mean / b1.sterr)
pt(-abs(b1.tval), df = fertility.mod$df.residual)
```

Here our $p$-value is much smaller than $\alpha$.

## Part 4

```{r q1_p4}
summary(fertility.mod)$r.squared
```

The $R^2$ value can be interpreted as the proportion of variance that has been 
explained by the model. In this case, about half of the variance of $Y$ is 
accounted for by the linear fit with $X$. 

## Part 5

```{r q1_p5}
cross.prod.sum <- function(x, y = NULL) {
  # sum of cross product (e.g., SXX, SYY, SXY)
  
  if (is.null(y)) y <- x
  sum((x - mean(x)) * (y - mean(y)))
}

pred.y <- function(x, model, alpha = .05) {
  # predicts new y given a simple linear model and input
  # also gives confidence interval for a certain alpha value
  
  # compute t for the alpha C.I. and deg of freedom
  t.alpha <- qt(1 - alpha / 2, model$df.residual)
  
  # compute expected
  y.mean <- unname(model$coefficients[1] + model$coefficients[2] * x)
  
  # compute standard error
  x.mean <- mean(model$model[, 2])
  sxx <- cross.prod.sum(model$model[, 2])
  y.se <- 
    sigma(model) * 
    sqrt(1 + 1 / nrow(model$model) + (x - x.mean) ** 2 / sxx)
  
  return(list(y = y.mean, 
              ci = c(y.mean - y.se * t.alpha, y.mean + y.se * t.alpha)))
}

new.y <- pred.y(log(1000), fertility.mod)
new.y
lapply(new.y, exp)
```

So the 95\% prediction interval for $Y$ given $\log{X} = 1000$ is 
[`r round(exp(new.y$ci), 3)`]

## Part 6

```{r q1_p6, fig.height = 13, fig.width = 8}
# lowest fertility
dp$filter(un.df, fertility == min(fertility))$country

# highest fertility
dp$filter(un.df, fertility == max(fertility))$country

# most positive residuals
un.df$country[order(-fertility.mod$residuals)[1:2]]

# most negative residuals
un.df$country[order(fertility.mod$residuals)[1:2]]

ggplot(un.df) + 
  geom_point(aes(x = ppgdp, y = fertility)) + 
  ggrepel::geom_label_repel(aes(x = ppgdp, y = fertility, label = country)) +
  scale_x_log10() + scale_y_log10() + 
  stat_smooth(aes(x = ppgdp, y = fertility), method = 'lm', level = .99)
```

# Question 2

```{r q2_setup}
sahlins.df <- read.delim('~/dev/stats-hw/stat-s631/Sahlins.txt', sep = ' ')
```

## Part a

```{r q2_pa}
ggplot(sahlins.df) + 
  geom_point(aes(x = consumers, y = acres)) + 
  labs(x = 'consumers/gardener', y = 'acres/gardener')
```

From the scatterplot, the data do not appear to be particularly linear, although 
there appears to be a very slight positive correlation. We can compute this:

```{r q2_pa_2}
cor(sahlins.df$consumers, sahlins.df$acres)
```

Most of the data appear to be clustered in the center with a ring of points 
surrounding it. One household has an unusually high value for acres per 
gardener---it's almost 3 times its value for consumers per gardener (~3 vs ~1). 

## Part b

```{r q2_pb_1}
sahlins.mod <- lm(acres ~ consumers, data = sahlins.df)
summary(sahlins.mod)
```

The results indicate that, if we set the conventional value of $\alpha = .05$, 
we would fail to reject the null hypothesis that $\beta_1 \neq 0$, implying that 
there is no significant relationship between acres per gardener and consumers per 
gardener. However, using the same value of $\alpha$, we reject the null 
hypothesis that $\beta_0 = 0$, indicating that each household receives some 
amount regardless of productivity. 

The residual standard error $\hat{\sigma}$ is `r round(sigma(sahlins.mod), 4)`

If we remove the 4^th^ data point:

```{r q2_pb_2}
sahlins.mod.2 <- lm(acres ~ consumers, data = sahlins.df[-4, ])
summary(sahlins.mod.2)
```

We would reject the null hypothesis for both $\beta_0$ and $\beta_1$. This would 
imply that each household receives some base amount but also can work for 
additional resources. This is a different conclusion than before, when we used 
all the data.

```{r q2_pb_3}
ggplot() + 
  geom_point(data = sahlins.df[-4, ], 
             aes(x = consumers, y = acres, shape = 'not outlier')) + 
  labs(x = 'consumers/gardener', 
       y = 'acres/gardner', 
       colour = 'linear fit', fill = 'linear fit',  
       shape = NULL) + 
  geom_point(data = sahlins.df[4, ], 
             aes(x = consumers, y = acres, shape = 'outlier')) + 
  stat_smooth(data = sahlins.df, 
              aes(x = consumers, y = acres, 
                  colour = 'all data', fill = 'all data'), 
              method = 'lm') + 
  stat_smooth(data = sahlins.df[-4, ], 
              aes(x = consumers, y = acres, 
                  colour = 'outlier removed', fill = 'outlier removed'), 
              method = 'lm') + 
  scale_colour_brewer(palette = 'Set1') + 
  scale_fill_brewer(palette = 'Set1')
```

We can say that the first model is not a good fit since we failed to reject 
the null hypothesis that $\beta_1 \neq 0$. 

For the second model (with the 4^th^ data point removed), we can check the 
residuals:

```{r q2_pb_4}
sahlins.outlier.removed.df <- sahlins.df[-4, ] %>% 
  dp$mutate(resids = sahlins.mod.2$residuals)

ggplot(sahlins.outlier.removed.df) + 
  geom_histogram(aes(x = resids), 
                 fill = 'white', colour = 'black', bins = 16)

ggplot(sahlins.outlier.removed.df) + 
  geom_point(aes(x = consumers, 
                 y = resids)) + 
  geom_hline(yintercept = 0, colour = 'red') + 
  labs(x = 'consumers/gardener', y = 'residuals')
```

```{r q2_pb_5, fig.width = 3}
qqnorm(sahlins.outlier.removed.df$resids)
```

Here we can see that the residuals do not depend on the input variable and 
appear approximately normally distributed around 0, so the assumptions of the 
linear model hold. As for the strength of the predictions, 
$\hat{\sigma} \approx 0.3681$, so on average the model is ~0.4 acres/gardener 
off the true value. 

## Part c

### For the first model

```{r q2_pc_1}
summary(sahlins.mod)$coefficients
```

If we set $\alpha = .05$ and the null hypotheses as $\beta_0 = 0$ and 
$\beta_1 = 0$, then we would reject the $\beta_0 = 0$ and fail to reject 
$\beta_1 = 0$ (these are from the `summary` command from part (b)).

Using the same $\alpha$, we can compute the confidence intervals:

```{r q2_pc_2}
alpha <- .05
t.alpha <- qt(1 - alpha / 2, sahlins.mod$df.residual)

mod1.b0.ci <- 
  c(-t.alpha, t.alpha) * 
summary(sahlins.mod)$coefficients['(Intercept)', 'Std. Error'] + 
  sahlins.mod$coefficients['(Intercept)']

mod1.b1.ci <- 
  c(-t.alpha, t.alpha) * 
summary(sahlins.mod)$coefficients['consumers', 'Std. Error'] + 
  sahlins.mod$coefficients['consumers']

print(mod1.b0.ci)
print(mod1.b1.ci)
```

The C.I. for $\beta_0$ does not contain 0 while the C.I. for $\beta_1$ does, 
which is consistent with our hypothesis tests.

### For the second model

Using the same $\alpha$, we reject the null hypothesis for both $\beta_0 = 0$ 
and $\beta_1 = 0$ (from the `summary` command from part (b)). 

For the confidence intervals: 

```{r q2_pc_3}
alpha <- .05
t.alpha <- qt(1 - alpha / 2, sahlins.mod.2$df.residual)

mod2.b0.ci <- 
  c(-t.alpha, t.alpha) * 
summary(sahlins.mod.2)$coefficients['(Intercept)', 'Std. Error'] + 
  sahlins.mod.2$coefficients['(Intercept)']

mod2.b1.ci <- 
  c(-t.alpha, t.alpha) * 
summary(sahlins.mod.2)$coefficients['consumers', 'Std. Error'] + 
  sahlins.mod.2$coefficients['consumers']

print(mod2.b0.ci)
print(mod2.b1.ci)
```

The C.I.s for neither $\beta_0$ nor $\beta_1$ contain 0, which is consistent 
with the results of our hypothesis tests.

## Part d

For prediction (the first question):

```{r q2_pd_predict}
pred.y(1.5, sahlins.mod, alpha = .02)

# alternatively
predict(sahlins.mod, 
        newdata = data.frame(consumers = 1.5), 
        interval = 'predict', 
        level = .98)
```

For our estimate of the mean (the second question):

```{r q2_pd_confidence}
mean.y <- function(x, model, alpha = .05) {
  # predicts mean y given a simple linear model and input
  # also gives confidence interval for a certain alpha value
  
  # compute t for the alpha C.I. and deg of freedom
  t.alpha <- qt(1 - alpha / 2, model$df.residual)
  
  # compute expected
  y.mean <- unname(model$coefficients[1] + model$coefficients[2] * x)
  
  # compute standard error
  x.mean <- mean(model$model[, 2])
  sxx <- cross.prod.sum(model$model[, 2])
  y.se <- 
    sigma(model) * 
    sqrt(1 / nrow(model$model) + (x - x.mean) ** 2 / sxx)
  
  return(list(y = y.mean, 
              ci = c(y.mean - y.se * t.alpha, y.mean + y.se * t.alpha)))
}

mean.y(1.5, sahlins.mod, alpha = .02)

# alternatively
predict(sahlins.mod, 
        newdata = data.frame(consumers = 1.5), 
        interval = 'confidence', 
        level = .98)
```

The prediction interval (first value) is what we expect a new value to have 
given that $x = 1.5$. On the other hand, the confidence interval (second 
value) is what we expect the mean of all $y$ would be given that $x = 1.5$. 
The standard error for each case is different. 