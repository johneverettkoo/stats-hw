{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automating Probabilistic ML\n",
    "\n",
    "* probabilistic programming\n",
    "* specify a probabilistic model\n",
    "    * $\\to$ \"directy\" get the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preliminaries for black box variational inference\n",
    "\n",
    "* $\\nabla_\\lambda E_{q_\\lambda(z)}[f(z)] = E_{q_{\\lambda}(z)}[(\\nabla_\\lambda \\log q_\\lambda(z)) f(z)]$\n",
    "    * *proof*  \n",
    "    $\\nabla_\\lambda E_{q_\\lambda(z)}[f(z)] = \\nabla_\\lambda \\int q_\\lambda(z) f(z) dz$  \n",
    "    $= \\int (\\nabla_\\lambda q_\\lambda(z)) f(z) dz$  \n",
    "    $= \\int q_\\lambda(z) (\\nabla_\\lambda \\log q_\\lambda (z)) f(z) dz$\n",
    "* $E_{q_\\lambda(z)} [\\nabla_\\lambda \\log q_\\lambda (z)] = 0$\n",
    "    * *proof*  \n",
    "    the above expression $= \\int q_\\lambda (z) (q_\\lambda(z))^{-1} \\nabla q_\\lambda (z) dz$  \n",
    "    $= \\nabla_\\lambda \\int q_\\lambda (z) dz$\n",
    "    $= \\nabla_\\lambda (1) = 0$\n",
    "\n",
    "* $\\nabla_\\lambda E_{q_\\lambda(z)} [\\log \\frac{p(x, z)}{q_\\lambda(z)}]$  \n",
    "$= \\nabla_\\lambda \\int q_\\lambda(z) \\log \\frac{p(x, z)}{q_\\lambda(z)} dz$  \n",
    "$= \\int (\\nabla_\\lambda q_\\lambda(z)) \\log \\frac{p(x, z)}{q_\\lambda(z)} dz - \\int q_\\lambda(z) (\\nabla_\\lambda \\log q_\\lambda(z)) dz$  \n",
    "$= E_{q_\\lambda(z)}[(\\nabla_\\lambda q_\\lambda(z)) \\log \\frac{p(x, z)}{q_\\lambda(z)}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## black box variational inference\n",
    "\n",
    "* $\\nabla_\\lambda E_{q_\\lambda(z)}[\\log \\frac{p(x, z)}{q_\\lambda(z)}]$\n",
    "$\\approx \\frac{1}{S} \\sum_s^S (\\nabla_\\lambda \\log q_\\lambda(z^{(s)})) (\\log p(x, z^{(s)}) - q_\\lambda (z^{(s)}))$\n",
    "    * $z^{(s)} \\stackrel{iid}{\\sim} q_\\lambda(z)$\n",
    "    * optimize variational distribution with stochastic gradient descent\n",
    "    * consider $p(x, z) = \\frac{1}{c} v(x, z)$  \n",
    "    then we can ignore the proportion $c$ since $\\nabla_\\lambda \\log c = 0$\n",
    "\n",
    "* naive black box gradient has high variance--SGD is inefficient\n",
    "\n",
    "**rao-blackwellization**\n",
    "\n",
    "* given estimate that depends on multiple random variables and we can integrate out some of the random variables,\n",
    "then we get an estimate with lower variance\n",
    "* e.g. let $\\bar{J} = E_X E_Y[J(X, Y)]$  \n",
    "$\\hat{J}(X) = E_Y[J(X, Y) | X]$  \n",
    "$E_X[\\hat{J}(X)] = \\bar{J}$  \n",
    "then $J(X, Y)$ can be sampled to estimate $\\bar{J}$  \n",
    "and $\\hat{J}(X)$ can be sampled to estimate $\\bar{J}$\n",
    "\n",
    "**theorem** $Var(\\hat{J}(X)) \\leq Var(J(X, Y))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**simple case** $P(X, Z) = \\prod_i p(z_i) p(x_i | z_i) = \\prod_i p(x_i, z_i)$  \n",
    "$q(z) = \\prod q_{\\lambda_i} (z_i)$ (mean field approximation)  \n",
    "$\\nabla_{\\lambda_i} E_{q(z)} [\\log \\frac{p(x, z)}{q(z)}]$  \n",
    "$= \\nabla_{\\lambda_i} E_{q(z_1), ... q(z_n)} [\\sum_j \\log p(x_j, z_j) - \\sum_j \\log q(z_j)]$  \n",
    "$= E_{q(z_1), ..., q(z_n)} [\\sum_j (\\nabla_{\\lambda_i} \\log q(z_j))  (\\sum_j \\log p(x_j, z_j) - \\sum_j \\log q(z_j))]$  \n",
    "$= E_{q(z_i)}[(\\nabla_{\\lambda_i} \\log q(z_i))(\\log p(x_i, z_i) - \\log q(z_i))]$  \n",
    "\n",
    "and we can see that the gradient of the ELBO is an expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** logistic regression  \n",
    "$p(w) \\prod_i p(t_i | w)$  \n",
    "$q(w) = \\mathcal{N}(w | m, V)$  \n",
    "can't factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**control variances**\n",
    "\n",
    "**e.g.** $\\mu = E_{p(v)}[f(v)]$  \n",
    "then $\\hat{\\mu} = f(v^{(s)})$ with $v^{(s)} \\sim p(v)$  \n",
    "high variance\n",
    "\n",
    "consider any random variable $w$ s.t. $E[w] = 0$  \n",
    "use $\\tilde{\\mu} = \\hat{\\mu} - c w$  \n",
    "$E[\\tilde{\\mu}] = E[\\hat{\\mu}] - c E[w] = E[\\hat{\\mu}]$  \n",
    "want to show $Var(\\tilde{\\mu}) < Var(\\hat{\\mu})$  \n",
    "$Var(\\tilde{\\mu}) = Var(\\hat{\\mu}) + c^2 Var(w) - 2 c Cov(\\hat{\\mu}, w)$  \n",
    "to minimize, $c = \\frac{Cov(\\hat{\\mu}, w)}{Var(w)}$, and we get  \n",
    "$= Var(\\hat{\\mu}) - \\frac{(Cov(\\hat{\\mu}, w))^2}{Var(w)}$ \n",
    "$\\leq Var(\\hat{\\mu})$  \n",
    "this is also equal to $Var(\\hat{\\mu}) - Cor(\\hat{\\mu}, w)^2 Var(\\hat{\\mu})$\n",
    "$= Var(\\hat{\\mu}) (1 - Cor(\\hat{\\mu}, w))$\n",
    "\n",
    "how to choose $w$ and calculate $c$?\n",
    "\n",
    "* $w^{(s)} = \\nabla_\\lambda \\log q_\\lambda (z^{(s)})$\n",
    "* $\\hat{\\mu}^{(s)} = (\\nabla_\\lambda \\log q_\\lambda(z^{(s)}) (\\log p(x, z^{(s)} - q_\\lambda (z^{(s)}))$  \n",
    "where $z^{(s)} \\sim q_\\lambda$\n",
    "* to calculate/estimate $c$\n",
    "    * $b = Cov(\\hat{\\mu}, w) \\approx \\frac{1}{S} \\sum_s^S (\\hat{\\mu}^{(s)} - \\bar{\\mu}) (w^{(s)} - 0)$\n",
    "    * $\\bar{\\mu} = \\frac{1}{S} \\sum_s \\hat{\\mu}^{(s)}$\n",
    "    * $a = Var(w) \\approx \\frac{1}{S} \\sum_s (w^{(s)})^2$\n",
    "    * then $c = b / a$\n",
    "\n",
    "then we get $\\frac{1}{S} \\sum_s^S (\\nabla_\\lambda \\log q_\\lambda (z^{(s)}) (\\log p(x, z^{(s)}) - q_\\lambda(z^{(s)}) - c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variational auto-encoder\n",
    "\n",
    "two ideas\n",
    "\n",
    "1. alternative method to express gradients as expectations and estimate them through sampling\n",
    "2. amortized inference\n",
    "\n",
    "recall,  \n",
    "$ELBO = E_{q(z)}[\\log \\frac{p(x, z)}{q(z)}]$  \n",
    "$= E_{q(z)}[\\log p(z) + \\log p(x|z) - \\log q(z)]$  \n",
    "$= E_{q(z)}[\\log p(x|z)] - d_{KL}(q(z) || p(z))$  \n",
    "and gradients for the second term should be easy to obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reparameterization\n",
    "\n",
    "given $E_{q_\\phi(z)}[f(x, z)]$  \n",
    "want to optimize by choosing $q_\\phi(z)$ (need gradients w.r.t. $\\phi$)\n",
    "\n",
    "in some cases, can rewrite $E[\\cdots]$ so that $\\nabla$ is easy\n",
    "\n",
    "**e.g.** let $q_\\phi(z) = \\mathcal{N}(z | \\mu, \\Sigma)$\n",
    "\n",
    "* $\\phi = (\\mu, \\Sigma)$\n",
    "* cholesky decomposition $\\Sigma = L L^\\top$, $L$ is lower diagonal\n",
    "* then $q_\\phi(z) = \\mathcal{N}(z | \\mu, L L^\\top)$\n",
    "* let $\\epsilon \\mathcal{N}(0, I)$\n",
    "* then we can say $z = \\mu + L \\epsilon$\n",
    "* $z \\sim \\mathcal{N}(\\mu, L I L^\\top)$\n",
    "* $E_{q_\\phi(z)} [f(x, z)] = E_{\\epsilon}[f(x, \\mu + L \\epsilon)]$\n",
    "\n",
    "**more formally**,  \n",
    "$z \\sim q_\\phi(z) \\iff \\epsilon \\sim p(\\epsilon)$ and $z = g_\\phi(\\epsilon)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ELBO = E_{q_\\phi(z)}[\\log \\frac{p(x, z)}{q(z)}]$\n",
    "$= E_\\epsilon[\\log \\frac{p(x, g_\\phi(\\epsilon))}{q_\\phi(z)}]$\n",
    "\n",
    "$\\nabla_\\phi ELBO = E_\\epsilon[\\nabla_\\phi \\log p(x, g_\\phi(\\epsilon))] - E_\\epsilon[\\nabla_\\phi \\log q_\\phi(g_\\phi(\\epsilon))]$\n",
    "\n",
    "then $\\nabla_\\phi ELBO \\sim \\frac{1}{S} \\sum_s^S \\nabla_\\phi \\log p(x, g_\\phi(\\epsilon^{(s)})) - \\nabla_\\phi \\log q_\\phi(g_\\phi(\\epsilon^{(s)}))$\n",
    "\n",
    "then use chain rule  \n",
    "requirements w.r.t. $\\nabla$ are stronger than in BBVI  \n",
    "requires $g_\\phi(\\epsilon)$ to be differentiable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consider $p_\\theta(x, z) = \\prod_i p_\\theta(x_i, z_i) = \\prod_i p_\\theta(z_i) p_\\theta(x_i, z_i)$  \n",
    "\n",
    "* also assume $q(z) = \\prod_i q_i(z_i)$\n",
    "\n",
    "$ELBO = E_{q(z)}[\\log \\frac{p(x, z)}{q(z)}]$  \n",
    "$= E_{q(z)}[\\log \\prod p(x_i | z_i) + \\log \\prod p(z_i)- \\log \\prod q(z_i)]$  \n",
    "$= \\sum_i E_{q(z_i)}[\\log p(x_i | z_i)] + \\sum_i E_{q(z_i)}[\\log p(z_i)] - \\sum_i E_{q(z_i)}[\\log q(z_i)]$  \n",
    "$= \\sum_i E_\\epsilon [\\log p(x_i, g_\\phi(\\epsilon))] + \\cdots$\n",
    "\n",
    "$\\nabla_{\\phi_k} ELBO E_\\epsilon [\\log p(x_k, g_{\\phi_k}(\\epsilon))] + \\cdots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### amortized inference\n",
    "\n",
    "consider $p(x, z) = \\prod_i p(z_i) p(x_i | z_i)$ and $q(z) = \\prod_i q(z_i)$\n",
    "\n",
    "for variational inference, pick $q(z_i)$ to optimize ELBO\n",
    "\n",
    "* implicitly, $q(z_i)$ depends on on $x_i$, but dependence is not explicit\n",
    "* $q(z_i)$ \"flexible\" to adjust according to the data\n",
    "* $q(z_i) = q_\\phi(z_i | x_i)$\n",
    "* **e.g.**, $\\mu = A x_i$\n",
    "    * puts some restrictions on parameters\n",
    "    * if we want to be flexible, instead of a linear model, use something like a DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** $q_\\phi(z_i | x_i) = \\mathcal{N}(z_i | \\mu_\\phi(x_i), \\sigma_\\phi(x_i)^2)$\n",
    "\n",
    "then $x_i$ is input to the network  \n",
    "and network has two outputs, $\\mu$ and $\\sigma^2$  \n",
    "$\\phi$ represents the weights/biases of network and are parameters to be tuned  \n",
    "$z_i \\in \\mathbb{R}^d$, use diagonal approximation  \n",
    "$z_{ik} \\sim \\mathcal{N}(\\mu_{k, \\phi}(x_i), \\sigma^2_{k, \\phi}(x_i))$\n",
    "\n",
    "tradeoff: number of parameters goes from $2 N$ (two parameters, $\\mu$ and $\\sigma^2$), we have $|\\phi|$ parameters (number of weights/biases in NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minibatch sample\n",
    "\n",
    "$\\nabla_\\phi ELBO = \\frac{1}{S} \\sum_s \\frac{N}{M} \\sum_{i \\in \\mathcal{M}} \\log p_\\theta(g_\\phi(\\epsilon_i^{(s)})) p_\\theta (x_i | g_\\phi(\\epsilon_i^{(s)})) - \\log q_\\phi(g_\\phi(\\epsilon_i^{(s)}))$\n",
    "\n",
    "* $S$ : number of samples of $\\epsilon$\n",
    "* $N$ : number of examples\n",
    "\n",
    "(in paper, $S = 1$ as long as $M \\geq 100$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAE: combine the two ideas  \n",
    "$+$ $p(x_i | z_i)$ is also approximated by a DNN\n",
    "\n",
    "**e.g.**: $x_i$ is an image $x_i \\in \\{0, 1\\}^{d \\times d}$ $p(x_i | z_i) = \\prod_d \\mu_{\\theta, d}(z_i)^{x_i} (1 - \\mu_{\\theta, d}(z_i))^{1 - x_i}$\n",
    "\n",
    "**e.g.**, $x_i \\in \\mathbb{R}^d$ with diagonal variance $p(x_i | z_i) = \\prod_d \\mathcal{N}(\\mu_{\\theta, d}(z_i), \\sigma^2_{\\theta, d}(z_i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** logistic regression\n",
    "\n",
    "* $f_i = w^\\top \\phi(x_i)$\n",
    "* $y_i = \\sigma(f_i)$\n",
    "* $t_i \\sim Bernoulli(y_i)$\n",
    "* $\\log L = \\sum t_i \\log y_i + \\sum (1 - t_i) \\log (1 - y_i)$\n",
    "* $\\nabla_w \\log L = \\sum (t_i - y_i) \\phi(x_i)$\n",
    "* $\\nabla^2_w \\log L = \\sum y_i (1 - y_i) \\phi(x_i) \\phi(x_i)^\\top = \\Phi^\\top R \\Phi$\n",
    "\n",
    "* prior: $w \\sim \\mathcal{N}(0, \\frac{1}{\\alpha} I)$\n",
    "* approx posterior: $q(w) = \\mathcal{N}(m, V) = \\mathcal{N}(m, L L^\\top)$\n",
    "\n",
    "* ELBO: $E_{q_\\lambda(z)}[\\log \\frac{p(x, z)}{q_\\lambda(z)}]$\n",
    "    * $x \\to t$\n",
    "    * $z \\to w$\n",
    "    * $\\lambda \\to (m, L)$ (or $(m, V)$)\n",
    "* BBVI: $\\nabla_\\lambda ELBO = E_{q_\\lambda(z)}[(\\nabla_\\lambda \\log q_\\lambda(z)) (\\log p(x, z) - \\log q_\\lambda(z))]$\n",
    "    * can also add a constant to reduce variance (reparameterization)\n",
    "* reparameterization: $ELBO = E_\\epsilon[\\log \\frac{p(x, g_\\lambda(\\epsilon))}{q_\\lambda(g_\\lambda(\\epsilon))}]$\n",
    "    * reparameterize $z = g_\\lambda(\\epsilon)$\n",
    "        * for $z \\sim \\mathcal{N}(\\mu, V)$, $z = \\mu + L \\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0, I)$\n",
    "    * $ELBO = E_{q(w)}[\\log p(w) - \\log p(t|w) - \\log q(w)]$  \n",
    "    $= E_{q(w)}[(-\\frac{d}{2} \\log \\alpha - \\frac{\\alpha}{2} w^\\top w) + (\\sum_i t_i \\log y_i + (1 - t_i) \\log (1 - y_i)) - (-\\frac{1}{2} \\log |V| - \\frac{1}{2} (w - m)^\\top V^{-1} (w - m))]$\n",
    "        * $\\partial_X \\log |X| = (X^{-1})^\\top$\n",
    "        * $\\partial_X \\log |X^\\top X| = 2 |X^{-1}|^\\top$\n",
    "        * $\\partial_A (q^\\top A^{-1} b) = -A^{-1} ab^\\top (A^{-1})^\\top$\n",
    "        * $\\partial_X Tr(X X\\top B) = B X + B^\\top X$\n",
    "        * $\\partial_X (q^\\top X b) = ab^\\top$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "direct solution: calculate ELBO and $\\nabla_m$ and $\\nabla_L$ and use (S)GA\n",
    "\n",
    "* gaussian identities given $q(w) = \\mathcal{N}(w | m, V)$\n",
    "    * $\\nabla_m E_{q(w)}[f(w)] = E_{q(w)}[\\nabla_w f(w)]$\n",
    "    * $\\nabla_V E_{q(w)}[f(w)] = \\frac{1}{2} E_{q(w)}[\\nabla_w^2 f(w)]$\n",
    "* $\\nabla_m = -\\alpha m + \\sum (t_i - E[y_i]) \\phi(x_i)$\n",
    "* $\\nabla_V = \\frac{1}{2} (-\\alpha I - \\Phi^\\top R \\Phi + V^{-1})$\n",
    "    * $R = diag(E[y_i (1 - y_i)])$\n",
    "    * $V \\leftarrow V + \\gamma \\nabla_V$ (may not be PSD)  \n",
    "    better to use gradient ascent on $L$ instead\n",
    "* $\\nabla_L ELBO = -\\alpha L - \\Phi^\\top R \\Phi + (L^{-1})^\\top$\n",
    "    * use gradient ascent on $m, L$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BBVI approach\n",
    "\n",
    "* $\\log q(z) \\to -\\frac{1}{2} \\log |V| - \\frac{1}{2} (w - m)^\\top V^{-1} (w - m)$  \n",
    "$= -\\frac{1}{2} \\log ||L L^\\top| - \\frac{1}{2} (w - m)^\\top (L^\\top)^{-1} L^{-1} (w - m)$\n",
    "* $\\log p(x, z) \\to -\\frac{\\alpha}{2} \\log \\alpha - \\frac{\\alpha}{2} w^\\top w - \\sum t_i \\log y_i - (1 - t_i) \\log (1 - y_i)$\n",
    "* $\\nabla_m \\log q(z) = V^{-1} (m - w)$\n",
    "* $\\nabla_L \\log q(z) = -((L^{-1})^\\top - V^{-1} (w - m) (w - m)^\\top (L^{-1})^\\top)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reparameterization approach: $w = m + L \\epsilon$\n",
    "\n",
    "* $ELBO = E_\\epsilon[\\frac{d}{2} \\log \\alpha + \\frac{\\alpha}{2} (m + L \\epsilon)^\\top (m + L \\epsilon) + \\frac{1}{2} \\log |L L^\\top| + \\frac{1}{2} \\epsilon^\\top L^\\top (L^\\top)^{-1} L^{-1} L \\epsilon + \\sum_i t_i \\log \\sigma(m + L \\epsilon)^\\top \\phi(x_i) + \\sum_i (1 - t_i) \\log \\sigma(1 - m - L \\epsilon)^\\top \\phi(x_i)]$\n",
    "* $\\nabla_m ELBO = E_\\epsilon[\\nabla_m \\cdots]$  \n",
    "$= E_\\epsilon[-\\alpha (m + L \\epsilon + \\sum_i (t_i - y_i) \\phi(x_i)]$  \n",
    "$= -\\alpha m + \\sum (t_i - E_\\epsilon[y_i]) \\phi(x_i)$  \n",
    "which is the same as in the direct solution\n",
    "* $\\nabla_L ELBO = E_\\epsilon[\\nabla_L \\cdots]$  \n",
    "$= -\\alpha L + (L^{-1})^\\top \\sum_i \\phi(x_i) E[y_i \\epsilon^\\top]$  \n",
    "not the same as in the direct method in form but should be equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
