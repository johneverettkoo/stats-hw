{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automating Probabilistic ML\n",
    "\n",
    "* probabilistic programming\n",
    "* specify a probabilistic model\n",
    "    * $\\to$ \"directy\" get the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preliminaries for black box variational inference\n",
    "\n",
    "* $\\nabla_\\lambda E_{q_\\lambda(z)}[f(z)] = E_{q_{\\lambda}(z)}[(\\nabla_\\lambda \\log q_\\lambda(z)) f(z)]$\n",
    "    * *proof*  \n",
    "    $\\nabla_\\lambda E_{q_\\lambda(z)}[f(z)] = \\nabla_\\lambda \\int q_\\lambda(z) f(z) dz$  \n",
    "    $= \\int (\\nabla_\\lambda q_\\lambda(z)) f(z) dz$  \n",
    "    $= \\int q_\\lambda(z) (\\nabla_\\lambda \\log q_\\lambda (z)) f(z) dz$\n",
    "* $E_{q_\\lambda(z)} [\\nabla_\\lambda \\log q_\\lambda (z)] = 0$\n",
    "    * *proof*  \n",
    "    the above expression $= \\int q_\\lambda (z) (q_\\lambda(z))^{-1} \\nabla q_\\lambda (z) dz$  \n",
    "    $= \\nabla_\\lambda \\int q_\\lambda (z) dz$\n",
    "    $= \\nabla_\\lambda (1) = 0$\n",
    "\n",
    "* $\\nabla_\\lambda E_{q_\\lambda(z)} [\\log \\frac{p(x, z)}{q_\\lambda(z)}]$  \n",
    "$= \\nabla_\\lambda \\int q_\\lambda(z) \\log \\frac{p(x, z)}{q_\\lambda(z)} dz$  \n",
    "$= \\int (\\nabla_\\lambda q_\\lambda(z)) \\log \\frac{p(x, z)}{q_\\lambda(z)} dz - \\int q_\\lambda(z) (\\nabla_\\lambda \\log q_\\lambda(z)) dz$  \n",
    "$= E_{q_\\lambda(z)}[(\\nabla_\\lambda q_\\lambda(z)) \\log \\frac{p(x, z)}{q_\\lambda(z)}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### black box variational inference\n",
    "\n",
    "* $\\nabla_\\lambda E_{q_\\lambda(z)}[\\log \\frac{p(x, z)}{q_\\lambda(z)}]$\n",
    "$\\approx \\frac{1}{S} \\sum_s^S (\\nabla_\\lambda \\log q_\\lambda(z^{(s)})) (\\log p(x, z^{(s)}) - q_\\lambda (z^{(s)}))$\n",
    "    * $z^{(s)} \\stackrel{iid}{\\sim} q_\\lambda(z)$\n",
    "    * optimize variational distribution with stochastic gradient descent\n",
    "    * consider $p(x, z) = \\frac{1}{c} v(x, z)$  \n",
    "    then we can ignore the proportion $c$ since $\\nabla_\\lambda \\log c = 0$\n",
    "\n",
    "* naive black box gradient has high variance--SGD is inefficient\n",
    "\n",
    "**rao-blackwellization**\n",
    "\n",
    "* given estimate that depends on multiple random variables and we can integrate out some of the random variables,\n",
    "then we get an estimate with lower variance\n",
    "* e.g. let $\\bar{J} = E_X E_Y[J(X, Y)]$  \n",
    "$\\hat{J}(X) = E_Y[J(X, Y) | X]$  \n",
    "$E_X[\\hat{J}(X)] = \\bar{J}$  \n",
    "then $J(X, Y)$ can be sampled to estimate $\\bar{J}$  \n",
    "and $\\hat{J}(X)$ can be sampled to estimate $\\bar{J}$\n",
    "\n",
    "**theorem** $Var(\\hat{J}(X)) \\leq Var(J(X, Y))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**simple case** $P(X, Z) = \\prod_i p(z_i) p(x_i | z_i) = \\prod_i p(x_i, z_i)$  \n",
    "$q(z) = \\prod q_{\\lambda_i} (z_i)$ (mean field approximation)  \n",
    "$\\nabla_{\\lambda_i} E_{q(z)} [\\log \\frac{p(x, z)}{q(z)}]$  \n",
    "$= \\nabla_{\\lambda_i} E_{q(z_1), ... q(z_n)} [\\sum_j \\log p(x_j, z_j) - \\sum_j \\log q(z_j)]$  \n",
    "$= E_{q(z_1), ..., q(z_n)} [\\sum_j (\\nabla_{\\lambda_i} \\log q(z_j))  (\\sum_j \\log p(x_j, z_j) - \\sum_j \\log q(z_j))]$  \n",
    "$= E_{q(z_i)}[(\\nabla_{\\lambda_i} \\log q(z_i))(\\log p(x_i, z_i) - \\log q(z_i))]$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** logistic regression  \n",
    "$p(w) \\prod_i p(t_i | w)$  \n",
    "$q(w) = \\mathcal{N}(w | m, V)$  \n",
    "can't factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**control variances**\n",
    "\n",
    "**e.g.** $\\mu = E_{p(v)}[f(v)]$  \n",
    "then $\\hat{\\mu} = f(v^{(s)})$ with $v^{(s)} \\sim p(v)$  \n",
    "high variance\n",
    "\n",
    "consider any random variable $w$ s.t. $E[w] = 0$  \n",
    "use $\\tilde{\\mu} = \\hat{\\mu} - c w$  \n",
    "$E[\\tilde{\\mu}] = E[\\hat{\\mu}] - c E[w] = E[\\hat{\\mu}]$  \n",
    "want to show $Var(\\tilde{\\mu}) < Var(\\hat{\\mu})$  \n",
    "$Var(\\tilde{\\mu}) = Var(\\hat{\\mu}) + c^2 Var(w) - 2 c Cov(\\hat{\\mu}, w)$  \n",
    "to minimize, $c = \\frac{Cov(\\hat{\\mu}, w)}{Var(w)}$, and we get  \n",
    "$= Var(\\hat{\\mu}) - \\frac{(Cov(\\hat{\\mu}, w))^2}{Var(w)}$ \n",
    "$\\leq Var(\\hat{\\mu})$  \n",
    "this is also equal to $Var(\\hat{\\mu}) - Cor(\\hat{\\mu}, w)^2 Var(\\hat{\\mu})$\n",
    "$= Var(\\hat{\\mu}) (1 - Cor(\\hat{\\mu}, w))$\n",
    "\n",
    "how to choose $w$ and calculate $c$?\n",
    "\n",
    "* $w^{(s)} = \\nabla_\\lambda \\log q_\\lambda (z^{(s)})$\n",
    "* $\\hat{\\mu}^{(s)} = (\\nabla_\\lambda \\log q_\\lambda(z^{(s)}) (\\log p(x, z^{(s)} - q_\\lambda (z^{(s)}))$  \n",
    "where $z^{(s)} \\sim q_\\lambda$\n",
    "* to calculate/estimate $c$\n",
    "    * $b = Cov(\\hat{\\mu}, w) \\approx \\frac{1}{S} \\sum_s^S (\\hat{\\mu}^{(s)} - \\bar{\\mu}) (w^{(s)} - 0)$\n",
    "    * $\\bar{\\mu} = \\frac{1}{S} \\sum_s \\hat{\\mu}^{(s)}$\n",
    "    * $a = Var(w) \\approx \\frac{1}{S} \\sum_s (w^{(s)})^2$\n",
    "    * then $c = b / a$\n",
    "\n",
    "then we get $\\frac{1}{S} \\sum_s^S (\\nabla_\\lambda \\log q_\\lambda (z^{(s)}) (\\log p(x, z^{(s)}) - q_\\lambda(z^{(s)}) - c)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
