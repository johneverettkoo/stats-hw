{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Inference\n",
    "\n",
    "### Motivation\n",
    "\n",
    "**e.g.** (bayesian) logistic regression\n",
    "\n",
    "* can't obtain exact posterior\n",
    "* possible solution: laplace approximation\n",
    "\n",
    "**e.g.** LDA\n",
    "\n",
    "* use MCMC to sample from posterior\n",
    "    * slow\n",
    "\n",
    "**e.g.** EM algorithm\n",
    "\n",
    "* requires calculation of expectation\n",
    "    * sometimes not possible to calculate analytically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convex Duality\n",
    "\n",
    "**outline**\n",
    "\n",
    "* suppose $f(x)$ is concave\n",
    "* find a linear function that captures the upper bound of $f(x)$\n",
    "* $f(x) = \\min_\\lambda \\{\\lambda^\\top x - f^*(\\lambda)\\}$\n",
    "    * $f^*(\\lambda) = \\min_x |\\lambda^\\top x - f(x)|$\n",
    "* $f(x) \\leq \\lambda^\\top x - f^*(\\lambda)$ (by definition of minimum)\n",
    "\n",
    "**e.g.** $f(x) = \\log x$\n",
    "\n",
    "* already know this is concave\n",
    "* $f^*(\\lambda) = \\min_x \\{\\lambda x - \\log x\\}$\n",
    "* $f'(x) = -\\lambda - x^{-1}$  \n",
    "$\\implies x = \\lambda^{-1}$\n",
    "* then $f^*(\\lambda) = 1 + \\log \\lambda$\n",
    "* $\\implies f(x) = \\min_\\lambda \\{\\lambda x - (1 + \\log \\lambda)\\}$\n",
    "    * i.e., $f(x) \\leq \\lambda x - (1 + \\log \\lambda)$\n",
    "    \n",
    "**e.g.** $f(x) = \\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "* not concave (or convex for that matter)\n",
    "* $g(x) = \\log f(x) = -\\log (1 + e^{-x})$ is concave\n",
    "    * \"$f(x)$ is log concave\"\n",
    "    * can verify concaveness with the second derivative  \n",
    "    $g''(x) = \\frac{-e^x}{(1 + e^x)^2} \\geq 0$\n",
    "* $g^*(\\lambda) = \\min_x \\{\\lambda x + \\log (1 + e^{-x}) \\}$\n",
    "* $g'(x) = \\lambda - \\frac{e^{-x}}{1 + e^{-x}}$  \n",
    "$\\implies \\lambda = \\frac{e^{-x}}{1 + e^{-x}}$  \n",
    "$\\implies x = \\log \\frac{1 - \\lambda}{\\lambda}$  \n",
    "$\\implies 1 + e^{-x} = \\frac{1}{1 - \\lambda}$  \n",
    "$\\implies g^*(\\lambda) = \\lambda \\log \\frac{1 - \\lambda}{\\lambda} + \\log \\frac{1}{1 - \\lambda}$  \n",
    "$= \\lambda \\log (1 - \\lambda) - \\lambda \\log \\lambda - \\log (1 - \\lambda)$  \n",
    "$= -\\lambda \\log \\lambda - (1 - \\lambda) \\log (1 - \\lambda)$\n",
    "* entropy function $H(\\lambda)$\n",
    "* $g(x) \\leq \\lambda x - H(\\lambda)$\n",
    "* $f(x) = e^g(x) \\leq e^{\\lambda x - H(\\lambda)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using convex duality bounds in machine learning\n",
    "\n",
    "**local variational methods**\n",
    "\n",
    "* likelihood function $L = f_1 f_2 \\cdots$  \n",
    "$\\leq f_1 g_2 g_3 f_4 \\cdots$ (replace some of the $f$'s with approximations)\n",
    "    * not the same as the likelihood, but hopefully the bounds are close enough\n",
    "    * a linear approximation is okay locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global/block variational approximation\n",
    "\n",
    "* if we can't calculate posterior $p(z | data)$, \n",
    "what is the best replacement/approximation?\n",
    "* find $q(z)$ s.t. $distance(q(z), p(z | data)$ is minimized\n",
    "    * need some measure of distance/divergence\n",
    "    * one possibility: KL divergence $d_{KL}(p_1(z), p_2(z)) = E_1[\\log \\frac{p_1(z)}{p_2(z)}]$\n",
    "        * can minimize this over $q(z)$ (variational) or expectation propagation (KL divergence is not symmetric)\n",
    "        * for this class use $\\min_{q(z)} d_{KL}(q, p)$\n",
    "            * other way around is more difficult since we don't have $p(z)$\n",
    "            * if $p$ is multimodal, the variational approach will choose one mode\n",
    "            * if $p$ is multimodal, the EP approach will average the modes (although we still need $p$ for direct variational approach)\n",
    "            * either way, there's a compromise for multimodal $p$\n",
    "* $\\log p(y) = E_{q(z)}[\\log \\frac{q(z)}{p(z | y)}] + E_{q(z)}[\\log \\frac{p(y, z)}{q(z)}]$\n",
    "    * *proof*: the above rhs is  \n",
    "    $= E_{q(z)}[\\log \\frac{q(z) p(y) p(z|y)}{p(z|y) p(z)}]$  \n",
    "    $= E_{q(z)}[\\log p(y)] = \\log p(y)$\n",
    "* then $\\log p(y) = d_{KL}(q(z) p(z|y)) + L$ ($L$ is the lower bound)\n",
    "    * $p(y)$ doesn't depend on our parameter $z$, so we can treat it as constant\n",
    "    * minimizing the KL-divergence is equivalent to maximizing $L$, the variational lower bound (also called evidence lower bound, ELBO)\n",
    "* $\\log p(y) \\leq L$\n",
    "* $\\log p(y) = \\log \\int p(z) p(y|z) dz$ \n",
    "$= \\log \\int q(z) \\frac{p(z)}{q(z)} p(y|z) dz$\n",
    "$\\geq \\int q(z) \\log \\frac{p(z) p(y|z)}{q(z)} dz$\n",
    "$= E_{q(z)} [\\log \\frac{p(y, z)}{q(z)}] = L$\n",
    "* alternative and equivalent form of ELBO:  \n",
    "$ELBO = E_{q(z)}[\\log \\frac{p(y, z)}{q(z)}]$\n",
    "$= E_{q(z)}[\\log p(y|z)] + E_{q(z)}[\\log \\frac{p(z)}{q(z)}]$\n",
    "$= E_{q(z)}[\\log p(y|z)] - d_{KL}(q(z), p(z))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection between global and local variational methods\n",
    "\n",
    "* global approximation can be derived from convex duality\n",
    "* for this part, $z$ is discrete\n",
    "* consider $\\log p(y) = \\log \\sum_z p(y, z) = \\log \\sum_z e^{\\log p(y, z)}$\n",
    "    * let $x$ be a vector indexed by $z$ with entries $\\log p(y, z)$\n",
    "    * $\\log p(y) = f(x) = \\log \\sum_z e^{\\log p(y, z)} = \\log \\sum_z e^{x_z}$\n",
    "    * $f(x)$ is convex so we can apply convex duality to minimize\n",
    "* $f(x) = \\max_\\lambda \\{\\lambda^\\top x - f^*(\\lambda)\\}$\n",
    "$= \\geq \\lambda^\\top x - f^*(\\lambda)$\n",
    "$= \\sum_z q(z) \\log p(y, z) - f^*([q_z])$\n",
    "    * $\\lambda$ is a vector indexed by $z$ with values $q(z) = q_z$\n",
    "    * $f^*(\\lambda) = \\max_z \\{\\lambda^\\top x - f(x)\\}$\n",
    "    $= \\max_{p(y, z)} \\{\\sum_z q(z) \\log p(y, z) - \\log p(y)\\}$\n",
    "    $= \\max_{p(y, z)} \\{\\sum_z q(z) \\log p(z|y)\\}$  \n",
    "    $= \\sum_z q(z) \\log q(z)$\n",
    "    * then we get $\\log p(y) = f(x) \\geq \\sum_z q(z) \\log p(y, z) - \\sum_z q(z) \\log q(z)$  \n",
    "    $= E_{q(z)} [\\log \\frac{p(y, z)}{q(z)}] = ELBO$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational EM algorithm\n",
    "\n",
    "* $p(y, z)$ parameterized by $\\theta$\n",
    "* $q(z)$ parameterized by $\\lambda$\n",
    "* then $\\log p(y) \\leq L(\\lambda, \\theta)$  \n",
    "$= E_{q(z|\\lambda}[\\log \\frac{p(y, z)|\\theta)}{q(z|\\lambda)}]$\n",
    "\n",
    "**algorithm** Variational EM\n",
    "\n",
    "* initialize $\\theta^{(0)}$, $i = 0$\n",
    "* E-step\n",
    "    * pick $q$, i.e., $\\lambda^{(i)}$ by $\\max L(\\lambda^{(i)}, \\theta^{(i)})$\n",
    "* M-step\n",
    "    * pick $\\theta^{(i+1)}$ by $\\max L(\\lambda^{(i)}, \\theta^{(i+1)}$\n",
    " \n",
    "**e.g.** $q(z)$ is not restricted  \n",
    "$\\min d_{KL}(q(z), p(z|y)) \\implies q(z) = p(z, y) = p(z, y | \\theta)$  \n",
    "then this is the implicit solution of the E-step  \n",
    "M-step: $\\max_{\\theta^{(i+1)}} E_{p(z|y, \\theta^{(i)})}[\\log p(y, z | \\theta^{(i+1)})] - E_{p(z | y, \\theta^{(i)}}[\\log p(z | y, \\theta^{(i)}]$  \n",
    "first term is exact, second term doesn't depend on $\\theta^{(i+1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean field approximation\n",
    "\n",
    "* pick $q(z)$ to maximize $L = E_{q(z)}[\\log \\frac{p(y, z)}{q(z)}]$\n",
    "* if family of $q(z)$ is not restricted, then $q(z) = p(z|y)$\n",
    "* restrict family $Q = \\{q(z)\\}$\n",
    "* how to pick $Q$?\n",
    "    * restrict form of $q(z)$, e.g., $q$ can be gaussian with different parameters (fixed form variational inference)\n",
    "    * divide $z$ into indepedndent subgroups $z_1, ..., z_K$, then $q(z) = \\prod_j q(z_j)$  (**mean field approximation**)\n",
    "    \n",
    "**General form of the solution to the mean field approximation**\n",
    "\n",
    "* consider discrete case for now\n",
    "* again, $\\log p(y) \\geq L = \\sum_z q(z) \\log \\frac{p(y, z)}{q(z)}$\n",
    "* restrict $q(z) = \\prod_j q(z_j)$\n",
    "* then we get $L = \\sum_z (\\prod_j q(z_j)) \\log p(y, z) - \\sum_z (\\prod_j q(z_j)) (\\sum_l \\log q (z_l))$  \n",
    "$= \\sum_{z_j} q(z_j) \\prod_{l \\neq j} q(z_l) \\log p(y, z) - \\sum_z (\\prod_a q(z_a)) \\log q_j (z_j) - \\sum_z (\\prod_a q(z_a)) \\sum_{j \\neq l} q_l (z_l)$\n",
    "    * optimize w.r.t. $z_j$\n",
    "    * third term integrates out $z_j$, so constant w.r.t. $z_j$\n",
    "    * second term: integration w.r.t. $l \\neq j$ results in $1$, so it simplifies to $\\sum_{z_j} q(z_j) \\log q(z_j)$\n",
    "    * first term can be rewritten as  \n",
    "    $\\sum_{z_j} q(z_j) \\prod_{l \\neq j} q(z_l) \\log p(y, z)$  \n",
    "    $= \\sum_{z_j} q(z_j) \\sum_{z_{l \\neq j}} \\prod_{l \\neq j} q(z_l) \\log p(y, z)$  \n",
    "    $= \\sum_{z_j} q(z_j) E_{\\prod_{l \\neq j} q(z_l)} [\\log p(y, z)]$\n",
    "* so $L = constant + \\sum_{z_j} q(z_j) g(z_j, y) - \\sum_{z_j} q(z_j) \\log q(z_j)$\n",
    "    * where $g(z_j, y) = E_{\\prod_{l \\neq j} q(z_l)}[\\log p(y, z)]$\n",
    "* rewriting again, we get $L = constant + \\sum_{z_j} q(z_j) \\log \\frac{\\exp(g(z_j, y))}{q(z_j)}$\n",
    "* if $e^{g(z_j, y)}$ is normalized (i.e., treat as distribution), then $L = constant - d_{KL}(q(z_j), e^{g(z_j, y)})$\n",
    "* pick $q(z_j)$ to minimize divergence $\\implies q(z_j) = h(z_j)$  \n",
    "$\\implies q(z_j) \\propto e^{g(z_j, y)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** linear regression\n",
    "\n",
    "* prior: \n",
    "    * $w \\sim \\mathcal{N}(0, \\alpha^{-1} I)$\n",
    "    * $\\alpha \\sim Gamma(a_0, b_0)$\n",
    "* likelihood: $t \\mid w, \\beta \\sim \\mathcal{N}(\\Phi w, \\beta^{-1} I)$\n",
    "* posterior: $p(w, \\alpha | t)$, complicated to solve\n",
    "* use approximate posterior $q(w, \\alpha) = q(w) q(\\alpha)$\n",
    "* $\\log p(\\alpha, w, t) = \\log G(\\alpha | a_0, b_0) + \\log \\mathcal{N}(w | \\alpha) + \\log \\mathcal{N}(t | w, \\beta^{-1} I)$  \n",
    "$= constant + (a_0 - 1) \\log \\alpha -b_0 \\alpha + \\frac{d}{2} \\log \\alpha - \\frac{\\alpha}{2} w^\\top w - \\frac{\\beta}{2} w^\\top \\Phi^\\top \\Phi w + \\beta w^\\top \\Phi^\\top t$\n",
    "* $\\log q(\\alpha) + E_{q(w)}[\\log p(\\alpha, w, t)]$ (can ignore constants w.r.t. $\\alpha$)  \n",
    "$= (a_0 + d / 2 - 1) \\log \\alpha - (b_0 + \\frac{1}{2} ||m_N||^2 + tr(S_N)) \\alpha$  \n",
    "$\\implies q(\\alpha) = Gamma(a_N, b_N)$\n",
    "    * $a_N = a_0 + d / 2$\n",
    "    * $b_N = b_0 + \\frac{1}{2} (||m_N||^2 + tr(S_N))$\n",
    "    * $E_{q(w)}[w^\\top w] = ||m_N||^2 + tr(S_N)$ (currently unknown)\n",
    "* $\\log q(w) = E_{q(\\alpha)}[-\\frac{\\alpha}{2} w^\\top w - \\frac{\\beta}{2} w^\\top \\Phi^\\top \\Phi w + \\beta w^\\top \\Phi^\\top t] + constant$  \n",
    "$= -\\frac{E[\\alpha]}{2} w^\\top w - \\frac{\\beta}{2} w^\\top \\Phi^\\top \\Phi w + \\beta w^\\top \\Phi^\\top t + constant$  \n",
    "$\\implies q(w) = \\mathcal{N}(m_N, S_N)$\n",
    "    * the quadratic term is $E[\\alpha] I + \\beta \\Phi^\\top \\Phi$  \n",
    "    $\\implies S_N = (E_{q(\\alpha)}[\\alpha] I + \\beta \\Phi^\\top \\Phi)^{-1}$\n",
    "    * $m_N = \\beta S_N \\Phi^\\top t$\n",
    "    * $E[\\alpha] = \\frac{a_N}{b_N} = \\frac{a_0 + d / 2}{b_0 + \\frac{1}{2} (||m_N||^2 + tr(S_N))}$\n",
    "* so we get an alternating optimization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering with constraints\n",
    "\n",
    "**motivation**\n",
    "\n",
    "* difficult clustering problem\n",
    "* but we have some affinity information (e.g., whether some pairs of points belong in the same cluster or not\n",
    "* \"must link\" vs \"do not link\" constraints\n",
    "* alternatively, we might have soft constraints\n",
    "* penalized probabilistic clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**penalized probabilistic clustering**\n",
    "\n",
    "* GMM: $L = \\prod_i \\prod_j (p_j p_i(y_i | \\theta_j))^{z_{ij}}$\n",
    "* want to impose \"must link\" or \"do not link\" constraints (soft constraints)\n",
    "* encode preferences using weights $w_{il}$\n",
    "    * $w_{il} = 0 \\implies$ no constraint\n",
    "    * $w_{il} > 0 \\implies$ prefer to link $i$ and $l$\n",
    "    * $w_{il} < 0 \\implies$ prefer to put $i$ and $l$ in separate clusters\n",
    "    * $w_{il}$ given a priori\n",
    "* $\\prod_i \\prod_{l \\neq i} e^{w_{il} \\delta(z_i, z_l)}$  \n",
    "$= \\exp(\\sum_i \\sum_{l \\neq i} \\sum_j z_{ij} z_{il} w_{il})$\n",
    "* $p(z) = (\\prod_i \\prod_j p_j^{z_{ij}})(\\exp(\\sum_i \\sum_{l \\neq i} \\sum_j z_{ij} z_{il} w_{il}))$ (not normalized)\n",
    "* $\\Omega = \\sum_z p(z)$ normalizating coefficient\n",
    "* complete data likelihood $L = \\Omega^{-1} \\prod_i \\prod_j (p_j p(y_i | \\theta_j))^{z_{ij}} \\exp(\\sum_i \\sum_l \\sum_j z_{ij} z_{lj} w_{il})$\n",
    "    * $\\alpha_{ij} = \\log p_j p(y_i \\theta_j)$\n",
    "* $\\log L = -\\log \\Omega + \\sum_i \\sum_j z_{ij} \\alpha_{ij} + \\sum_i \\sum_{l \\neq i} \\sum_j z_{ij} z_{lj} w_{il}$\n",
    "    * $z_{ij}$ and $z_{lj}$ are correlated in the true posterior (not separable)\n",
    "* mean field variational approximation $q(z) = \\prod_i^N q(z_i) = \\prod_i \\prod_j q_{ij}^{z_{ij}}$\n",
    "    * implies each $i$ has its own parameters for variational distribution\n",
    "    * but those are integrated out\n",
    "* E-step: compute new approximation $q(z)$\n",
    "* can calculate ELBO directly and optimize\n",
    "* alternatively can use general solutionof mean field\n",
    "* $g(z_i) = E_{\\prod_{l \\neq i} q_{z_l}}[\\log L]$  \n",
    "$= constant + E[\\sum_j z_{ij} \\alpha_{ij}] + E[2 \\sum_{l \\neq i} \\sum_j z_{ij} z_{lj} w_{il}]$  \n",
    "$= constant + \\sum_j z_{ij} \\alpha_{ij} + 2 \\sum_{l \\neq i} \\sum_j z_{ij} q_{lj} w_{il}$\n",
    "    * if $z_i = j$ then $z_{ij} = 1$ and $z_{ik} = 0$ $\\forall k \\neq j$  \n",
    "    then $g(z_i = j) = constant + \\alpha_j + 2 \\sum_{l \\neq i} q_{lj} w_{il}$  \n",
    "    weighted sum of $w_{il}$ constraints weighted by $q_{lj}$\n",
    "    * $q_{ij} = q(z_i = j) \\propto \\exp(\\alpha_{ij} + 2 \\sum_{l \\neq i} q_{lj} w_{il})$  \n",
    "    $\\propto p_j p(y_i | \\theta_j) \\exp( \\sum_{l \\neq i} q_{lj} w_{il})$\n",
    "* then the E-step becomes:\n",
    "    * init $q_{ij}$'s\n",
    "    * for $i = 1, ..., N$, calculate $q_{ij}$ for $j = 1, ..., K$ using  \n",
    "    $q_{ij} \\propto p_j p(y_i | \\theta_j) \\exp(\\sum_{l \\neq i} q_{lj} w_{il})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* alternatively, the direct derivation of the E-step:  \n",
    "    * $q(z) = \\prod_i q(z_i) = \\prod_i \\prod_j q_{ij}^{z_{ij}}$\n",
    "    * ELBO $= E_{q(z)}[\\log \\frac{p(y, z)}{q(z)}]$  \n",
    "    $= E[-\\log \\Omega + \\sum_i \\sum_j z_{ij} \\alpha_{ij} + \\sum_i \\sum_l \\sum_j z_{ij} z_{lj} w_{il} - \\sum_i \\sum_j z_{ij} \\log q_{ij}]$  \n",
    "    $= -\\log \\Omega + \\sum_i \\sum_j q_{ij} \\alpha_{ij} + \\sum_i \\sum_l \\sum_j q_{ij} q_{lj} w_{il} - \\sum_i \\sum_j q_{ij} \\log q_{ij}$\n",
    "    * optimize ELBO w.r.t. $q(z_i)$, i.e. $\\{q_{ij}\\}_{j=1}^K$\n",
    "        * we have constraint $\\sum_j q_{ij} = 1$\n",
    "    * $Lagrangian = ELBO + \\lambda_i (\\sum_j q_{ij} - 1)$\n",
    "    * $\\partial_{q_{ab}} L = \\alpha_{ab} + \\sum_l q_{lb} w_{al} + \\sum_i q_{ib} w_{ia} - \\log q_{ab} + \\frac{q_{ab}}{q_{ab}} + \\lambda_a$  \n",
    "    $= constant + \\alpha_{ab} + 2 \\sum_{l \\neq a} q_{lb} w_{al} - \\log q_{ab}$  \n",
    "    $\\implies \\log q_{ab} = constant + \\alpha_{ab} + 2 \\sum_{l \\neq a} q_{lb} w_{al}$  \n",
    "    $\\implies q_{ab} \\propto e^{\\alpha_{ab}} e^{2 \\sum_l q_{lb} w_{al}}$  \n",
    "    $\\propto p_b p(y_a | \\theta_b) e^{2 \\sum_l q_{lb} w_{al}}$\n",
    "    * then $q_{ij} \\propto p_j p(y_i | \\theta_j) e^{2\\sum_{l \\neq i} q_{lj} w_{il}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* M-step: maximize w.r.t. $\\theta_j$'s and $p_j$'s\n",
    "    * recall $ELBO = -\\log \\Omega + \\sum_i \\sum_j q_{ij} \\alpha_{ij} + \\sum_i \\sum_l \\sum_j q_{ij} q_{lj} w_{il} - \\sum_i \\sum_j q_{ij} \\log q_{ij}$\n",
    "    * last two terms don't contain any model parameters and can be ignored when optimizing\n",
    "    * $\\Omega$ and $\\alpha$ contain model parameters\n",
    "    * then $ELBO = -\\log \\sum_z p(z) + \\sum_i \\sum_j q_{ij} \\log(_j p(y_i | \\theta_j)) + constant$\n",
    "    * M-step for $\\mu_j$ and $\\Sigma_j$ identical to GMM except $q_{ij}$ instead of $\\gamma_{ij}$, since we can ignore $\\Omega$\n",
    "        * $\\mu_j = \\frac{\\sum_i q_{ij} y_i}{\\sum_i q_{ij}}$\n",
    "        * $\\Sigma_j = \\frac{1}{\\sum_i q_{ij}} \\sum_i q_{ij} (y_i - \\mu_j) (y_i - \\mu_j)^\\top$\n",
    "    * M-step for $p_j$'s is difficult\n",
    "        * relaxed/inaccurate solution assumes $\\partial_{p_j} \\Omega = 0$ which yields the same result as in GMM: $p_j = \\frac{\\sum_i q_{ij}}{N}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
