{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Inference\n",
    "\n",
    "### Motivation\n",
    "\n",
    "**e.g.** (bayesian) logistic regression\n",
    "\n",
    "* can't obtain exact posterior\n",
    "* possible solution: laplace approximation\n",
    "\n",
    "**e.g.** LDA\n",
    "\n",
    "* use MCMC to sample from posterior\n",
    "    * slow\n",
    "\n",
    "**e.g.** EM algorithm\n",
    "\n",
    "* requires calculation of expectation\n",
    "    * sometimes not possible to calculate analytically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convex Duality\n",
    "\n",
    "**outline**\n",
    "\n",
    "* suppose $f(x)$ is concave\n",
    "* find a linear function that captures the upper bound of $f(x)$\n",
    "* $f(x) = \\min_\\lambda \\{\\lambda^\\top x - f^*(\\lambda)\\}$\n",
    "    * $f^*(\\lambda) = \\min_x |\\lambda^\\top x - f(x)|$\n",
    "* $f(x) \\leq \\lambda^\\top x - f^*(\\lambda)$ (by definition of minimum)\n",
    "\n",
    "**e.g.** $f(x) = \\log x$\n",
    "\n",
    "* already know this is concave\n",
    "* $f^*(\\lambda) = \\min_x \\{\\lambda x - \\log x\\}$\n",
    "* $f'(x) = -\\lambda - x^{-1}$  \n",
    "$\\implies x = \\lambda^{-1}$\n",
    "* then $f^*(\\lambda) = 1 + \\log \\lambda$\n",
    "* $\\implies f(x) = \\min_\\lambda \\{\\lambda x - (1 + \\log \\lambda)\\}$\n",
    "    * i.e., $f(x) \\leq \\lambda x - (1 + \\log \\lambda)$\n",
    "    \n",
    "**e.g.** $f(x) = \\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "* not concave (or convex for that matter)\n",
    "* $g(x) = \\log f(x) = -\\log (1 + e^{-x})$ is concave\n",
    "    * \"$f(x)$ is log concave\"\n",
    "    * can verify concaveness with the second derivative  \n",
    "    $g''(x) = \\frac{-e^x}{(1 + e^x)^2} \\geq 0$\n",
    "* $g^*(\\lambda) = \\min_x \\{\\lambda x + \\log (1 + e^{-x}) \\}$\n",
    "* $g'(x) = \\lambda - \\frac{e^{-x}}{1 + e^{-x}}$  \n",
    "$\\implies \\lambda = \\frac{e^{-x}}{1 + e^{-x}}$  \n",
    "$\\implies x = \\log \\frac{1 - \\lambda}{\\lambda}$  \n",
    "$\\implies 1 + e^{-x} = \\frac{1}{1 - \\lambda}$  \n",
    "$\\implies g^*(\\lambda) = \\lambda \\log \\frac{1 - \\lambda}{\\lambda} + \\log \\frac{1}{1 - \\lambda}$  \n",
    "$= \\lambda \\log (1 - \\lambda) - \\lambda \\log \\lambda - \\log (1 - \\lambda)$  \n",
    "$= -\\lambda \\log \\lambda - (1 - \\lambda) \\log (1 - \\lambda)$\n",
    "* entropy function $H(\\lambda)$\n",
    "* $g(x) \\leq \\lambda x - H(\\lambda)$\n",
    "* $f(x) = e^g(x) \\leq e^{\\lambda x - H(\\lambda)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using convex duality bounds in machine learning\n",
    "\n",
    "**local variational methods**\n",
    "\n",
    "* likelihood function $L = f_1 f_2 \\cdots$  \n",
    "$\\leq f_1 g_2 g_3 f_4 \\cdots$ (replace some of the $f$'s with approximations)\n",
    "    * not the same as the likelihood, but hopefully the bounds are close enough\n",
    "    * a linear approximation is okay locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global/block variational approximation\n",
    "\n",
    "* if we can't calculate posterior $p(z | data)$, \n",
    "what is the best replacement/approximation?\n",
    "* find $q(z)$ s.t. $distance(q(z), p(z | data)$ is minimized\n",
    "    * need some measure of distance/divergence\n",
    "    * one possibility: KL divergence $d_{KL}(p_1(z), p_2(z)) = E_1[\\log \\frac{p_1(z)}{p_2(z)}]$\n",
    "        * can minimize this over $q(z)$ (variational) or expectation propagation (KL divergence is not symmetric)\n",
    "        * for this class use $\\min_{q(z)} d_{KL}(q, p)$\n",
    "            * other way around is more difficult since we don't have $p(z)$\n",
    "            * if $p$ is multimodal, the variational approach will choose one mode\n",
    "            * if $p$ is multimodal, the EP approach will average the modes (although we still need $p$ for direct variational approach)\n",
    "            * either way, there's a compromise for multimodal $p$\n",
    "* $\\log p(y) = E_{q(z)}[\\log \\frac{q(z)}{p(z | y)}] + E_{q(z)}[\\log \\frac{p(y, z)}{q(z)}]$\n",
    "    * *proof*: the above rhs is  \n",
    "    $= E_{q(z)}[\\log \\frac{q(z) p(y) p(z|y)}{p(z|y) p(z)}$  \n",
    "    $= E_{q(z)}[\\log p(y)] = \\log p(y)$\n",
    "* then $\\log p(y) = d_{KL}(q(z) p(z|y)) + L$ ($L$ is the lower bound)\n",
    "    * $p(y)$ doesn't depend on our parameter $z$, so we can treat it as constant\n",
    "    * minimizing the KL-divergence is equivalent to maximizing $L$, the variational lower bound (also called evidence lower bound, ELBO)\n",
    "* $\\log p(y) \\leq L$\n",
    "* $\\log p(y) = \\log \\int p(z) p(y|z) dz$ \n",
    "$= \\log \\int q(z) \\frac{p(z)}{q(z)} p(y|z) dz$\n",
    "$\\geq \\int q(z) \\log \\frac{p(z) p(y|z)}{q(z)} dz$\n",
    "$= E_{q(z)} [\\log \\frac{p(y, z)}{q(z)}] = L$\n",
    "* alternative and equivalent form of ELBO:  \n",
    "$ELBO = E_{q(z)}[\\log \\frac{p(y, z)}{q(z)}]$\n",
    "$= E_{q(z)}[\\log p(y|z)] + E_{q(z)}[\\log \\frac{p(z)}{q(z)}]$\n",
    "$= E_{q(z)}[\\log p(y|z)] - d_{KL}(q(z), p(z))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection between global and local variational methods\n",
    "\n",
    "* global approximation can be derived from convex duality\n",
    "* for this part, $z$ is discrete\n",
    "* consider $\\log p(y) = \\log \\sum_z p(y, z) = \\log \\sum_z e^{\\log p(y, z)}$\n",
    "    * let $x$ be a vector indexed by $z$ with entries $\\log p(y, z)$\n",
    "    * $\\log p(y) = f(x) = \\log \\sum_z e^{\\log p(y, z)} = \\log \\sum_z e^{x_z}$\n",
    "    * $f(x)$ is convex so we can apply convex duality to minimize\n",
    "* $f(x) = \\max_\\lambda \\{\\lambda^\\top x - f^*(\\lambda)\\}$\n",
    "$= \\geq \\lambda^\\top x - f^*(\\lambda)$\n",
    "$= \\sum_z q(z) \\log p(y, z) - f^*([q_z])$\n",
    "    * $\\lambda$ is a vector indexed by $z$ with values $q(z) = q_z$\n",
    "    * $f^*(\\lambda) = \\max_z \\{\\lambda^\\top x - f(x)\\}$\n",
    "    $= \\max_{p(y, z)} \\{\\sum_z q(z) \\log p(y, z) - \\log p(y)\\}$\n",
    "    $= \\max_{p(y, z)} \\{\\sum_z q(z) \\log p(z|y)\\}$  \n",
    "    $= \\sum_z q(z) \\log q(z)$\n",
    "    * then we get $\\log p(y) = f(x) \\geq \\sum_z q(z) \\log p(y, z) - \\sum_z q(z) \\log q(z)$  \n",
    "    $= E_{q(z)} [\\log \\frac{p(y, z)}{q(z)}] = ELBO$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational EM algorithm\n",
    "\n",
    "* $p(y, z)$ parameterized by $\\theta$\n",
    "* $q(z)$ parameterized by $\\lambda$\n",
    "* then $\\log p(y) \\leq L(\\lambda, \\theta)$  \n",
    "$= E_{q(z|\\lambda}[\\log \\frac{p(y, z)|\\theta)}{q(z|\\lambda)}]$\n",
    "\n",
    "**algorithm** Variational EM\n",
    "\n",
    "* initialize $\\theta^{(0)}$, $i = 0$\n",
    "* E-step\n",
    "    * pick $q$, i.e., $\\lambda^{(i)}$ by $\\max L(\\lambda^{(i)}, \\theta^{(i)})$\n",
    "* M-step\n",
    "    * pick $\\theta^{(i+1)}$ by $\\max L(\\lambda^{(i)}, \\theta^{(i+1)}$\n",
    " \n",
    "**e.g.** $q(z)$ is not restricted  \n",
    "$\\min d_{KL}(q(z), p(z|y)) \\implies q(z) = p(z, y) = p(z, y | \\theta)$  \n",
    "then this is the implicit solution of the E-step  \n",
    "M-step: $\\max_{\\theta^{(i+1)}} E_{p(z|y, \\theta^{(i)})}[\\log p(y, z | \\theta^{(i+1)})] - E_{p(z | y, \\theta^{(i)}}[\\log p(z | y, \\theta^{(i)}]$  \n",
    "first term is exact, second term doesn't depend on $\\theta^{(i+1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean field approximation\n",
    "\n",
    "* pick $q(z)$ to maximize $L = E_{q(z)}[\\log \\frac{p(y, z)}{q(z)}]$\n",
    "* if family of $q(z)$ is not restricted, then $q(z) = p(z|y)$\n",
    "* restrict family $Q = \\{q(z)\\}$\n",
    "* how to pick $Q$?\n",
    "    * restrict form of $q(z)$, e.g., $q$ can be gaussian with different parameters (fixed form variational inference)\n",
    "    * divide $z$ into indepedndent subgroups $z_1, ..., z_K$, then $q(z) = \\prod_j q(z_j)$  (**mean field approximation**)\n",
    "    \n",
    "**General form of the solution to the mean field approximation**\n",
    "\n",
    "* consider discrete case for now\n",
    "* again, $\\log p(y) \\geq L = \\sum_z q(z) \\log \\frac{p(y, z)}{q(z)}$\n",
    "* restrict $q(z) = \\prod_j q(z_j)$\n",
    "* then we get $L = \\sum_z (\\prod_j q(z_j)) \\log p(y, z) - \\sum_z (\\prod_j q(z_j)) (\\sum_l \\log q (z_l))$  \n",
    "$= \\sum_{z_j} q(z_j) \\prod_{l \\neq j} q(z_l) \\log p(y, z) - \\sum_z (\\prod_a q(z_a)) \\log q_j (z_j) - \\sum_z (\\prod_a q(z_a) \\sum_{j \\neq l} q_l (z_l)$\n",
    "    * optimize w.r.t. $z_j$\n",
    "    * third term integrates out $z_j$, so constant w.r.t. $z_j$\n",
    "    * second term: integration w.r.t. $l \\neq j$ results in $1$, so it simplifies to $\\sum_{z_j} q(z_j) \\log q(z_j)$\n",
    "    * first term can be rewritten as  \n",
    "    $\\sum_{z_j} q(z_j) \\prod_{l \\neq j} q(z_l) \\log p(y, z)$  \n",
    "    $= \\sum_{z_j} q(z_j) \\sum_{z_{l \\neq j}} \\prod_{l \\neq j} q(z_l) \\log p(y, z)$  \n",
    "    $= \\sum_{z_j} q(z_j) E_{\\prod_{l \\neq j} q(z_l)} [\\log p(y, z)]$\n",
    "* so $L = constant + \\sum_{z_j} q(z_j) g(z_j, y) - \\sum_{z_j} q(z_j) \\log q(z_j)$\n",
    "    * where $g(z_j, y) = E_{\\prod_{l \\neq j} q(z_l)}[\\log p(y, z)]$\n",
    "* rewriting again, we get $L = constant + \\sum_{z_j} q(z_j) \\log \\frac{\\exp(g(z_j, y))}{q(z_j)}$\n",
    "* if $e^{g(z_j, y)}$ is normalized (i.e., treat as distribution), then $L = constant - d_{KL}(q(z_j), e^{g(z_j, y)})$\n",
    "* pick $q(z_j)$ to minimize divergence $\\implies q(z_j) = h(z_j)$  \n",
    "$\\implies q(z_j) \\propto e^{g(z_j, y)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
