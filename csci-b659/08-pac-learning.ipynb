{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAC Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recall from first lecture\n",
    "\n",
    "* classification problems  \n",
    "$loss = \\begin{cases} 1 & \\text{prediction is wrong} \\\\ 0 & \\text{else} \\end{cases}$\n",
    "\n",
    "* class of hyp/predictors $H$\n",
    "\n",
    "* distribution $D$ over $(x_i, t_i)$  \n",
    "$t_i \\in \\{-, +\\}$\n",
    "\n",
    "* realizability  \n",
    "$\\exists h \\in H$ s.t. $L_D(h) = E_{(x, y) \\in D}[h(x) \\neq y] = 0$\n",
    "\n",
    "* sample $s \\sim D^{(m)}$, i.e., iid from $D$\n",
    "\n",
    "**theorem** (imprecise statement)  \n",
    "if sample size $m$ is large enough,  \n",
    "then w.p. $1 - \\delta$, $L_D(ERM(s)) \\leq \\epsilon$\n",
    "\n",
    "ERM outputs $h \\in H$ with minimum training error  \n",
    "in our case, outputs $h$ with zero error due to realizability assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning intervals\n",
    "\n",
    "### min interval (naive/imprecise statement)\n",
    "\n",
    "* given $X_i \\sim D$ on the real line (maybe iid required?)\n",
    "* $t_i = f(x_i)$ such that $t_i \\in \\{-1, 1\\}$\n",
    "* all $+$ examples are within some interval, all $-$ examples are outside the interval\n",
    "* then the min interval is $[a, b]$,  \n",
    "where $a = \\arg\\min_i x_i s.t. t_i = 1$  \n",
    "and $b = \\arg\\max_i x_i s.t. t_i = -1$\n",
    "* note that for some true interval $[c, d]$, $a \\geq c$ and $b \\leq d$\n",
    "* **claim**: if min interval uses \"large enough\" sample size $m$, then w.p. $\\geq 1 - \\delta$, the error of $[a, b]$ is $\\leq \\epsilon$  \n",
    "where the error of the interval is the probability of $X_i$ landing in $[a, b] \\setminus [c, d]$\n",
    "* *proof*\n",
    "    * define $c^+ \\geq c$ such that $P(X \\in [c, c^+]) \\leq \\epsilon / 2$\n",
    "    * define $d^- \\leq d$ s.t. $P(X \\in [d^-, d]) \\leq \\epsilon / 2$\n",
    "    * then $c^+$ and $d^-$ don't actually depend on $a, b$ but only on $c, d$ and $D$\n",
    "    * \"good example\": sample includes at least one point in $[c, c^+]$ and one in $[d^-, d]$  \n",
    "    then $a \\leq c^+$ and $b \\geq d^-$\n",
    "    * then if GE holds, the error of $[a, b]$ is $\\leq \\epsilon$  \n",
    "    $err([a, b]) = P([c, a]) + P([b, d]) \\leq P([c, c^+]) + P([d^-, d]) \\leq \\epsilon$\n",
    "    * then with probability $\\geq 1 - \\delta$, GE holds  \n",
    "    $P($ no samples in $[c, c^+]) \\leq (1 - \\epsilon / 2)^m \\leq e^{-\\epsilon m / 2}$  \n",
    "    we need this to be $\\leq \\delta / 2 \\implies m \\geq \\frac{2}{\\epsilon} \\log \\frac{2}{\\delta}$  \n",
    "    * then if $m(\\epsilon, \\delta)$ is large, GE holds (also similar procedure for upper bound)\n",
    "        * (recall $1 - x \\leq e^{-x}$)\n",
    "* what can we hope for\n",
    "    * if \"true function\" that assigns labels to data is not represented in $H$\n",
    "* e.g., using linear classifier but true is quadratic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### agnostic PAC learning\n",
    "\n",
    "* PAC: \"probably approximately correct\"\n",
    "\n",
    "* algorithm $A$ agnostically PAC learns class $H$ if for any target function labeling, e.g., if $A$ uses $m \\geq f(\\delta, \\epsilon)$ examples sampled iid from $D$, then with probability $\\geq 1 - \\delta$, $A$ outputs $h$ such that $err(h) \\leq err(h^*) + \\epsilon$ where $h^* = \\arg\\min_{h \\in H} err(h)$\n",
    "    * $f(\\delta, \\epsilon)$ is some polynomial in $(\\frac{1}{\\delta}, \\frac{1}{\\epsilon})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**claim**: the ERM algorithm agnostically PAC learns $H$ if $m \\geq (*)$ \n",
    "\n",
    "* note: ERM finds $h \\in H$ which minimizes training set error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chernoff/hoeffding bounds\n",
    "\n",
    "* estimating $p = P(H)$\n",
    "* $\\hat{p} = \\frac{1}{n} \\sum x_i$ where $x_i$ is 1 if heads (0 if tails) from iid\n",
    "* $P(|p - \\hat{p}| > \\alpha) = 2 e^{-2 n \\alpha^2}$\n",
    "\n",
    "* more generally, $X_i \\in [a, b]$ s.t. $E[X_i] = \\mu$\n",
    "    * $\\hat{\\mu} = \\bar{X}_i$\n",
    "    * then $P(|\\mu - \\hat{\\mu}| > \\alpha) \\leq 2 e^{-2 n \\alpha^2 / (b - a)^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*proof*\n",
    "\n",
    "* thought to consider: what would guarantee $err(ERM(S)) \\leq err(h^*) + \\epsilon$?\n",
    "    * what we know: ERM picks model with minimum training set error  \n",
    "    $\\hat{err}(h) = \\frac{1}{n} \\sum_i I(h(x_i) \\neq t_i)$, training set error of $h$  \n",
    "    (note that $err(h)$ is the population error of $h$)\n",
    "\n",
    "* \"good event\": $\\forall h \\in H$, $|err(h) - \\hat{err}(h)| \\leq \\epsilon / 2$\n",
    "    * $err(h) = P(h(X) \\neq t)$\n",
    "* lemma 1: if GE holds, then $err(ERM(S)) \\leq err(h^*) + \\epsilon$\n",
    "\n",
    "    * let $\\hat{h} = ERM(S)$\n",
    "    * then $err(\\hat{h}) \\leq \\hat{err}(\\hat{h}) + \\epsilon / 2$ (GE)  \n",
    "    $\\leq \\hat{err}(h^*) + \\epsilon / 2$ (ERM)  \n",
    "    $\\leq err(h^*) + \\epsilon / 2 + \\epsilon / 2$  \n",
    "    $= err(h^*) + \\epsilon$\n",
    "* lemma 2: w.p. $\\geq 1 - \\delta$, $\\forall h \\in H$, $|err(h) - \\hat{err}(h)| \\leq \\epsilon / 2$\n",
    "    * let $e_i = 1$ if $h(x_i) \\neq t_i$ and $0$ otherwise\n",
    "    * $P(e_i = 1) = P(h(X_i) \\neq t_i) = err(h)$\n",
    "    * fix one hypothesis $h$ and take a random sample $S \\sim D^n$  \n",
    "    $P(|err(h) - \\hat{err}(h)| \\geq \\epsilon / 2) \\leq 2 \\exp(-n \\epsilon^2 / 2)$  \n",
    "    we want this to be $\\leq \\delta / |H|$\n",
    "        * to guarantee this, need $n \\epsilon^2 / 2 \\geq \\log \\frac{2 |H|}{\\delta}$  \n",
    "        $\\implies n \\geq \\frac{2}{\\epsilon^2} \\log \\frac{2 |H|}{\\delta}$\n",
    "    * $P(\\text{GE does not occur})$  \n",
    "    $= P(|\\hat{err}(h) - err(h)| > \\epsilon / 2$ $\\forall h \\in H)$  \n",
    "    $\\leq \\sum_h P(|\\hat{err}(h) - err(h)| > \\epsilon / 2)$  \n",
    "    $\\leq \\sum_h \\frac{\\delta}{|H|} = \\delta$\n",
    "    \n",
    "* $P(|err(f(S)) - \\hat{err}(f(S))| \\geq \\epsilon / 2)$  \n",
    "realizable case: $P(\\hat{err}(ERM(S)) = 0) = 1$ by definition/assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from classification to any type of prediction (e.g., regression)\n",
    "\n",
    "recall for (one type of) classification:\n",
    "\n",
    "* $l_{0-1}(t, \\hat{t}) = \\begin{cases} 1 & t \\neq \\hat{t} \\\\ 0 & else \\end{cases}$\n",
    "* $err(h) = E[l_{0, 1}(t, h(x))]$\n",
    "* let $risk(h) = E[l(t, h(x))]$, the expected loss\n",
    "\n",
    "for regression\n",
    "\n",
    "* typically $l_2(t, \\hat{t}) = (t - \\hat{t})^2$\n",
    "* $\\hat{t} = w^\\top x$\n",
    "* $H = \\{w\\}$, the possible choices (domain?) of $w$\n",
    "* $risk(w) = E[l(t, w^\\top x)]$\n",
    "\n",
    "log loss\n",
    "\n",
    "* $l(t, p_w) = -\\log p(t | x, w)$\n",
    "* $risk(w) = E[-\\log p(t | x, w)]$\n",
    "\n",
    "important caveat:  \n",
    "ERM agnostically PAC learns $H$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R(h) = E_{x, t}[l(h, (x, t))]$\n",
    "\n",
    "then $\\hat{R}(h) = N^{-1} \\sum_i l(h, (x_i, t_i))$ (which $\\stackrel{p}{\\to} R(h)$)\n",
    "\n",
    "* lemma 1 is evident from this\n",
    "\n",
    "for lemma 2, use hoeffding inequality\n",
    "\n",
    "* e.g., for square loss\n",
    "    * need to assume $|t_i| \\leq T$ and $||x_i|| \\leq r$ and $||w|| \\leq B$\n",
    "    * then $(t_i - w^\\top x_i)^2 \\leq (T + B r)^2 = B^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### markov's inequality\n",
    "\n",
    "if random variable $X \\geq 0$, then $P(X \\geq a) \\leq \\frac{E[X]}{a}$\n",
    "\n",
    "*proof*  \n",
    "\n",
    "$E[X] = \\int_0^\\infty x p(x) dx$\n",
    "$= \\int_0^a x p(x) dx + \\int_a^\\infty x p(x) dx$  \n",
    "$\\leq \\int_a^\\infty x p(x) dx$\n",
    "$\\leq \\int_a^\\infty a p(x) dx$  \n",
    "$= a P(X \\geq a)$\n",
    "\n",
    "#### chebychev's inequality\n",
    "\n",
    "let $E[Z] = \\mu$ and $Var(Z) = \\sigma^2$  \n",
    "then $P(|Z - \\mu| \\geq a) \\leq \\frac{\\sigma^2}{a^2}$\n",
    "\n",
    "*proof*\n",
    "\n",
    "start with markov's inequality  \n",
    "$P(|Z - \\mu| \\geq a) = P((Z - \\mu)^2 \\geq a^2)$\n",
    "$\\leq \\frac{E[(Z - \\mu)^2]}{a^2}$\n",
    "$= \\sigma^2 / a^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using chebychev's inequality to estimate the mean\n",
    "\n",
    "let $\\bar{X}$ be the sample mean of iid random variables with $E[X_i] = \\mu$ and $Var(X_i) = \\sigma^2$\n",
    "\n",
    "then $E[\\bar{X}] = \\mu$  \n",
    "and $Var(\\bar{X}) = \\sigma^2 / n$\n",
    "\n",
    "$P(|\\bar{X} - \\mu| > a) = \\frac{\\sigma^2}{n a^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hoeffding's inequality: if $X \\in [a, b]$, then \n",
    "$P(|\\bar{X} - \\mu| > \\alpha) \\leq 2 e^{-\\frac{2 n \\alpha^2}{(b - a)^2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chernoff: $P(|p - \\hat{p}| > \\alpha) \\leq 2 e^{-2 n \\alpha^2}$\n",
    "\n",
    "hoeffding: $P(|\\bar{X} - \\mu| > \\alpha) \\leq 2 e^{-\\frac{2 n \\alpha^2}{(b - a)^2}}$\n",
    "\n",
    "**e.g.** markov vs chernoff\n",
    "\n",
    "given \n",
    "\n",
    "* coin with $P(H) = .25$\n",
    "* $n = 200$\n",
    "* $\\hat{p} = n^{-1} \\sum_i X_i$\n",
    "\n",
    "solve for an upper bound for $P(\\hat{p} \\geq .5)$\n",
    "\n",
    "* markov: $E[X] / a = .25 / .5 = .5$\n",
    "* chernoff: this is equivalent to $P(|p - \\hat{p}| > .25) \\leq 2 e^{-2 (200) (.0625)} \\approx 3 \\times 10^{-11}$\n",
    "* can also use chebychev (upper bound of $.015$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** \n",
    "\n",
    "given two coins $p_1 = .25$, $p_2 = .5$\n",
    "\n",
    "want to identify which coin has $p = .25$\n",
    "\n",
    "one method: pick one coin, flip $n$ times, and if $\\hat{p} \\leq .375$, then it is coin 1, else it's coin 2\n",
    "\n",
    "claim: $\\forall \\delta > 0$, $\\exists N$ s.t. $n > N \\implies$ we pick the correct coin w.p. $1 - \\delta$\n",
    "\n",
    "*proof*  \n",
    "in either case, if we make the wrong choice, $|p - \\hat{p} \\geq .125$  \n",
    "$P(|p - \\hat{p} \\geq .125) \\leq 2 e^{-2 n (.125)^2} \\leq \\delta$  \n",
    "$\\implies n \\geq 32 \\log \\frac{2}{\\delta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.**\n",
    "\n",
    "given two coins with $p_1 = .25$ and unknown $p_2$\n",
    "\n",
    "want to identify coin with $P(H) \\leq .25$ (coin 1 is this, coin 2 could be this)\n",
    "\n",
    "no guarantee without knowing what $|p_1 - p_2|$ is (or characterizing it in some way)\n",
    "\n",
    "**e.g.**\n",
    "\n",
    "given two coins $p_1 = .25$, $p_2$ unknown\n",
    "\n",
    "goal: identify coin with $P(H) \\leq .375$\n",
    "\n",
    "method/algorithm: \n",
    "\n",
    "* if $p_2 < .25$, then flip both coins $n$ times and choose one with smaller $\\hat{p}$, guaranteed to choose a correct answer\n",
    "* if $p_2 \\in [.25, .375]$, same method as in previous case, guaranteed to choose a correct answer\n",
    "* if $p_2 > .375$, not guaranteed to choose correct answer if $\\hat{p}_1 > \\hat{p}_2$\n",
    "    * happens when at least one estimate is off by $\\geq (.375 - .25) / 2 = .0625$\n",
    "    * need $P(|p_1 - \\hat{p}_1| \\geq .0625) \\leq \\delta / 2$ and $P(|p_2 - \\hat{p}| \\geq .0625) \\leq \\delta / 2$\n",
    "    * $e^{-2 n (.0625)^2} \\leq \\delta / 2 \\implies n \\geq 128 \\log \\frac{2}{\\delta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**remark**: we require $n \\propto \\epsilon^{-2}$ and $n \\propto -\\log \\delta$ for this to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.**\n",
    "\n",
    "given\n",
    "\n",
    "* $k$ coins\n",
    "* known: $p_1 \\leq \\alpha$\n",
    "* unknown: everything else about the other coins\n",
    "\n",
    "goal: find coin s.t. $p_i \\leq 2 \\alpha$\n",
    "\n",
    "algorithm: \n",
    "\n",
    "* flip each coin $n$ times\n",
    "* pick coin $i = \\min_i \\hat{p}_i$\n",
    "\n",
    "claim: if $n \\geq \\frac{2}{\\alpha^2} \\log \\frac{2 k}{\\delta}$, then w.p. $\\geq 1 - \\delta$, we choose coin that satisfies the goal\n",
    "\n",
    "*proof*\n",
    "\n",
    "* lemma 1: if $\\forall i$, $|p_i - \\hat{p}_i| \\leq \\alpha / 2$, then the algorithm picks $i$ s.t. $p_i \\leq 2 \\alpha$  \n",
    "let $j$ be any coin s.t. $p_j > 2 \\alpha$  \n",
    "$\\hat{p}_j \\geq p_j - \\alpha / 2 > \\alpha + \\alpha / 2$  \n",
    "$\\hat{p}_1 \\geq p_1 + \\alpha / 2 = \\frac{3}{2} \\alpha$  \n",
    "$\\hat{p}_1 < \\hat{p}_j$  \n",
    "so we do not pick $j$\n",
    "* lemma 2: w.p. $\\leq 1 - \\delta$, $\\forall i$, $|p_i - \\hat{p}_i| \\leq \\alpha / 2$  \n",
    "$P(|\\hat{p}_i - p_i| > \\alpha / 2) \\leq 2 \\exp(-2 n \\alpha^2 / 4)$  \n",
    "$n \\geq \\frac{2}{\\alpha^2} \\log \\frac{2 k}{\\delta} \\leq \\delta / k$  \n",
    "$P( \\exists i$ s.t. $(|\\cdot| > \\alpha / 2) \\leq k \\delta / 2k = \\delta$  \n",
    "$2 \\exp(-2 n \\alpha^2 / 4) \\leq \\cdots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*proof of Hoeffding's bound*\n",
    "\n",
    "statement: $P(|\\mu - \\hat{\\mu}| > \\alpha) \\leq 2 e^{-2 n \\alpha^2 / (b - a)^2}$\n",
    "\n",
    "we will consider $x_i \\in [0, 1]$\n",
    "\n",
    "consider $P(\\hat{\\mu} \\geq \\mu + \\alpha)$\n",
    "\n",
    "claim: $\\hat{\\mu} \\geq \\mu + \\alpha \\iff e^{\\hat{\\mu}} > e^{\\mu + \\alpha}$\n",
    "$\\iff e^{n h \\hat{\\mu}} \\geq e^{n h (\\hat{\\mu} + \\alpha)}$\n",
    "\n",
    "$P(e^{n h \\hat{\\mu}} \\geq e^{n h (\\mu + \\alpha)}) \\leq \\frac{E[e^{n h \\hat{\\mu}}]}{e^{n h (\\mu + \\alpha)}}$\n",
    "$= e^{-n h \\mu - n h \\alpha} E[e^{h \\sum x_i}]$  \n",
    "$= e^{n h \\mu - n h \\alpha} E[e^{\\sum h x_i}]$  \n",
    "$= e^{n h \\mu - n h \\alpha} \\prod E[e^{h x_i}]$  \n",
    "$\\leq e^{-n h \\mu - n h \\alpha} \\prod E[1 - x_i + x_i e^h]$  \n",
    "$= e^{-n h \\mu - n h \\alpha} (1 - \\mu + \\mu e^h)^n$  \n",
    "$= e^{-n h \\mu - n h \\alpha + n \\log (1 - \\mu + \\mu e^h)}$  \n",
    "$= e^{-n h \\alpha + n (-h \\mu + \\log (1 - \\mu + \\mu e^h))}$\n",
    "\n",
    "we can show that the part in the parentheses is $L(\\mu, h) \\leq h^2 / 8$  \n",
    "$L(\\mu, h) = -h \\mu + \\log (1 - \\mu + \\mu e^h)$  \n",
    "$L(\\mu, 0) = 0$, $L'(\\mu, 0) = 0$, $L''(\\mu, h) \\leq 1 / 4$\n",
    "\n",
    "and we get $\\cdots \\leq e^{-n h \\alpha + n h^2 / 8}$  \n",
    "letting $h = 4 \\alpha$, we get  \n",
    "$e^{-n 4 \\alpha^2 + n (16 \\alpha^2) / 8}$  \n",
    "$= e^{-2 n \\alpha^2}$\n",
    "\n",
    "similar proof for $P(\\hat{\\mu} < \\mu - \\alpha) \\leq e^{-2 n \\alpha^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PAC: $\\forall D$, $\\forall \\epsilon$, $\\forall \\delta$, algorithm runs in time/sample $O(\\epsilon^{-a} \\delta^{-b})$ w.p. $\\geq 1 - \\delta$ outputs $h$ s.t. $E[l(h, (x, t))] \\leq E[l(h^*, (x, t))] + \\epsilon$ where $h^* = \\arg\\min E[l(h, (x, t))]$, the optimal solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def: \"perhaps PAC learnable\": $\\delta = 1/2$, arbitrary $D$ and $\\epsilon$\n",
    "\n",
    "claim: if $H$ is perhaps PAC learnable, then $H$ is PAC learnable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternatively, relax the claim on $\\epsilon$ and focus on $\\forall D$, $\\forall \\delta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "claim: if $H$ is weakly PAC learnable, then $H$ is PAC learnable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "* so far, finite hypotheses\n",
    "* fixed $m$ of hierarchy of classes $H_0 \\subset H_1 \\subset H_2 \\subset \\cdots$\n",
    "* pretend that learn on each separately\n",
    "    * learn on $H_j$ ERM will pick $h_j \\in H_j$  \n",
    "    $err(h_j) \\leq err(h^*) + \\sqrt{\\frac{2}{m} (\\log \\frac{|H|}{\\delta})^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\epsilon_k = \\sqrt{\\frac{2}{m} (\\log \\frac{4 |H_k}{\\delta} + 2 \\log k}$\n",
    "\n",
    "* \"good event\": if $\\forall k$ $\\forall h \\in H_k$, $|err(h) - \\hat{err}(h)| \\leq \\frac{\\epsilon_k}{2}$\n",
    "\n",
    "* structural risk minimization: pick $k$ and $\\hat{h}_k \\in H_k$ s.t. $\\hat{err}(h_k) + \\frac{\\epsilon}{2}$ is minimized\n",
    "    * training set error + penalty\n",
    "    * to run this: run ERM on each $H_k$ and compare error + penalty for each $k$\n",
    "\n",
    "proof structure\n",
    "\n",
    "* L1: if good event holds, then $err(\\hat{h}_k) \\leq err(h^*) + \\epsilon_M$\n",
    "* L2: w.p. $\\geq 1 - \\delta$, good event holds\n",
    "\n",
    "*proof of L1*\n",
    "\n",
    "$err(h^*) \\geq \\hat{err}(h^*) + \\frac{\\epsilon_M}{2} \\geq \\hat{err}(\\hat{h}_M) - \\frac{\\epsilon_M}{2}$\n",
    "$= \\hat{err}(\\hat{h}_M) + \\frac{\\epsilon_M}{2} - \\epsilon_M$  \n",
    "$\\geq \\hat{err}(\\hat{h}_k) + \\frac{\\epsilon_k}{2} - \\epsilon_M$  \n",
    "$\\geq err(\\hat{h}_K) - \\epsilon_M$  \n",
    "$\\implies err(\\hat{h}_k) \\leq err(h^*) + \\epsilon_M$\n",
    "\n",
    "*proof of L2*\n",
    "\n",
    "* fix $i$ and $h \\in H_i$\n",
    "* $P(|err(h) - \\hat{err}(h)| \\geq \\epsilon_i / 2) \\leq 2 e^{2 m (\\epsilon_i / 2)^2}$  \n",
    "$= 2 e^{-2 m \\frac{1}{4} \\frac{2}{m} (\\log \\frac{4 |H_i|}{\\delta} + 2 \\log i)}$  \n",
    "$= 2 (\\frac{\\delta}{4 |H_i|} \\frac{1}{i^2})$\n",
    "$= \\frac{\\delta}{2 |H_i|} \\frac{1}{i^2}$\n",
    "* then $P(\\exists h \\in H_i \\text{ s.t. } |\\cdot| > \\frac{\\epsilon_i}{2}) \\leq |H_i| \\frac{\\delta}{2 |H_i| i^2}$ \n",
    "$= \\frac{\\delta}{2 i^2}$\n",
    "* then $P(\\exists i, \\exists h \\in H_i \\text{ s.t. } |\\cdot| > \\epsilon_i / 2) \\leq \\frac{\\delta}{2} \\sum_i^\\infty \\frac{1}{i^2} \\leq \\delta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\forall \\epsilon, \\delta, h$, let $k(h) =$ min level s.t. $h \\in H_k$  \n",
    "$m(\\epsilon, \\delta, h) = \\frac{2}{\\epsilon^2} (\\log \\frac{4 |H_{k(h)}}{\\delta} + 2 \\log k(h))$\n",
    "\n",
    "* claim: $\\forall h \\in \\cup H_i$, if $m \\geq m(\\epsilon, \\delta, h)$  \n",
    "then $err(\\hat{h}_k) \\leq err(h) + \\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VC-Dimension\n",
    "\n",
    "* previous bounds: $\\frac{1}{\\epsilon} \\log \\frac{|H|}{\\delta}$\n",
    "    * look at $|H|$ as number of \"events\" to analyze  \n",
    "    (numbber of \"behaviors\" that $H$ can exhibit on data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathcal{X}$ set of possible examples\n",
    "\n",
    "$c$ concept/hypothesis, $c \\subset \\mathcal{X}$\n",
    "\n",
    "$2^\\mathcal{X}$ set of all subsets of $\\mathcal{X}$\n",
    "\n",
    "$\\mathcal{H}$ hypothesis class / concept class, $\\mathcal{H} \\subset 2^\\mathcal{X}$\n",
    "\n",
    "$S$ sample, $S \\subset \\mathcal{X}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\pi_c(S)$ set of behaviors that $c$ can induce on $S$\n",
    "\n",
    "$\\pi_c(m)$ largest $|\\pi_c(S)|$ for sample of size $m$\n",
    "\n",
    "**def** $c$ shatters $S \\iff |\\pi_c(S)| = 2^{|S|}$\n",
    "\n",
    "$VCD(c)$ is the largest size set that is shattered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**case**: interval\n",
    "\n",
    "claim: VCD(intervals) = 2\n",
    "\n",
    "proof (sketch): need to show that largest shattered set is of size 2\n",
    "\n",
    "* show one set of size 2 which is shattered\n",
    "* show no set of size 3 can be shattered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "significance of VCD\n",
    "\n",
    "in realizable case, $O(\\epsilon^{-1} \\log \\delta^{-1} + \\frac{d}{\\epsilon} \\log \\epsilon^{-1})$ suffices for PAC learning\n",
    "\n",
    "* $d = VCD(H)$\n",
    "\n",
    "in nonrealizable (agnostic) case, $O(\\epsilon^{-2} \\log \\frac{d}{\\delta})$\n",
    "\n",
    "$\\Omega(d / \\epsilon)$ sample size required for PAC learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**case:** union of two intervals\n",
    "\n",
    "VCD is 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**case**: rectangles in $\\mathbb{R}^2$\n",
    "\n",
    "VCD is 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conjunctions over boolean variables\n",
    "\n",
    "claim: VCD(conjuctions over $n$ variables) = $n$\n",
    "\n",
    "*proof that $VCD \\geq n$*\n",
    "\n",
    "* let $S = \\{0_i\\}$\n",
    "* for any $+/-$ assignment, can find conjunction\n",
    "* pick any $I \\subset \\{1, ..., n\\}$, e.g., $I = \\{1, 3\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linear threshold function\n",
    "\n",
    "$\\sum_{k=1}^d w_k x_k \\geq \\theta$\n",
    "\n",
    "claim: $VCD(LTF) = d + 1$\n",
    "\n",
    "claim: $VCD(NN_{s, d}) \\leq 2 (d + 1) (1 + \\log s)$\n",
    "\n",
    "* $NN_{s, d}$ is any neural net with $s$ nodes where each node is LTF with $d$ input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**def**  \n",
    "$\\phi_d(0) = \\phi_0(m) = 1$  \n",
    "$\\phi_d(m) = \\phi_d(m - 1) + \\phi_{d-1}(m-1)$\n",
    "\n",
    "**lemma 1** $\\Phi_d(m) = \\sum_{i=0}^d \\binom{m}{i}$\n",
    "\n",
    "**lemma 2** $d \\geq m \\implies \\Phi_d(m) = 2^m$, $d < m \\implies \\Phi_d(m) \\leq (e m / d)^d = O(m^d)$\n",
    "\n",
    "* *proof*\n",
    "    * *case $d \\geq m$*\n",
    "        * $\\sum_i^d \\binom{m}{i} = \\sum_i^m \\binom{m}{i} = (1 + 1)^m = 2^m$\n",
    "    * *case $d < m$*\n",
    "        * $\\sum_i^d \\binom{m}{i} = (m/d)^d (d/m)^d \\sum_i^d \\binom{m}{i}$  \n",
    "        $= (m/d)^d \\sum_i^d \\binom{m}{i} (d/m)^d$  \n",
    "        $\\leq (m/d)^d \\sum_i^d \\binom{m}{i} (d/m)^i$  \n",
    "        $< (m/d)^d \\sum_i^m \\binom{m}{i} (d/m)^i$  \n",
    "        $= (m/d)^d (1 + d/m)^m$  \n",
    "        $\\leq (m/d)^d e^{(d/m) m}$  \n",
    "        $= (e m / d)^d$\n",
    "\n",
    "**lemma 3** $VDC(H) = d \\implies \\pi_H(m) \\leq \\Phi_d(m)$\n",
    "\n",
    "* *proof*\n",
    "    * consider any sample $S$, $|\\pi_H(S)|$\n",
    "    * pick $x \\in S$ and \"remove it\"\n",
    "    * compare $\\pi_H(S)$ and $\\pi_H(S \\setminus \\{x\\})$\n",
    "    * consider subsets of $S$ that include $x$ and subsets that do not include $x$\n",
    "        * $|\\pi_H(S) = |\\pi_H(S \\setminus \\{x\\})| + |H'|$\n",
    "            * $H'$ are sets in $\\pi_H(S)$ that do not include $x$ but where version with $x$ is also in $\\pi_H(S)$\n",
    "            * $H'$ is a set of sets so it is a concept class  \n",
    "            but only includes subsets of $S \\setminus \\{x\\}$  \n",
    "            $H' = \\pi_{H'}(S \\setminus \\{x\\})$\n",
    "    * *claim*: $VCD(H') \\leq d - 1$\n",
    "        * *proof*\n",
    "            * if set $B$ is shattered by $H'$, then $B \\cup \\{x\\}$ is shattered by $H$\n",
    "    * $|\\pi_H(S)| = |\\pi_H(S \\setminus \\{x\\})| + |H'|$\n",
    "        * *proof* by induction\n",
    "            * case $m = 0$  \n",
    "            $\\phi_H(\\emptyset) = 1$, $\\phi_d(\\emptyset) = 1$\n",
    "            * case $m$  \n",
    "            $\\phi_H(m) \\leq \\phi_d(m-1) + \\phi_{d-1}(m-1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A = |err_D(h) - err_{S_1}(h)| > \\alpha$\n",
    "\n",
    "$B = |err_{S_1}(h) - err_{S_2}(h)| > \\alpha / 2$\n",
    "\n",
    "claim 1: $P(B) = P(B|A) P(A) + P(B | \\neg A) P(\\neg A) \\geq P(B|A) P(A) \\geq P(A) / 2$  \n",
    "need to show $P(B|A) \\geq 1/2$  \n",
    "assume $A$ happens  \n",
    "$|err_D(h) - err_{S_1}(h)| \\geq \\alpha$  \n",
    "$P(|err_D(h) - err_{S_2})h)| \\geq \\alpha / 2) \\leq 2 \\exp(-2 m (\\alpha^2 / 4))$, want this to be $\\leq 1/2$  \n",
    "then $m \\geq \\frac{\\log 4}{\\alpha^2}$\n",
    "\n",
    "$|err_{S_1}(h) - err_{S_2}(h)| = |err_{S_1}(h) - err_D(h) + err_D(h) - err_{S_2}(h)|$  \n",
    "$\\geq |err_{S_1}(h) - err_D(h)| - |err_D(h) - err_{S_2}(h)|$  \n",
    "$\\geq \\alpha - \\alpha / 2 = \\alpha / 2$  \n",
    "then w.p. $\\geq 1/2$, $|err_{S_1}(h) - err_{S_2}(h)| \\geq \\alpha / 2$ $\\implies B$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*proof* of claim 4\n",
    "\n",
    "$S_1, S_2, h$ are fixed  \n",
    "then we swap $i^{th}$ examples in $S_1, S_2$ w.p. $1/2$  \n",
    "let $P(\\text{error in } S_1) = 1/2$  \n",
    "$|err_{S_1}(h) - err_{S_2}(h) | \\geq \\alpha / 2$  \n",
    "$|n(H) - n(T)| \\geq \\alpha / 2 \\times m$, let $n(T) = m' - n(H)$  \n",
    "$(2 m')^{-1} (n(H) - n(T)) = (2 m')^{-1} (n(H) - (m' - n(H))) = (2 m')^{-1} (2 n(H) - m') = \\hat{p} - 1/2 = \\hat{p} - p$  \n",
    "$\\implies |p - \\hat{p}| \\geq \\alpha / 2 \\times m (2 m')^{-1} = \\frac{\\alpha}{4} \\frac{m}{m'}$  \n",
    "$P(|p - \\hat{p}| \\geq \\alpha m / (4 m')) \\geq 2 \\exp(-2 m' (\\alpha^2 / 16) (m^2 / (m')^2))$ \n",
    "$= 2 \\exp(-(\\alpha^2 / 8) m^2 / m')$ \n",
    "$\\leq 2 e^{-\\alpha^2 m / 8}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rademacher complexity\n",
    "\n",
    "$\\sup_h |L_D(h) - L_S(h)| \\leq \\epsilon$  \n",
    "$stackrel{?}{\\implies} L_{S_1}(h) - L_{S_2}(h)$\n",
    "\n",
    "$\\sup_h L_{S_1}(h) - L_{S_2}(h) = \\sup_h m^{-1} \\sum_i^m l(h, z_i) - l_h(z_i)$  \n",
    "$= \\sup_h m^{-1} \\sum_i^{2m} \\sigma_i l(h(z_i))$  \n",
    "where $\\sigma_i = +1$ for $i = 1, ..., m$ and $\\sigma_i = -1$ for $i = m+1, ..., 2m$  \n",
    "and $z_i = z_{i-m}$ for $i > m$\n",
    "\n",
    "rademacher random variable: $x = \\pm 1$ each with probability $1/2$,  \n",
    "typically denoted as $\\sigma \\sim Rademacher$\n",
    "\n",
    "for set $A \\subset \\mathbb{R}^m$,  \n",
    "$R(A) = m^{-1} E_{\\sigma \\in \\{-1, 1\\}^m} [\\sup_{a \\in A} \\sum_i^m \\sigma_i a_i]$\n",
    "\n",
    "normally, we think about $h \\in H$ but only care about losses of $h$  \n",
    "$h \\to l(h, z)$, $z = (x, y)$  \n",
    "$H = \\{h\\} \\implies F = \\{l_h(z) = l(h, z) \\mid h \\in H\\}$\n",
    "\n",
    "compose $F$ with sample $S$ $\\implies F(S)$  \n",
    "$F(S) = \\{ \\begin{bmatrix} f(z_1) & \\cdots & f(z_m) \\end{bmatrix}^\\top \\mid f \\in F\\}$\n",
    "\n",
    "then $R(F(S)) = m^{-1} E_\\sigma [\\sup_{f \\in F} \\sum \\sigma_i f(z_i)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Rep(S) = \\sup_h L_D(h) - L_S(h)$\n",
    "\n",
    "*lemma*: $E_S[Rep(S)] \\leq 2 E_S[R(F(S))]$\n",
    "\n",
    "*proof*  \n",
    "$E_S[\\sup_h L_D(h) - L_S(h)] = E_S[\\sup_h E_{S'} [L_{S'}(h) - L_S(h)]]$  \n",
    "$\\leq E_S [E_{S'} [\\sup_h L_{S'}(h) - L_S(h)]]$  \n",
    "$= E_S [E_{S'} [\\sup_h m^{-1} \\sum_i l_h(z_i') - m^{-1} \\sum_i l_h(z_i)]]$  \n",
    "$E_S [E_{S'} [E_\\sigma [\\sup_h m^{-1} \\sum \\sigma_i (l_h(z_i') - l_h(z_i))]]]$  \n",
    "$\\leq E_{S, S', \\sigma} [\\sup_h m^{-1} \\sum_i \\sigma_i l_h(z_i') + \\sup_h m^{-1} \\sum_i (-\\sigma_i) l_h(z_i)]$  \n",
    "$\\leq E_S[R(F(S)) + R(F(S))]$\n",
    "\n",
    "*cor*: $E_S[L_D(ERM(S))] \\leq L_D(h^*) + 2 E_S[R(F(S))]$\n",
    "\n",
    "*proof*  \n",
    "from lemma, $\\forall h \\in H$, $E_S[L_D(h) - L_S(h)] \\leq E_S[R(F(S))]$  \n",
    "$\\implies$ also holds for $h \\implies ERM(S)$  \n",
    "$\\implies E_S[L_D(ERM(S))] \\leq E_S[L_S(ERM(S))] + 2 E_S[R(F(S))]$  \n",
    "$\\leq E_S[L_S(h^*)] + 2 E_S[R(F(S))]$  \n",
    "$= L_D(h^*) + 2 E_S[R(F(S))]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### McDiarmid's inequality\n",
    "\n",
    "* consider function $f(\\cdot)$ of sample, e.g. $f(z_1, ..., z_m)$  \n",
    "and $|f(z_1, ... z_i, ..., z_m) - f(z_1, ..., z_i', ..., z_m)| \\leq c_i$ (\"not too sensitive\")  \n",
    "$P(f(z_1, ..., z_m) - E[f(z_1, ..., z_m) \\geq \\epsilon] \\leq e^{-2 \\epsilon^2 / \\sum c_i^2}$\n",
    "    * also true for the other side  \n",
    "    $P(E[f(\\cdots)] - f(\\cdots) > \\epsilon) \\leq e^{-2 \\epsilon^2 / \\sum c_i^2}$\n",
    "\n",
    "* Hoeffding's inequality is a special case of this\n",
    "    * $f = m^{-1} \\sum x_i$, $xi \\in [a, b]$\n",
    "    * then $c_i \\leq \\frac{b - a}{m}$\n",
    "    * $\\sum c_i^2 = \\frac{(b-a)^2}{m}$  \n",
    "    plugging this into the exponential yields $e^{-2 \\epsilon^2 m / (b-a)^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conditions for bounded losses\n",
    "\n",
    "* $-c \\leq l_h(z_i) \\leq c$\n",
    "* then $Rep(S) = \\sup_h L_D(h) - L_S(h)$\n",
    "* what happens to $Rep(S)$ when we swap one example?  \n",
    "$c_i \\leq \\frac{2 c}{m}$\n",
    "* $R(F(S)) = m^{-1} E_\\sigma \\sup_f \\sum \\sigma_i f(z_i)$\n",
    "* $\\sum c_i^2 = \\sum_i^m \\frac{4 c^2}{m^2} = \\frac{4 c^2}{m}$\n",
    "* then $e^{-2 \\epsilon^2 / \\sum c_i^2} = e^{-2 \\epsilon^2 m / (4 c^2)} = e^{-\\frac{\\epsilon^2 m}{2 c^2} < \\delta}$  \n",
    "$\\implies \\epsilon \\geq \\frac{2 c^2}{m} \\log \\frac{1}{\\delta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*theorem*: with probability at least $1 - \\delta$, \n",
    "\n",
    "1. $\\forall h$, $L_d(h) \\leq L_S(h) + 2 E_S[R(F(S))] + \\sqrt{\\frac{2 c^2}{m} \\log \\frac{1}{\\delta}}$\n",
    "\n",
    "2. $\\forall h$, $L_D(h) \\leq L_S(h)+ 2 R(F(S)) + 3 \\sqrt{\\frac{2 c^2}{m} \\log \\frac{2}{\\delta}}$\n",
    "\n",
    "3. $L_D(ERM(S)) \\leq L_D(h^*) + 2 E_S[R(F(S))] + 2 \\sqrt{\\frac{2 c^2}{m} \\log \\frac{2}{\\delta}}$\n",
    "\n",
    "4. $L_D(ERM(S)) \\leq L_D(h^*) + 2 R(F(S)) + 4 \\sqrt{\\frac{2 c^2}{m} \\log \\frac{4}{\\delta}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*proof* (of 1)  \n",
    "\n",
    "w.p. $\\geq 1 - \\delta$, $Rep(S) \\leq E[Rep(S)] + ((2 c^2 / m) \\log (1 / \\delta))^{1/2} \\leq 2 E_S[R(F(S))] + ((2 c^2 / m) \\log (1 / \\delta))^{1/2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*proof* (of 2)\n",
    "\n",
    "w.p. $\\geq 1 - \\delta / 2$, $Rep(S) \\leq E[Rep(S)] + ((2 c^2 / m) \\log (2 / \\delta))^{1/2} \\leq 2 E_S[R(F(S))] + ((2 c^2 / m) \\log (2 / \\delta))^{1/2}$  \n",
    "then w.p. $\\geq 1 - \\delta / 2$, this is $\\leq 2 R(F(S)) + 3 ((2 c^2 / m) \\log (2 / \\delta))^{1/2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*proof* (of 3)\n",
    "\n",
    "$L_D(ERM(S)) - L_D(h^*) = L_D(ERM(S)) - L_S(ERM(S)) + L_S(ERM(S)) - L_S(h^*) + L_S(h^*) - L_D(h^*)$\n",
    "\n",
    "* $L_S(ERM(S)) - L_S(h^*) \\leq 0$\n",
    "* for $L_D(ERM(S)) - L_D(h^*)$, apply part (1) with $1 - \\delta / 2$:  \n",
    "$2 E_S[R(F(S))] + ((2 c^2 / m) \\log (2 / \\delta))^{1/2}$\n",
    "* for $L_S(h^*) - L_D(h^*)$, apply Hoeffding's inequality:  \n",
    "$e^{-2 m \\alpha^2} = \\delta / 2$ where we plug in $\\alpha^2 = (2 c^2 / m) \\log (2 / \\delta) (4 c^2)^{-1}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*fact*: let $A' = \\{c A + a_0\\}$, $a \\in A \\in \\mathbb{R}^d$  \n",
    "then $R(a') = |c| R(A)$\n",
    "\n",
    "*lemma* (Massart's lemma for $R(\\cdot)$ of finite sets): $R(A) \\leq \\max_{a \\in A} ||a - \\bar{a}|| m^{-1} \\sqrt{2 \\log |A|}$  \n",
    "$\\bar{a} = |A|^{-1} \\sum_{a \\in A} a$\n",
    "\n",
    "*theorem*: if $m \\geq \\frac{4}{\\epsilon^2} (4 \\log \\phi_d(m) + 8 \\log \\frac{2}{\\delta})$,  \n",
    "then w.p. $\\geq 1 - \\delta$, $L_D(ERM(S)) \\leq L_D(h^*) + \\epsilon$  \n",
    "where the loss function is 0-1 loss\n",
    "\n",
    "*proof*: consider $h \\in H$ and their predictions on $S$, which is $\\in \\{0, 1\\}^m$  \n",
    "$\\pi_H(S)| \\leq \\pi_H(m) \\leq \\phi_d(m)$  \n",
    "then $|F(S)| \\leq \\phi_d(m)$, so it is finite  \n",
    "bound on norm is $\\sqrt{\\sum_i^m 1^2} = \\sqrt{m}$  \n",
    "$\\implies R(F(S)) \\leq \\sqrt{m} m^{-1} \\sqrt{2 \\log \\phi_d(m)}$  \n",
    "$\\implies$ w.p. $\\geq 1 - \\delta$, $L_D(ERM(S)) \\leq L_D(h^*) + \\sqrt{2 m^{-1} \\log \\phi_d(m)} + 2 \\sqrt{2 m^{-1} \\log \\frac{2}{\\delta}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear functions and predictions based on them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assumptions/conditions\n",
    "\n",
    "* $||x_i|| \\leq R < \\infty$\n",
    "* $||w|| \\leq B < \\infty$\n",
    "    * also implies $||w^*|| \\leq B$\n",
    "* $F_1$ are linear functions\n",
    "    * $F_1 = \\{f = w^\\top x\\}$\n",
    "    * lemma (26.10): $R(F_1(S)) \\leq \\frac{B R}{\\sqrt{m}}$\n",
    "* $F_2$: loss function applied over linear score $w^\\top x$  \n",
    "$f = \\phi_y(w^\\top x) = l(w, (x, y))$  \n",
    "$F_2 = \\{f = \\phi_y (w^\\top x)\\}$\n",
    "    * e.g., square loss, log loss, hinge loss, 0-1 loss, ramp loss\n",
    "    * lemma (26.9): if $\\phi_y(a)$ is $\\rho$-lipschitz $\\forall y$,  \n",
    "    then $R(F_2(S)) \\leq \\frac{\\rho B R}{\\sqrt{m}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "want to derive bound $|l| \\leq c$, $|l'| \\leq \\rho$\n",
    "\n",
    "* square loss: $(w^\\top x - y)^2$\n",
    "    * need another assumption: $|y| \\leq B R$\n",
    "    * then $|l| \\leq (BR + BR)^2 = 4 B^2 R^2$\n",
    "    * $l' = 2 (w^\\top x - y) \\leq 4 B R$\n",
    "* logistic loss: $-\\log \\sigma(y a) = \\log (1 + e^{-y a})$ where $a = w^\\top x$\n",
    "    * $|-y a| < B R \\implies l \\leq \\log (1 + e^{B R})$\n",
    "    $\\leq \\log 2 e^{B R} \\leq 1 + BR$\n",
    "    * $|l'| = \\frac{e^{-y a}}{1 + e{-y a}}$\n",
    "    $\\leq \\frac{1}{1 + e^{ya}} \\leq 1$\n",
    "* hinge loss\n",
    "    * $|l| \\leq 1 + BR$\n",
    "    * $|l'| \\leq 1$\n",
    "* ramp loss: trivial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*corollary* (from theorem 3): w.p. $\\geq 1 - \\delta$, $L_D(ERM(S)) \\leq L_D(h^*) + 2 m^{-1/2} \\rho B R + 2 \\sqrt{2 m^{-1} c^2 \\log \\frac{2}{\\delta}}$  \n",
    "$\\rho = 1$, $c = 1 + BR$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consider SVM-esque algorithm: minimize hinge loss on $S$  \n",
    "$L_D^{0-1}(ERM^{hinge}(S)) \\leq L_D^{hinge}(w^*) + 2 m^{-1} BR + 2 (1 + B R) \\sqrt{2 m^{-1} \\log \\frac{2}{\\delta}}$\n",
    "\n",
    "focus on separable case $\\implies \\exists w$ s.t. the hinge loss is 0$\n",
    "\n",
    "recall for SVM $||w||^2 = \\gamma^{-2}$  \n",
    "$\\implies$ bound $\\sim R / \\gamma$  \n",
    "$\\implies$ not dependent on dimension  \n",
    "and $VCD(linear) = d + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*corollary* (from 1 and 3): \n",
    "\n",
    "* w.p. $\\geq 1 - \\delta$, $\\forall w$, $L_D(w) \\leq L_S(w) + 4 m^{-1/2} \\rho B R ||w|| + \\sqrt{2 m^{-1} c^2 \\log \\frac{2 (1 + \\log_2 ||w||)^2}{\\delta}}$\n",
    "\n",
    "* $L_D(ERM(S)) \\leq L_D(h^*) + 4 m^{-1/2} \\rho R ||ERM(S)|| + 2 \\sqrt{2 m^{-1} c^2 \\log \\frac{2 (1 + \\log_2 ||ERM(S)||)^2}{\\delta}}$\n",
    "\n",
    "*proof*\n",
    "\n",
    "let $B_i = 2^i$, $H_i = \\{w \\mid ||w|| \\leq B_i\\}$, $\\delta_i = \\frac{\\delta}{2 i^2}$  \n",
    "apply previous corollary to each $H_i$  \n",
    "fix any $w$ and let $i = \\lceil \\log_2 ||w|| \\rceil$ $\\implies w \\in H_i$  \n",
    "$\\implies B_i \\leq 2 ||w||$, \n",
    "$\\frac{1}{\\delta_i} = \\frac{2 i^2}{\\delta} \\leq \\frac{2}{\\delta} (1 + \\log_2 ||w||)^2$  \n",
    "then $\\sum \\delta_i < \\delta$, so w.p. $\\geq 1 - \\delta$, bounds $\\forall i$ hold simultaneously  \n",
    "$\\implies$ can write one unified bound $\\forall i$: $B \\leq 2 ||w||$, $\\delta_i^{-1} \\leq \\cdots$\n",
    "\n",
    "$L_D(ERM(S)) - L_D(w^*) = L_D(ERM(S)) - L_S(ERM(S)) + L_S(ERM(S)) - L_S(w^*) + L_S(w^*)$  \n",
    "$= (L_D(ERM(S)) - L_S(ERM(S))) - (L_S(ERM(S) - L_S(w^*)) + (L_S(w^*) - L_D(w^*))$\n",
    "\n",
    "* $L_S(ERM(S) - L_S(w^*) \\leq 0$\n",
    "* $L_D(ERM(S)) - L_S(ERM(S))$: use (1) with $\\delta / 2$  \n",
    "$P(L_S(\\cdot) - L_D(\\cdot) \\geq \\alpha) \\leq \\delta / 2$\n",
    "* $L_S(w^*) - L_D(w^*)$: apply Hoeffding inequality with $\\alpha = \\sqrt{2 m^{-1} c^2 \\log \\frac{2}{\\delta}}$\n",
    "\n",
    "$E_S[L_D(ERM(S)) - L_D(w^*)] \\leq 4 m^{-1/2} \\rho R ||ERM(S))|| + \\sqrt{ 2 m^{-1} c^2 \\log \\frac{4 (1 + \\log_2 ||ERM(S)||)^2}{\\delta}} + \\sqrt{2 m^{-1} c^2 \\log \\frac{2}{\\delta}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* VCD bounds are \"conservative\"/loose (possibly overestimates required sample size)\n",
    "* rademacher bounds and data dependence $\\to$ tigher bounds\n",
    "* maybe optimization algorithm constraints results and does not overfit\n",
    "* rademacher bound for NN: $R(F(S)) = O(\\frac{R \\sqrt{d} \\prod_j^d ||W_j||_F}{\\sqrt{m}})$\n",
    "    * $d$ is depth of NN\n",
    "    * $W_j$ is matrix of weights from $j-1^{th}$ layer to $j^{th}$ layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying learning theory to Bayesian algorithms\n",
    "\n",
    "**e.g.** GLM\n",
    "\n",
    "* prior: $w \\sim P(\\omega)$\n",
    "* likelihood: $\\prod_i p(t_i | w)$\n",
    "* ELBO: $\\log p(t) = \\log \\int_w p(w) p(t | w) dw$  \n",
    "$= \\log \\int_w q(w) \\frac{p(w)}{q(w)} p(t | w) dw$  \n",
    "$\\geq \\int q(w) \\log (\\frac{p(w)}{q(w)} p(t | w)) dw$  \n",
    "$= E_q[\\log p(t|w)] - d_{KL}(q(w) || p(w))$  \n",
    "$= \\sum_i E_q[\\log p(t_i | w)] - d_{KL}(q(w) || p(w))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some remarks on the ELBO\n",
    "\n",
    "* recall: $-ELBO = \\sum_i E_q[-\\log p(t_i | w)] + d_{KL}(q(w) || p(w))$\n",
    "* objective: minimize $-ELBO$  \n",
    "\"regularized cumulative loss minimization\" (RCLM)  \n",
    "$\\arg\\min_{q(w)} \\sum_i E_q[-\\log p(t_i | w)] + \\eta^{-1} d_{KL}(q(w) || p(w))$\n",
    "\n",
    "compare to MAP estimation for GLM:  \n",
    "$\\arg\\min_w \\sum_i -\\log p(t_i | w) + \\lambda ||w||^2$\n",
    "\n",
    "if $q(w) = \\mathcal{N}(m, V)$, given new observation $(x^*, t^*)$, how to predict and compute loss?\n",
    "\n",
    "* $p(t | x^*) = E_q[p(t | x^*, w)]$\n",
    "* use logloss: $l(q(w) | (x, t)) = -\\log E_q[p(t | x, w)]$\n",
    "\n",
    "mismatch in placement of $\\log$ between ELBO and logloss?\n",
    "\n",
    "$l_G(q(w) | (x, t)) = E_q [-\\log p(t | w)]$ (VI)  \n",
    "$l_B(q(w) | (x, t)) = -\\log E_q[p(t | w)]$  \n",
    "$r_G(q(w)) = E_{x, t}[l_G(q(w) | (x, t))]$  \n",
    "$r_B(q(w)) = E_{x, t}[l_B(q(w) | (x, t))]$\n",
    "$l_w(w | (x, t)) = -\\log p(t | w)$ (MAP)  \n",
    "$r_w(w) = E_{x, t}[l_w(w | (x, t))]$\n",
    "\n",
    "* want: $alg(S) = RCLM_{q(w), l, reg}(S) \\to \\hat{q}(w)$  \n",
    "variational inference: $RCLM_{q, l_g, d_{KL}(q||p)}(S)$\n",
    "* w.p. $\\geq 1 - \\delta$, $\\forall q$, $r_B(\\hat{q}) \\leq r_B(q) + \\epsilon$\n",
    "* $r_B(\\hat{q}_{l_G}(S)) \\leq \\cdots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
