{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of Machine Learning Concepts\n",
    "\n",
    "**e.g.** Linear Regression\n",
    "\n",
    "$t_i \\stackrel{iid}{\\sim} \\mathcal{N}(w^\\top \\phi(x_i), \\beta^{-1}$  \n",
    "$t \\sim \\mathcal{N}(\\Phi w, \\beta^{-1} I)$  \n",
    "$L = p(t | w)$  \n",
    "\"optimal\" $w$ maximizes $L$, $\\hat{w} = (\\Phi^\\top \\Phi)^{-1} \\Phi^\\top t$\n",
    "\n",
    "Alternatively, use prior $w \\sim \\mathcal{N}(0, \\alpha^{-1} I)$  \n",
    "Posterior $w | t \\sim \\mathcal{N}(m_N, S_N)$  \n",
    "If our beliefs are captured by the prior, the model is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** Logistic Regression\n",
    "\n",
    "$t_i \\stackrel{indep}{\\sim} Bernoulli(\\sigma(w^\\top \\phi(x_i))$  \n",
    "prior: $w \\sim \\mathcal{N}(0, \\alpha^{-1} I)$  \n",
    "$\\hat{w}_{MAP} = w^\\top w - (\\Phi^\\top diag(y (1 - y)) \\Phi)^{-1} \\Phi^\\top (y - t)$  \n",
    "$y_i = \\sigma(w^\\top \\phi(x_i))$  \n",
    "true posterior is not gaussian, requires MCMC  \n",
    "can use approximation (Laplace): $\\hat{w} \\sim \\mathcal{N}(m_N, S_N)$, $m_N = \\hat{w}_{MAP}$, $S_N = (\\alpha I + \\Phi^\\top R \\Phi)^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Framework\n",
    "\n",
    "Assumptions/rules\n",
    "\n",
    "* $(x_i, y_i) \\sim D$, $D$ is not related to my algorith/model  \n",
    "each $(x_i, y_i)$ drawn independently\n",
    "* what we care about is captured by a loss or objective function $\\ell(y, \\hat{y})$\n",
    "    * 0-1 loss for classification (whether $y$ is the same as $\\hat{y}$)\n",
    "    * log-loss $\\ell(y, p(y | model)) = -\\log p(y | model)$\n",
    "* there are a finite set of options (hypothesis class $H$)\n",
    "    * if parameter is continuous (infinite set of options), e.g., $w \\in \\mathbb{R}^d$,  \n",
    "    for now, assume each entry $w_k$ in vector $w$ is a float64 ($2^{64 d}$ options)\n",
    "* realizability assumption  \n",
    "$\\forall h \\in H$, $L_D(h) = E_{(x, y) \\sim D}[\\ell(y, h(x))]$  \n",
    "then $\\exists h^* \\in H$ s.t. $L_D(h) = 0$  \n",
    "i.e., there exists a \"true\" solution\n",
    "* learning setup/framework\n",
    "    * sample $s \\sim D^N$ (independent sample of size $N$)\n",
    "* algorithm: ERM  \n",
    "sets $s$ as input and finds $h \\in H$ that has zero error on $s$  \n",
    "$L_s(h) = N^{-1} \\sum_i \\ell(y_i, h(x_i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**theorem**\n",
    "\n",
    "if ERM is run on problem satisfying the above with sample size $N \\geq \\frac{log |H| / \\delta}{\\epsilon}$,  \n",
    "then with probability $\\geq 1 - \\delta$, $L_D(\\tilde{h}) \\leq \\epsilon$\n",
    "\n",
    "**corr**\n",
    "\n",
    "applying to linear regression,  \n",
    "$\\log |W| = 64 d \\log 2 \\leq 45 d$  \n",
    "$N \\geq \\frac{\\log(\\delta^{-1} + 45d)}{\\epsilon} \\implies L_D(\\tilde{w}) \\leq \\epsilon$ with probability $1 - \\delta$\n",
    "\n",
    "**proof**\n",
    "\n",
    "* only aiming to set small error $L_D \\leq \\epsilon$\n",
    "* $h$ is \"bad\" if $L_D(h) > \\epsilon$\n",
    "* focus on one bad hypothesis, $\\bar{h}$  \n",
    "$P(\\bar{h} \\text{ is not detected using } s)$  \n",
    "$= P(\\bar{h} \\text{ labels all examples in sample correctly})$  \n",
    "$= \\prod_i P(\\bar{h} \\text{ labels correctly } x_i)$  \n",
    "$\\leq (1 - \\epsilon)^N$  \n",
    "$\\leq e^{-\\epsilon N} \\leq e^{-\\epsilon \\frac{\\log |H| / \\delta}{\\epsilon}}$ (taylor approx)  \n",
    "$= \\delta / |H|$\n",
    "* $P(\\text{output of ERM is bad})$  \n",
    "$= P(L_D(\\tilde{h} > \\epsilon)$  \n",
    "$= P(\\exists h \\in H, L_D(H) \\geq \\epsilon, L_S(h) = 0)$  \n",
    "$\\leq |H| P(\\text{for some } \\bar{h}, L_D(\\bar{h}) > \\epsilon, L_S(\\bar{h}) > 0)$  \n",
    "$\\leq |H| \\frac{\\delta}{|H|}$  \n",
    "$= \\delta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
