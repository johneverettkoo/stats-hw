---
title: "STAT-S632"
subtitle: 'Assignment 4'
author: "John Koo"
output: pdf_document
# output: html_document
urlcolor: blue
header-includes:
- \usepackage{float}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      comment = NA, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.pos = 'H', 
                      fig.align = 'center', 
                      fig.height = 4, 
                      fig.width = 6)
options(xtable.comment = FALSE, 
        xtable.table.placement = 'H')
```

```{r packages_etc}
# packages, etc.
import::from(magrittr, `%>%`, `%<>%`, multiply_by, set_names)
dp <- loadNamespace('dplyr')
library(ggplot2)

theme_set(theme_bw())
```

# Problem 1

The optimization step for Newton-Raphson is:

$$\beta^{(t+1)} = \beta^{(t)} + I^{-1}(\beta) 
\frac{\partial \ell}{\partial \beta}$$

Since we want to find the root of $\frac{\partial \ell}{\partial \beta}$.

$I(\beta)$ is the Hessian matrix of $\ell$.

We know that $I(\beta) = -\frac{\partial^2}{\partial \beta^2} \ell(\beta)$. 
For Poisson regression, we have 

$$\ell(\beta) = 
\sum_i^n \Big( y_i x_i^T \beta - e^{x_i^T \beta} - \log(y_i!) \Big)$$

So:

$$\frac{\partial}{\partial \beta} \ell(\beta) = 
\sum_i^n \Big( y_i x_i^T - x_i^T e^{x_i^T \beta} \Big)$$

Differentiating this again with respect to $\beta$, we obtain:

$$\frac{\partial^2}{\partial \beta^2} = \sum_i^n x_i x_i^T e^{x_i \beta}$$
$$= X^T \begin{bmatrix}
  e^{x_1^T \beta} & & & \\
  & e^{x_2^T \beta} & & \\
  & & \ddots  & \\
  & & & e^{x_n^T \beta}
\end{bmatrix} X$$
$$= X^T D X$$

We can also say:

$$\frac{\partial \ell}{\partial \beta} = \sum_i y_i x_i^T -x_i^T e^{x_i^T \beta}$$
$$= \sum_i x_i^T (y_i - e^{x_i^T \beta})$$
$$= X^T (y - \mu)$$

Where $\mu \in R^n$ and $\mu_i = e^{x_i^T \beta}$.

Putting it all together, we get:

$$\beta^{(t+1)} = \beta^{(t)} + (X^T D X)^{-1} X^T (y - \mu)$$

And using the same trick as we did for binomial regression:

$$=(X^T D X)^{-1} X^T D X \beta^{(t)} + (X^T D X)^{-1} X^T D D ^{-1} (y - \mu)$$
$$= (X^T D X)^{-1} X^T D (X \beta^{(t)} + D^{-1} (y - \mu))$$

Then we can let $z = X^T \beta^{(t)} + D^{-1} (y - \mu)$ to obtain:

$$\beta^{(t+1)} = (X^T D X)^{-1} X^T D z$$

# Problem 2

[From ELM 5.1]

## Part a

```{r p2_a}
discoveries.df <- dplyr::data_frame(
  year = seq(1860, 1959), 
  n.discoveries = discoveries
)

ggplot(discoveries.df) + 
  geom_point(aes(x = year, y = n.discoveries)) + 
  stat_smooth(aes(x = year, y = n.discoveries))

ggplot(discoveries.df) + 
  geom_point(aes(x = year, y = n.discoveries)) + 
  stat_smooth(aes(x = year, y = n.discoveries), method = 'lm') + 
  scale_y_log10()
```

There doesn't appear to be any definitive trend, or at least no definitive 
linear trend. If there is a linear trend, it would be negative. 

## Part b

```{r p2_b}
constant.mod <- glm(n.discoveries ~ 1, data = discoveries.df, 
                    family = poisson)
summary(constant.mod)

discoveries.df %<>% 
  dp$mutate(n.pred = predict(constant.mod, 
                             newdata = discoveries.df, 
                             type = 'response'))

ggplot(discoveries.df) + 
  geom_point(aes(x = year, y = n.discoveries)) + 
  geom_line(aes(x = year, y = n.pred), colour = 'red') + 
  labs(y = 'number of discoveries')

mean(discoveries.df$n.discoveries)
unname(exp(constant.mod$coefficients))
```

We can see that the exponential of the coefficient of the intercept model is 
equal to the average number of discoveries.

## Part c

```{r p1_c}
pchisq(constant.mod$deviance, constant.mod$df.residual, lower.tail = FALSE)
```

Since $P(\chi^2 > 0) \approx 0$, we have evidence that the intercept model is 
a not so great fit to these data. This suggests that additional coefficients 
might be appropriate, i.e., there is some trend in the number of discoveries 
over time.

## Part d

```{r p1_d}
agg.disc.df <- discoveries.df %>% 
  dp$mutate(n.discoveries = ifelse(n.discoveries >= 8, '>=8', n.discoveries), 
            n.discoveries = factor(n.discoveries, 
                                   levels = c(as.character(seq(0, 7)), '>=8'))) %>% 
  dp$group_by(n.discoveries) %>% 
  dp$summarise(freq = n()) %>% 
  dp$ungroup()

expected.freq <- c(dpois(seq(0, 7), mean(discoveries)), 
  ppois(7, mean(discoveries), lower.tail = FALSE)) %>% 
  multiply_by(length(discoveries)) %>% 
  set_names(c(seq(0, 7), '>=8'))

agg.disc.df %<>% dp$mutate(expected.freq)
agg.disc.df

ggplot(agg.disc.df) + 
  geom_point(aes(x = n.discoveries, y = freq, colour = 'observed')) + 
  geom_point(aes(x = n.discoveries, y = expected.freq, colour = 'expected')) + 
  labs(x = 'number of discoveries', colour = NULL) + 
  scale_colour_brewer(palette = 'Set1')
```

The two do not seem to agree. The observed seem to prefer both fewer and more 
discoveries while the expected is a bit more evenly spread out.

## Part e

```{r p1_e}
chisq <- 
  sum((agg.disc.df$freq - agg.disc.df$expected.freq)^2 / agg.disc.df$expected.freq)
pchisq(chisq, nrow(agg.disc.df) - 1, lower.tail = FALSE)
```

According to the $\chi^2$-test, there is *some* evidence that this isn't from a 
Poisson distribution. 

We might suspect that if we do not group the $\geq 8$, the $p$-value would 
decrease.

## Part f

```{r p1_f}
quad.mod <- glm(n.discoveries ~ year + I(year ** 2), data = discoveries.df, 
                family = poisson)
summary(quad.mod)
```

The Wald test shows that the quadratic term is significant (i.e., not equal to 
0). We can also try a LR test:

```{r p1_f_2}
lin.mod <- glm(n.discoveries ~ year, data = discoveries.df, family = poisson)
anova(quad.mod, lin.mod, test = 'Chi')
```

And the two tests agree. 

## Part g

```{r p1_g}
discoveries.df %<>% 
  dp$mutate(n.pred = predict(quad.mod, 
                             newdata = discoveries.df, 
                             type = 'response'))

ggplot(discoveries.df) + 
  geom_point(aes(x = year, y = n.discoveries)) + 
  geom_line(aes(x = year, y = n.pred), colour = 'red') + 
  labs(y = 'number of discoveries')

ggplot(discoveries.df %>% dp$filter(n.discoveries > 0)) + 
  geom_point(aes(x = year, y = n.discoveries)) + 
  geom_line(aes(x = year, y = n.pred), colour = 'red') + 
  labs(y = 'number of discoveries') + 
  coord_trans(y = 'log10')
```

The second plot compares $X \beta$ to $x_1$, and here we should see a quadratic 
relationship.