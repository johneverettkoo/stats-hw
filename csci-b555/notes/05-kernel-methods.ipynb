{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main idea: data represented as pairwise inner products rather than as points in euclidean space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**def** kernel function\n",
    "\n",
    "$K(x_i, x_j) = \\phi(x_i)^\\top \\phi(x_j)$\n",
    "\n",
    "where $\\phi$ is some feature space\n",
    "\n",
    "we do not need to know the space, as long as it exists (certain criteria need to be met)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**def** kernel method\n",
    "\n",
    "learning algorithm that uses only the pairwise evaluations of a kernel function rather than the data directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** linear kernel\n",
    "\n",
    "$K_1(x_i, x_j) = x_i^\\top x_j$\n",
    "\n",
    "$\\phi(x) = x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** quadratic kernel\n",
    "\n",
    "$K_2(x_i, x_j) = (x_i^\\top x_j + 1)^2$\n",
    "\n",
    "to show it is a valid kernel, we need to find $\\phi$ s.t. $\\phi(x) \\in \\mathbb{R}^q$ for some $q \\in \\mathbb{N}$\n",
    "\n",
    "$K_2(x_i, x_j) = (\\sum_k x_{ik} x_{jk} + 1) (\\sum_l x_{il} x_{jl} + 1)$  \n",
    "$= \\sum_k \\sum_l x_{ik} x_{il} x_{jk} x_{jl} + \\sum_k x_{ik} x_{jk} + \\sum_l x_{il} x_{jl} + 1$\n",
    "\n",
    "then we can see that $\\phi(x_i)$ consists of $[x_{ik} x_{il}, \\cdots, x_{ik}, \\cdots, 1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** polynomial kernel\n",
    "\n",
    "$K_p(x_i, x_j) = (x_i^\\top x_j + 1)^p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** polynomial kernel of infinite dimension\n",
    "\n",
    "$K(x_i, x_j) = e^{x_i^\\top x_j}$\n",
    "\n",
    "can use taylor series to show that this is a kernel\n",
    "\n",
    "$K(x_i, x_j) = \\sum_k \\frac{1}{k!} (x_i^\\top x_j)^k$\n",
    "\n",
    "So $\\phi : \\mathbb{R}^d \\to \\mathbb{R}^\\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**theorem**\n",
    "\n",
    "let $K(\\cdot, \\cdot)$ be a kernel function with feature representation $\\phi(\\cdot)$  \n",
    "let $c \\in \\mathbb{R}+$\n",
    "\n",
    "then $c K(\\cdot, \\cdot)$ is also a kernel function and has representation function $\\sqrt{c} \\phi(\\cdot)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** RBF/gaussian/square exponential/heat kernel\n",
    "\n",
    "$K(x_i, x_j) = e^{-\\frac{|x_i - x_j|^2}{s^2}}$\n",
    "\n",
    "to show that it is a kernel, we can expand:\n",
    "\n",
    "$K(x_i, x_j) = e^{-|x_i|^2 / s^2} e^{-|x_j|^2 / s^2} e^{2 x_i^\\top x_j / s^2}$\n",
    "\n",
    "the last term is the polynomial kernel function, and then this is multiplied by terms that depend only on $x_i$ and $x_j$, so the representation function can be written as\n",
    "\n",
    "$\\phi(x) = e^{-|x|^2 / s^2} \\phi_p(x)$  \n",
    "where $\\phi_p$ is the polynomial kernel representation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**def.** online logistic regression\n",
    "\n",
    "for logistic regression, $\\partial_w \\ell = \\Phi^\\top (t - y)$  \n",
    "or for just one observation, $(t_i - y_i) \\phi(x_i)$\n",
    "\n",
    "regular gradient descent has update step $w^{(k+1)} = w^{(k)} + \\Phi^\\top (t - y^{(k)})$\n",
    "\n",
    "we can update one observation at a time and weigh the observations:\n",
    "\n",
    "$w_i^{(k+1)} = w_i^{(k)} + \\gamma_i (t_i - y_i^{(k)}) \\phi(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**def** perceptron\n",
    "\n",
    "redefine $t_i \\in \\{-1, +1\\}$\n",
    "\n",
    "initialize $w = 0$\n",
    "\n",
    "for $i \\in 1 .. n$:\n",
    "\n",
    "1. predict $\\hat{t}_i = sign(w^\\top \\phi(x_i))$\n",
    "2. if $\\hat{t}_i \\neq t$, then update $w \\leftarrow w + t_i \\phi(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**def** dual perceptron\n",
    "\n",
    "let $w = \\sum_i \\alpha_i t_i \\phi(x_i)$  \n",
    "where $\\alpha_i$ is the number of times updates were made on observation $i$\n",
    "\n",
    "then the algorithm becomes:\n",
    "\n",
    "initialize $\\alpha_i = 0$ $\\forall i \\leq n$  \n",
    "initialize $w = 0$\n",
    "\n",
    "for $i \\in 1 ..n$:\n",
    "\n",
    "1. $\\hat{t}_i = sign((\\sum_k \\alpha_k t_k \\phi(x_k))^\\top \\phi(x_i))$  \n",
    "$= sign \\bigg(\\sum_k \\alpha_k t_k K(x_k, x_i) \\bigg)$\n",
    "2. if $\\hat{t}_i \\neq t$, then update $\\alpha_i \\leftarrow \\alpha_i + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**def** nearest neighbors classifier\n",
    "\n",
    "given a training set in $\\mathbb{R}^d$, for a new point $z \\in \\mathbb{R}^d$, classify according to nearest point to $z$ in the training set\n",
    "\n",
    "note that $|x_i - z|^2 = |x_i|^2 + |z|^2 - 2 x_i^\\top z$\n",
    "$= K(x_i, x_i) + K(z, z) - 2 K(x_i^\\top z)$  \n",
    "for the linear kernel\n",
    "\n",
    "so we can redefine this as a kernel method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** regularized linear regression\n",
    "\n",
    "$\\arg\\min_w \\frac{1}{2} \\sum_i (w^\\top \\phi(x_i) - t_i)^2 + \\frac{1}{2} \\lambda |w|^2$\n",
    "\n",
    "we can solve this by taking the gradient w.r.t. $w$ and setting to 0, and we get an estimate for $w$:\n",
    "\n",
    "$0 = \\sum_i (w^\\top \\phi(x_i) - t_i) \\phi(x_i) + \\lambda w$  \n",
    "$\\implies w = -\\sum_i \\lambda^{-1} (w^\\top \\phi(x_i) - t_i) \\phi(x_i)$\n",
    "$= \\Phi^\\top a$  \n",
    "where $a = - \\lambda^{-1} (\\Phi w - t)$\n",
    "\n",
    "then we get $a = -\\frac{1}{\\lambda} (\\Phi \\Phi^\\top a - t)$  \n",
    "$\\implies a = (\\lambda I + \\Phi \\Phi^\\top)^{-1} t$\n",
    "\n",
    "then $w = \\Phi^\\top (\\lambda I + \\Phi \\Phi^\\top)^{-1} t$\n",
    "\n",
    "and $\\hat{t} = \\phi(x)^\\top w = \\phi(x) \\Phi^\\top (\\lambda I + \\Phi \\Phi^\\top)^{-1} t$  \n",
    "$= k(x)^\\top (\\lambda I + K)^{-1} t$  \n",
    "where $k(x)$ is a vector with entries $K(x, x_i)$ and $x_i$ are observations from the training set\n",
    "\n",
    "so we can just use the kernel values to compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**theorem** Mercer's theorem\n",
    "\n",
    "$K(\\cdot, \\cdot)$ is a kernel function iff for a sample $X_1, ..., X_n$, the kernel matrix $K \\in \\mathbb{R}^{n \\times n}$ where $K_{ij} = K(X_i, X_j)$, $K$ is symmetric and positive semidefinite\n",
    "\n",
    "*proof*\n",
    "\n",
    "right to left: \n",
    "\n",
    "we know that $K(x_i, x_j) = \\phi(x_i)^\\top \\phi(x_j)$  \n",
    "let $c$ be a nonzero vector in $\\mathbb{R}^n$  \n",
    "$c^\\top K c = \\sum_i \\sum_j c_i c_j K(x_i, x_j)$  \n",
    "$= \\sum_i \\sum_j c_i c_j \\phi(x_i)^\\top \\phi(x_j)$  \n",
    "$= (\\sum_i c_i \\phi(x_i)^\\top) (\\sum_j c_j \\phi(x_j))$  \n",
    "$= |\\sum_i c_i \\phi(x_i)|^2 \\geq 0$\n",
    "\n",
    "*left to right*\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**theorem** representer theorem\n",
    "\n",
    "given a optimization problem $\\arg\\min_w \\sum_i l(w^\\top \\phi(x_i), y_i) + \\lambda R(|w|)$ where $R$ is monotonic\n",
    "\n",
    "the solution has the form $\\hat{w} = \\sum_k \\gamma_k \\phi(x_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal regression\n",
    "\n",
    "labels are still called $\\{1, 2, ..., k\\}$, but we think of them as levels that can be compared\n",
    "\n",
    "parameters $\\alpha, \\phi_1, ..., \\phi_{k-1}$  \n",
    "$-\\infty = \\phi_0 < \\phi_1 < \\cdots < \\phi_{k-1} < \\phi_k = \\infty$\n",
    "\n",
    "$P(t_i = j) = \\sigma(\\alpha (\\phi_j - a_i)) - \\sigma(\\alpha(\\phi_{j-1} - a_i))$  \n",
    "if we denote $y_{ij} = \\sigma(\\alpha(\\phi_j - a_i))$, then \n",
    "$P(t_i = j) = y_{ij} - y_{i, j-1}$\n",
    "\n",
    "$a_i$ is a shifting parameter and $\\alpha$ is a scaling parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood estimation\n",
    "\n",
    "$L = \\prod_i^n \\prod_j^k (y_{ij} - y_{i, j-1})^{t_{ij}}$\n",
    "\n",
    "$\\ell = \\sum_i \\sum_j t_{ij} (\\log(y_{ij} - y_{i, j-1}))$\n",
    "\n",
    "$\\nabla_w \\ell = -\\sum_i \\sum_j t_{ij} \\frac{y_{ij} (1 - y_{ij})\\alpha \\phi(x_i) - y_{i, j-1} (1 - y_{i, j-1}) \\alpha \\phi(x_i)}{y_{ij} - y_{i, j-1}}$  \n",
    "$= \\sum_i \\sum_j t_{ij} \\phi(x_i) \\alpha (y_{ij} + y_{i, j-1} - 1)$  \n",
    "$= \\Phi^\\top d$  \n",
    "where $d_i = \\sum_j \\alpha t_{ij} (y_{ij} + y_{i, j-1} - 1)$\n",
    "$= \\alpha( y_{i, t_i} + y_{i, t_i - 1} - 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla \\times \\nabla \\ell = -\\alpha \\sum_i \\sum_j t_{ij} \\phi(x_i) (y_{ij} (1 - y_{ij}) \\alpha \\phi(x_i)^\\top + y_{i, j-1} (1 - y_{i, j-1}) \\alpha \\phi(x_i)^\\top)$  \n",
    "$= -\\alpha^2 \\sum_i \\sum_j t_{ij} (y_{ij} (1 - y_{ij}) + y_{i, j-1} (1 - y_{i, j-1})) \\phi(x_i) \\phi(x_i)^\\top$\n",
    "\n",
    "letting $r_i = -\\alpha^2 \\sum_j t_{ij} (y_{ij} (1 - y_{ij}) + y_{i, j-1} (1 - y_{i, j-1})$\n",
    "$= \\alpha (y_{i, t_i} (1 - y_{i, t_i}) + y_{i, t_i - 1} (1 - y_{i, t_i - 1})$  and $R = diag(r_1, ..., r_n)$, we can rewrite  \n",
    "$H = -\\Phi^\\top R \\Phi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian processes\n",
    "\n",
    "**def** gaussian process\n",
    "\n",
    "define a distribution over functions with the same domain in $\\mathbb{R}$:\n",
    "\n",
    "* mean function $m(x)$, $m : \\mathbb{R} \\to \\mathbb{R}$\n",
    "* covariance function $c(x_1, x_2), c : \\mathbb{R}^2 \\to \\mathbb{R}+$\n",
    "\n",
    "for a sample along the domain of the functions $\\mathbb{X} = \\{x_1, ..., x_n\\}$,  \n",
    "$f(x) = \\begin{bmatrix} f(x_1) \\\\ \\vdots \\\\ f(x_n) \\end{bmatrix} \\sim \\mathcal{N}_n \\Bigg(\\begin{bmatrix} m(x_1) \\\\ \\vdots \\\\ m(x_n) \\end{bmatrix}, C \\Bigg)$  \n",
    "where $C_{ij} = c(x_i, x_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** bayesian linear regression is a special case of gaussian processes\n",
    "\n",
    "$w \\sim \\mathcal{N}(0, \\alpha^{-1} I)$  \n",
    "$t \\mid w \\sim \\mathcal{N}(\\Phi w, \\beta^{-1} I)$  \n",
    "$t_i \\mid w \\sim \\mathcal{N}(w^\\top \\phi(x_i), \\beta^{-1})$\n",
    "\n",
    "then the marginal $t \\sim \\mathcal{N}(0, \\beta^{-1} I + \\alpha^{-1} K)$  \n",
    "where $K = \\Phi \\Phi^\\top$\n",
    "\n",
    "for linear regression, we use the linear kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** prediction via gaussian processes\n",
    "\n",
    "let $t$ be a vector of observed responses $\\begin{bmatrix} t_1 \\\\ \\vdots \\\\ t_n \\end{bmatrix}$\n",
    "\n",
    "let $\\hat{t}$ be $t_n$ with a new entry $t_{n+1}$ which is the prediction for a new set of observed features $\\phi(x_{n+1})$\n",
    "\n",
    "$\\hat{t} = \\begin{bmatrix} t_{n+1} \\\\ t_1 \\\\ \\vdots \\\\ t_n \\end{bmatrix} \\sim \\mathcal{N}(0, \\begin{bmatrix} c & v^\\top \\\\ v^\\top & C \\end{bmatrix})$\n",
    "\n",
    "where $C$ is the covariance matrix of the original sample,  \n",
    "$c = C(x_{n+1}, x_{n+1})$  \n",
    "$v = \\begin{bmatrix} c(x_1, x_{n+1}) \\\\ \\vdots \\\\ c(x_n, x_{n+1}) \\end{bmatrix}$\n",
    "\n",
    "then $t_{n+1} \\mid t \\sim \\mathcal{N}(v^\\top C^{-1} t, c - v^\\top C^{-1} v)$\n",
    "\n",
    "note that \n",
    "$v^\\top C^{-1} t = t^\\top C^{-1} v = \\sum_i \\gamma_i C(x_i, x_{n+1})$  \n",
    "where $\\gamma_i$ is the $i^{th}$ entry of $t^\\top C^{-1}$\n",
    "\n",
    "we can replace the covariance with any kernel function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection\n",
    "\n",
    "evidence has closed form $t \\sim \\mathcal{N}(0, C)$  \n",
    "can write as a function of hyperparameters  \n",
    "then to find estimates for hyperparameters, take the derivative of the evidence (or log evidence) w.r.t. the hyperparameters and set to 0\n",
    "\n",
    "**e.g.** RBF kernel\n",
    "\n",
    "* $K(x_i, x_j) = e^{-\\frac{1}{2s^2} (x_i - x_j)^2}$\n",
    "* let $C = \\beta^{-1} I + \\alpha^{-1} K$\n",
    "\n",
    "then take the derivatives w.r.t. $\\alpha$, $\\beta$, and $s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
