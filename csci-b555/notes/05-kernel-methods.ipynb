{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main idea: data represented as pairwise inner products rather than as points in euclidean space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**def** kernel function\n",
    "\n",
    "$K(x_i, x_j) = \\phi(x_i)^\\top \\phi(x_j)$\n",
    "\n",
    "where $\\phi$ is some feature space\n",
    "\n",
    "we do not need to know the space, as long as it exists (certain criteria need to be met)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**def** kernel method\n",
    "\n",
    "learning algorithm that uses only the pairwise evaluations of a kernel function rather than the data directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** linear kernel\n",
    "\n",
    "$K_1(x_i, x_j) = x_i^\\top x_j$\n",
    "\n",
    "$\\phi(x) = x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** quadratic kernel\n",
    "\n",
    "$K_2(x_i, x_j) = (x_i^\\top x_j + 1)^2$\n",
    "\n",
    "to show it is a valid kernel, we need to find $\\phi$ s.t. $\\phi(x) \\in \\mathbb{R}^q$ for some $q \\in \\mathbb{N}$\n",
    "\n",
    "$K_2(x_i, x_j) = (\\sum_k x_{ik} x_{jk} + 1) (\\sum_l x_{il} x_{jl} + 1)$  \n",
    "$= \\sum_k \\sum_l x_{ik} x_{il} x_{jk} x_{jl} + \\sum_k x_{ik} x_{jk} + \\sum_l x_{il} x_{jl} + 1$\n",
    "\n",
    "then we can see that $\\phi(x_i)$ consists of $[x_{ik} x_{il}, \\cdots, x_{ik}, \\cdots, 1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** polynomial kernel\n",
    "\n",
    "$K_p(x_i, x_j) = (x_i^\\top x_j + 1)^p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** polynomial kernel of infinite dimension\n",
    "\n",
    "$K(x_i, x_j) = e^{x_i^\\top x_j}$\n",
    "\n",
    "can use taylor series to show that this is a kernel\n",
    "\n",
    "$K(x_i, x_j) = \\sum_k \\frac{1}{k!} (x_i^\\top x_j)^k$\n",
    "\n",
    "So $\\phi : \\mathbb{R}^d \\to \\mathbb{R}^\\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**theorem**\n",
    "\n",
    "let $K(\\cdot, \\cdot)$ be a kernel function with feature representation $\\phi(\\cdot)$  \n",
    "let $c \\in \\mathbb{R}+$\n",
    "\n",
    "then $c K(\\cdot, \\cdot)$ is also a kernel function and has representation function $\\sqrt{c} \\phi(\\cdot)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** RBF/gaussian/square exponential/heat kernel\n",
    "\n",
    "$K(x_i, x_j) = e^{-\\frac{|x_i - x_j|^2}{s^2}}$\n",
    "\n",
    "to show that it is a kernel, we can expand:\n",
    "\n",
    "$K(x_i, x_j) = e^{-|x_i|^2 / s^2} e^{-|x_j|^2 / s^2} e^{2 x_i^\\top x_j / s^2}$\n",
    "\n",
    "the last term is the polynomial kernel function, and then this is multiplied by terms that depend only on $x_i$ and $x_j$, so the representation function can be written as\n",
    "\n",
    "$\\phi(x) = e^{-|x|^2 / s^2} \\phi_p(x)$  \n",
    "where $\\phi_p$ is the polynomial kernel representation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**def.** online logistic regression\n",
    "\n",
    "for logistic regression, $\\partial_w \\ell = \\Phi^\\top (t - y)$  \n",
    "or for just one observation, $(t_i - y_i) \\phi(x_i)$\n",
    "\n",
    "regular gradient descent has update step $w^{(k+1)} = w^{(k)} + \\Phi^\\top (t - y^{(k)})$\n",
    "\n",
    "we can update one observation at a time and weigh the observations:\n",
    "\n",
    "$w_i^{(k+1)} = w_i^{(k)} + \\gamma_i (t_i - y_i^{(k)}) \\phi(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**def** perceptron\n",
    "\n",
    "redefine $t_i \\in \\{-1, +1\\}$\n",
    "\n",
    "initialize $w = 0$\n",
    "\n",
    "for $i \\in 1 .. n$:\n",
    "\n",
    "1. predict $\\hat{t}_i = sign(w^\\top \\phi(x_i))$\n",
    "2. if $\\hat{t}_i \\neq t$, then update $w \\leftarrow w + t_i \\phi(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**def** dual perceptron\n",
    "\n",
    "let $w = \\sum_i \\alpha_i t_i \\phi(x_i)$  \n",
    "where $\\alpha_i$ is the number of times updates were made on observation $i$\n",
    "\n",
    "then the algorithm becomes:\n",
    "\n",
    "initialize $\\alpha_i = 0$ $\\forall i \\leq n$  \n",
    "initialize $w = 0$\n",
    "\n",
    "for $i \\in 1 ..n$:\n",
    "\n",
    "1. $\\hat{t}_i = sign((\\sum_k \\alpha_k t_k \\phi(x_k))^\\top \\phi(x_i))$  \n",
    "$= sign \\bigg(\\sum_k \\alpha_k t_k K(x_k, x_i) \\bigg)$\n",
    "2. if $\\hat{t}_i \\neq t$, then update $\\alpha_i \\leftarrow \\alpha_i + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**def** nearest neighbors classifier\n",
    "\n",
    "given a training set in $\\mathbb{R}^d$, for a new point $z \\in \\mathbb{R}^d$, classify according to nearest point to $z$ in the training set\n",
    "\n",
    "note that $|x_i - z|^2 = |x_i|^2 + |z|^2 - 2 x_i^\\top z$\n",
    "$= K(x_i, x_i) + K(z, z) - 2 K(x_i^\\top z)$  \n",
    "for the linear kernel\n",
    "\n",
    "so we can redefine this as a kernel method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** regularized linear regression\n",
    "\n",
    "$\\arg\\min_w \\frac{1}{2} \\sum_i (w^\\top \\phi(x_i) - t_i)^2 + \\frac{1}{2} \\lambda |w|^2$\n",
    "\n",
    "we can solve this by taking the gradient w.r.t. $w$ and setting to 0, and we get an estimate for $w$:\n",
    "\n",
    "$0 = \\sum_i (w^\\top \\phi(x_i) - t_i) \\phi(x_i) + \\lambda w$  \n",
    "$\\implies w = -\\sum_i \\lambda^{-1} (w^\\top \\phi(x_i) - t_i) \\phi(x_i)$\n",
    "$= \\Phi^\\top a$  \n",
    "where $a = - \\lambda^{-1} (\\Phi w - t)$\n",
    "\n",
    "then we get $a = -\\frac{1}{\\lambda} (\\Phi \\Phi^\\top a - t)$  \n",
    "$\\implies a = (\\lambda I + \\Phi \\Phi^\\top)^{-1} t$\n",
    "\n",
    "then $w = \\Phi^\\top (\\lambda I + \\Phi \\Phi^\\top)^{-1} t$\n",
    "\n",
    "and $\\hat{t} = \\phi(x)^\\top w = \\phi(x) \\Phi^\\top (\\lambda I + \\Phi \\Phi^\\top)^{-1} t$  \n",
    "$= k(x)^\\top (\\lambda I + K)^{-1} t$  \n",
    "where $k(x)$ is a vector with entries $K(x, x_i)$ and $x_i$ are observations from the training set\n",
    "\n",
    "so we can just use the kernel values to compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**theorem** Mercer's theorem\n",
    "\n",
    "$K(\\cdot, \\cdot)$ is a kernel function iff for a sample $X_1, ..., X_n$, the kernel matrix $K \\in \\mathbb{R}^{n \\times n}$ where $K_{ij} = K(X_i, X_j)$, $K$ is symmetric and positive semidefinite\n",
    "\n",
    "*proof*\n",
    "\n",
    "right to left: \n",
    "\n",
    "we know that $K(x_i, x_j) = \\phi(x_i)^\\top \\phi(x_j)$  \n",
    "let $c$ be a nonzero vector in $\\mathbb{R}^n$  \n",
    "$c^\\top K c = \\sum_i \\sum_j c_i c_j K(x_i, x_j)$  \n",
    "$= \\sum_i \\sum_j c_i c_j \\phi(x_i)^\\top \\phi(x_j)$  \n",
    "$= (\\sum_i c_i \\phi(x_i)^\\top) (\\sum_j c_j \\phi(x_j))$  \n",
    "$= |\\sum_i c_i \\phi(x_i)|^2 \\geq 0$\n",
    "\n",
    "*left to right*\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**theorem** representer theorem\n",
    "\n",
    "given a optimization problem $\\arg\\min_w \\sum_i l(w^\\top \\phi(x_i), y_i) + \\lambda R(|w|)$ where $R$ is monotonic\n",
    "\n",
    "the solution has the form $\\hat{w} = \\sum_k \\gamma_k \\phi(x_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal regression\n",
    "\n",
    "labels are still called $\\{1, 2, ..., k\\}$, but we think of them as levels that can be compared\n",
    "\n",
    "parameters $\\alpha, \\phi_1, ..., \\phi_{k-1}$  \n",
    "$-\\infty = \\phi_0 < \\phi_1 < \\cdots < \\phi_{k-1} < \\phi_k = \\infty$\n",
    "\n",
    "$P(t_i = j) = \\sigma(\\alpha (\\phi_j - a_i)) - \\sigma(\\alpha(\\phi_{j-1} - a_i))$  \n",
    "if we denote $y_{ij} = \\sigma(\\alpha(\\phi_j - a_i))$, then \n",
    "$P(t_i = j) = y_{ij} - y_{i, j-1}$\n",
    "\n",
    "$a_i$ is a shifting parameter and $\\alpha$ is a scaling parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood estimation\n",
    "\n",
    "$L = \\prod_i^n \\prod_j^k (y_{ij} - y_{i, j-1})^{t_{ij}}$\n",
    "\n",
    "$\\ell = \\sum_i \\sum_j t_{ij} (\\log(y_{ij} - y_{i, j-1}))$\n",
    "\n",
    "$\\nabla_w \\ell = -\\sum_i \\sum_j t_{ij} \\frac{y_{ij} (1 - y_{ij})\\alpha \\phi(x_i) - y_{i, j-1} (1 - y_{i, j-1}) \\alpha \\phi(x_i)}{y_{ij} - y_{i, j-1}}$  \n",
    "$= \\sum_i \\sum_j t_{ij} \\phi(x_i) \\alpha (y_{ij} + y_{i, j-1} - 1)$  \n",
    "$= \\Phi^\\top d$  \n",
    "where $d_i = \\sum_j \\alpha t_{ij} (y_{ij} + y_{i, j-1} - 1)$\n",
    "$= \\alpha( y_{i, t_i} + y_{i, t_i - 1} - 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla \\times \\nabla \\ell = -\\alpha \\sum_i \\sum_j t_{ij} \\phi(x_i) (y_{ij} (1 - y_{ij}) \\alpha \\phi(x_i)^\\top + y_{i, j-1} (1 - y_{i, j-1}) \\alpha \\phi(x_i)^\\top)$  \n",
    "$= -\\alpha^2 \\sum_i \\sum_j t_{ij} (y_{ij} (1 - y_{ij}) + y_{i, j-1} (1 - y_{i, j-1})) \\phi(x_i) \\phi(x_i)^\\top$\n",
    "\n",
    "letting $r_i = -\\alpha^2 \\sum_j t_{ij} (y_{ij} (1 - y_{ij}) + y_{i, j-1} (1 - y_{i, j-1})$\n",
    "$= \\alpha (y_{i, t_i} (1 - y_{i, t_i}) + y_{i, t_i - 1} (1 - y_{i, t_i - 1})$  and $R = diag(r_1, ..., r_n)$, we can rewrite  \n",
    "$H = -\\Phi^\\top R \\Phi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian processes\n",
    "\n",
    "**def** gaussian process\n",
    "\n",
    "define a distribution over functions with the same domain in $\\mathbb{R}$:\n",
    "\n",
    "* mean function $m(x)$, $m : \\mathbb{R} \\to \\mathbb{R}$\n",
    "* covariance function $c(x_1, x_2), c : \\mathbb{R}^2 \\to \\mathbb{R}+$\n",
    "\n",
    "for a sample along the domain of the functions $\\mathbb{X} = \\{x_1, ..., x_n\\}$,  \n",
    "$f(x) = \\begin{bmatrix} f(x_1) \\\\ \\vdots \\\\ f(x_n) \\end{bmatrix} \\sim \\mathcal{N}_n \\Bigg(\\begin{bmatrix} m(x_1) \\\\ \\vdots \\\\ m(x_n) \\end{bmatrix}, C \\Bigg)$  \n",
    "where $C_{ij} = c(x_i, x_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** bayesian linear regression is a special case of gaussian processes\n",
    "\n",
    "$w \\sim \\mathcal{N}(0, \\alpha^{-1} I)$  \n",
    "$t \\mid w \\sim \\mathcal{N}(\\Phi w, \\beta^{-1} I)$  \n",
    "$t_i \\mid w \\sim \\mathcal{N}(w^\\top \\phi(x_i), \\beta^{-1})$\n",
    "\n",
    "then the marginal $t \\sim \\mathcal{N}(0, \\beta^{-1} I + \\alpha^{-1} K)$  \n",
    "where $K = \\Phi \\Phi^\\top$\n",
    "\n",
    "for linear regression, we use the linear kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** prediction via gaussian processes\n",
    "\n",
    "let $t$ be a vector of observed responses $\\begin{bmatrix} t_1 \\\\ \\vdots \\\\ t_n \\end{bmatrix}$\n",
    "\n",
    "let $\\hat{t}$ be $t_n$ with a new entry $t_{n+1}$ which is the prediction for a new set of observed features $\\phi(x_{n+1})$\n",
    "\n",
    "$\\hat{t} = \\begin{bmatrix} t_{n+1} \\\\ t_1 \\\\ \\vdots \\\\ t_n \\end{bmatrix} \\sim \\mathcal{N}(0, \\begin{bmatrix} c & v^\\top \\\\ v^\\top & C \\end{bmatrix})$\n",
    "\n",
    "where $C$ is the covariance matrix of the original sample,  \n",
    "$c = C(x_{n+1}, x_{n+1})$  \n",
    "$v = \\begin{bmatrix} c(x_1, x_{n+1}) \\\\ \\vdots \\\\ c(x_n, x_{n+1}) \\end{bmatrix}$\n",
    "\n",
    "then $t_{n+1} \\mid t \\sim \\mathcal{N}(v^\\top C^{-1} t, c - v^\\top C^{-1} v)$\n",
    "\n",
    "note that \n",
    "$v^\\top C^{-1} t = t^\\top C^{-1} v = \\sum_i \\gamma_i C(x_i, x_{n+1})$  \n",
    "where $\\gamma_i$ is the $i^{th}$ entry of $t^\\top C^{-1}$\n",
    "\n",
    "we can replace the covariance with any kernel function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection\n",
    "\n",
    "evidence has closed form $t \\sim \\mathcal{N}(0, C)$  \n",
    "can write as a function of hyperparameters  \n",
    "then to find estimates for hyperparameters, take the derivative of the evidence (or log evidence) w.r.t. the hyperparameters and set to 0\n",
    "\n",
    "**e.g.** RBF kernel\n",
    "\n",
    "* $K(x_i, x_j) = e^{-\\frac{1}{2s^2} (x_i - x_j)^2}$\n",
    "* let $C = \\beta^{-1} I + \\alpha^{-1} K$\n",
    "\n",
    "then take the derivatives w.r.t. $\\alpha$, $\\beta$, and $s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector machines\n",
    "\n",
    "choosing the \"best\" separating hyperplane for classification\n",
    "\n",
    "**e.g.** \n",
    "\n",
    "given a training set with two classes that are linearly separable, how do we choose the best separating hyperplane?\n",
    "\n",
    "infinitely many separating hyperplanes may exist\n",
    "\n",
    "perhaps we want to choose one that maximizes the distance between the hyperplane and the points\n",
    "\n",
    "let $\\gamma_i = t_i (w^\\top \\phi(x_i) + w_0)$\n",
    "\n",
    "then we want $\\max_{w, w_0} \\min_i \\gamma_i$,  \n",
    "subject to $|w| = 1$,  \n",
    "i.e., maximize the distance from the hyperplane to the closest point\n",
    "\n",
    "$\\min_i \\gamma_i$ is the distance to the nearest point (margin)  \n",
    "so we want to maximize the margin\n",
    "\n",
    "if $w$ is a unit vector perpendicular to a hyperplane that contains the origin, then $|w^\\top \\phi(x_i)|$ is the distance form $\\phi(x_i)$ to the hyperplane (value inside the absolute value can be positive or negative, which is why we multiply by $t_i$)\n",
    "\n",
    "if we have an affline hyperplane, then we have to add by some value $w_0$ (before multiplying by $t_i$)\n",
    "\n",
    "let $w^*$ be the optimal solution and achieves value $\\gamma$ for some $i$  \n",
    "then $\\forall i$, $t_i (w^{*\\top} \\phi(x_i) + w_0) = \\gamma_i \\geq \\gamma$  \n",
    "dividing by $\\gamma$ yields $\\gamma^{-1} t_i (w^{*\\top} \\phi(x_i) + w_0) \\geq 1$  \n",
    "if we let $v = \\gamma^{-1} w^*$ and $v_0 = \\gamma^{-1} w_0$, then we get \n",
    "$t_i (v^\\top \\phi(x_i) + v_0) \\geq 1$\n",
    "\n",
    "so we can reparameterize the problem as  \n",
    "$\\min_v \\frac{1}{2} |v|^2$ subject to $t_i (v^\\top \\phi(x_i) + v_0) \\geq 1$  \n",
    "*primal form of support vector machines*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inequality constraints\n",
    "\n",
    "previously we used lagrange multipliers for optimization with equality constraints\n",
    "\n",
    "suppose we want to find $\\arg\\min_p f(p)$ subject to $h_j(p) = 0$ for $j = 1, ..., n$ and $g_k(p) \\leq 0$ for $k = 1, ..., m$\n",
    "\n",
    "then the lagrangian is $L(p) = f(p) + \\sum_j \\beta_j h_j(p) + \\sum_k \\alpha_k g_k(p)$  \n",
    "and we impose the condition $\\alpha_k \\geq 0$ $\\forall k$\n",
    "\n",
    "the *dual objective function* is $\\forall \\alpha, \\beta$, $\\theta(\\alpha, \\beta) = \\min_p L(p, \\alpha, \\beta)$  \n",
    "and the *dual optimization problem* is $\\max_{\\alpha, \\beta} \\theta(\\alpha, \\beta)$ with $\\alpha_k \\geq 0$\n",
    "\n",
    "theorems\n",
    "\n",
    "* for all points $p, \\alpha, \\beta$ in the constraint, $f(p) \\geq L(p, \\alpha, \\beta)$\n",
    "* if $f(p)$ is convex and $h_j$ and $g_k$ are linear, let $p^*$ be the solution to the original optimization problem and $\\alpha^*, \\beta^*$ be the solution to the dual problem, then $f(p^*) = \\theta(\\alpha^*, \\beta^*)$\n",
    "* the optimal solution corresponds to $\\partial_p L = 0$, $\\partial_\\beta L = 0$, $\\alpha_k \\geq 0$, and $\\alpha_k g_k(p) = 0$  \n",
    "if $\\alpha_k \\neq 0$, then $g_k(p) = 0$ (tight constraint)  \n",
    "if $g_k(p) \\neq 0$, then $\\alpha_k = 0$ ($\\alpha_k$ is redundant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.**\n",
    "\n",
    "let $f(x) = (x - 2) (x - 3) = x^2 - 5x + 6$\n",
    "\n",
    "minimize $f(x)$ s.t. $x \\geq 3 \\implies 3 - x \\leq 0$  \n",
    "then the solution is just $x = 3$\n",
    "\n",
    "$L = x^2 - 5x + 6 + \\alpha (3 - x)$  \n",
    "$\\partial_x L = 2x - 5 - \\alpha = 0 \\implies x= \\frac{5 \\alpha}{2}$  \n",
    "$\\theta(\\alpha) = \\min_x L = \\frac{(5 + \\alpha)^2}{4} - 5 \\frac{5 + \\alpha}{2} + 6 + \\alpha (3 - \\frac{5 + \\alpha}{2})$\n",
    "$= \\frac{(\\alpha - 1)^2}{4}$  \n",
    "$\\partial_\\alpha \\theta = 0 \\implies \\alpha = 1 \\implies \\theta = 0$ and $x = 6 / 2 = 3$  \n",
    "we have a tight constraint\n",
    "\n",
    "if we want to minimize $f(x)$ s.t. $x \\geq 2 \\implies 2 - x \\leq 0$  \n",
    "$L = x^2 - 5x + 6 + \\alpha (2 - x)$  \n",
    "$\\partial_x = 2x - 5 - \\alpha$, and setting this to 0 yields $x = \\frac{5 + \\alpha}{2}$  \n",
    "then $\\theta(\\alpha) = (\\frac{5 + \\alpha}{2})^2 - 5 (\\frac{5 + \\alpha}{2}) + \\alpha (2 - \\frac{5 + \\alpha}{2})$\n",
    "$= -\\frac{1}{4} (\\alpha + 1)^2$  \n",
    "maximizing this subject to $\\alpha \\geq 0$ is at $\\alpha = 0$, so $x = \\frac{5 + 0}{2} = 2.5$  \n",
    "the constraint isn't tight for this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### back to SVM\n",
    "\n",
    "for linearly separable SVMs, we have  \n",
    "$g_i(v) = 1 - t_i(v^\\top \\phi(x_i) + v_0) \\leq 0$  \n",
    "$L = \\frac{v^\\top v}{2} + \\sum_i \\alpha_i (1 - t_i (v^\\top \\phi(x_i) + v_0))$\n",
    "\n",
    "then $\\partial_v L = v - \\sum_i \\alpha_i t_i \\phi(x_i)$  \n",
    "setting this to 0 yields $v = \\sum_i \\alpha_i t_i \\phi(x_i)$\n",
    "\n",
    "and $\\partial_{v_0} L = \\sum_i \\alpha_i t_i = 0$\n",
    "\n",
    "$\\theta(\\alpha) = \\frac{1}{2} \\sum_i \\sum_k \\alpha_i t_i \\alpha_k t_k \\phi(x_i)^\\top \\phi(x_k) + \\sum_i \\alpha_i - \\sum_i \\alpha_i t_i v_0 - \\sum_i \\alpha_i t_i (\\sum_k \\alpha_k t_k \\phi(x_k))^\\top \\phi(x_i)$  \n",
    "note that $\\sum_i \\alpha_i t_i = 0$ and the first and last terms are equivalent up to a factor, so this becomes  \n",
    "$= \\sum_i \\alpha_i - \\frac{1}{2} \\sum_i \\sum_k \\alpha_i \\alpha_k t_i t_k \\phi(x_i)^\\top \\phi(x_k)$  \n",
    "$= \\sum_i \\alpha_i - \\frac{1}{2} \\sum_i \\sum_k \\alpha_i \\alpha_k t_i t_k K(x_i, x_k)$  \n",
    "we want to find $\\alpha$ that maximizes this subject to $\\alpha_i \\geq 0$, $\\sum_i \\alpha_i t_i = 0$  \n",
    "(dual form of hard margin SVM)  \n",
    "once we find the optimal $\\alpha$, we can plug into $v = \\sum_i \\alpha_i t_i \\phi(x_i)$\n",
    "\n",
    "if $\\alpha_i \\neq 0$, then $\\phi(x_i)$ is used for finding $v$ and $g_i(v) = 0$ (tight constraint)  \n",
    "if $g_i(v) \\neq 0$, then $\\alpha_i = 0$ and $\\phi(x_i)$ is not used to find $v$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### soft-margin SVM\n",
    "\n",
    "not linearly separable case\n",
    "\n",
    "constraints specified in hard-margin case violated\n",
    "\n",
    "new constraint: $t_i (v^\\top \\phi(x_i) + v_0) \\geq 1 - \\xi_i$  \n",
    "but we want $\\xi_i$ to be small, so the new objective is $\\min \\frac{1}{2} |v|^2 + c \\sum_i \\xi_i$\n",
    "\n",
    "could alternatively penalize by $c \\sum_i \\xi_i^p$, $p \\geq 1$\n",
    "\n",
    "then the lagrangian is  \n",
    "$L = \\frac{1}{2} |v|^2 + \\sum_i \\alpha_i (1 - \\xi_i - t_i(v^\\top \\phi(x_i) + v_0) - \\sum_i \\mu_i \\xi_i + c \\sum \\xi_i$  \n",
    "subject to $g_i(v) = 1 - \\xi_i - t_i(v^\\top \\phi(x_i) + v_0) \\leq 0$  \n",
    "and $\\alpha_i, \\mu_i \\geq 0$\n",
    "\n",
    "$\\partial_v L = v - \\sum_i \\alpha_i t_i \\phi(x_i)$, same as before, so $v = \\sum_i \\alpha_i t_i \\phi(x_i)$  \n",
    "$\\partial_{v_0} = \\sum_i \\alpha_i t_i = 0$, same as before  \n",
    "$\\partial_{\\xi_i} L = -\\alpha_i -\\mu_i + c = 0$\n",
    "$\\implies \\mu_i = c - \\alpha_i$, so $\\mu$ depends on $\\alpha$\n",
    "\n",
    "so $\\theta(\\alpha) = \\frac{1}{2} \\sum_i \\sum_k \\alpha_i \\alpha_k t_i t_k \\phi(x_i)^\\top \\phi(x_k) + \\sum_i \\alpha_i - \\sum_i \\alpha_i \\xi_i - \\sum_i \\alpha_i t_i v_0 - \\sum_i \\alpha_i t_i (\\sum_k \\alpha_k t_k \\phi(x_k))^\\top \\phi(x_i) - \\sum_i (c - \\alpha_i) \\xi_i + c \\sum_i \\xi_i$  \n",
    "this simplifies to the same expression as before:  \n",
    "$= \\sum_i \\alpha_i - \\frac{1}{2} \\sum_i \\sum_k \\alpha_i \\alpha_k t_i t_k K(x_i, x_k)$  \n",
    "this is the objective to maximize, but this time with constraint $0 \\leq \\alpha_k \\leq c$ and $\\sum_i \\alpha_i t_i = 0$\n",
    "\n",
    "if $t_i (v^\\top \\phi(x_i) + v_0) \\geq 1$, then $\\xi_i = 0$  \n",
    "if the quantity is less than 1, then $\\xi_i = \\max(0, 1 - t_i(v^\\top \\phi(x_i) - v_0))$  \n",
    "this is the hinge loss\n",
    "\n",
    "can rewrite the objective as  \n",
    "$\\min \\frac{v^\\top v}{2} + c \\sum_i \\max(0, 1 - t_i a_i)$  \n",
    "where $a_i = v^\\top \\phi(x_i) + v_0$  \n",
    "similar to regularized regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recall for regularized logistic regression we had  \n",
    "$\\min_w \\sum_i \\log p(t_i | w, x_i) + \\lambda |w|^2$  \n",
    "$P(t_i = 1) = \\sigma(a_i) = \\frac{1}{1 + e^{-a_i}}$  \n",
    "$P(t_i = -1) = 1 - \\sigma(a_i) = \\frac{1}{1 + e^{a_i}}$  \n",
    "so $p(t_i) = \\frac{1}{1 + e^{-t_i a_i}}$  \n",
    "and the objective is $\\min_w -\\sum_i \\log (1 + e^{-t_i a_i}) + \\lambda |w|^2$\n",
    "\n",
    "regularized linear regression is $\\min_w \\sum_i (t_i - a_i)^2 + \\lambda |w_i|^2$\n",
    "\n",
    "for SVM, # classification errors $\\leq \\sum_i \\xi_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
