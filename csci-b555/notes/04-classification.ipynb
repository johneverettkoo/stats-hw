{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Models for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "Suppose responses $y$ are of the form \"yes\" or \"no\". Say $y \\in \\{+1, -1\\}$.\n",
    "\n",
    "Suppose further that with each response there is an observable $x \\in D$, some domain.\n",
    "\n",
    "We want to come up with some $g : D \\to \\{+1, -1\\}$ that predicts $y$ from $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The generative model\n",
    "\n",
    "Suppose each response $t_i$ is independent and can take on one of $c$ classes.  \n",
    "For each $t_i$, let $x_i$ have some distribution given its corresponding $t_i$.\n",
    "\n",
    "$t_i \\stackrel{iid}{\\sim} Discrete(p_1, ..., p_c)$\n",
    "\n",
    "$X_i \\mid t_i \\sim P(x_i | \\theta_{t_i})$\n",
    "\n",
    "**case 1**: we abstract away $\\theta_k$ and the domain of $X_i$  \n",
    "**case 2**: $X_i \\in \\mathbb{R}^d$, $\\theta_k = (\\mu_k, \\Sigma_k)$, $p(x_i | \\theta_k) = \\mathcal{N}_d(x_i | \\mu_k, \\Sigma_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1\n",
    "\n",
    "we want to find $p(y = k \\mid x)$  \n",
    "$p(y = k \\mid x) = \\frac{p(y = k) p(x | y = k)}{\\sum_j p(y = j) p(x | y = j)}$\n",
    "\n",
    "let $a_k = \\log p(y = k) p(x | y = k)$  \n",
    "then $p(y = k | x) = \\frac{e^{a_k}}{\\sum_j e^{a_j}}$,\n",
    "the softmax function\n",
    "\n",
    "in the case where $k \\in \\{1, 2\\}$, we have  \n",
    "$p(y = 1 | x) = \\frac{1}{1 + e^{-(a_1 - a_2)}}$  \n",
    "$= \\frac{1}{1 + e^{-a}}$  \n",
    "where $a = a_1 - a_2$, the log odds $\\log \\frac{p(y = 1 | x)}{p(y = 2 | x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2\n",
    "\n",
    "suppose we have two classes with identical $\\Sigma_1 = \\Sigma_2 = \\Sigma$\n",
    "\n",
    "then $a = \\log p_1 - \\log p_2 - \\frac{d}{2} \\log 2 \\pi - \\frac{1}{2} \\log |\\Sigma| - \\frac{1}{2} (x - \\mu_1)^\\top \\Sigma^{-1} (x - \\mu_1) + \\frac{d}{2} \\log 2 \\pi + \\frac{1}{2} \\log |\\Sigma| + \\frac{1}{2} (x - \\mu_2)^\\top \\Sigma^{-1} (x - \\mu_2)$  \n",
    "$= \\log p_1 - \\log p_2 - \\frac{1}{2} \\mu_1^\\top \\Sigma^{-1} \\mu_1 + \\frac{1}{2} \\mu_2^\\top \\Sigma^{-1} \\mu_2 + x^\\top \\Sigma^{-1} (\\mu_1 - \\mu_2)$\n",
    "\n",
    "let $w_0$ be the terms that do not depend on $x$ and $w_1$ be the coefficient of $x$  \n",
    "then we have $w_0 + w_1^\\top x$, which is a linear function of $x$\n",
    "\n",
    "if $\\Sigma_1 \\neq \\Sigma_2$, we then have a quadratic term $-\\frac{1}{2} x^\\top (\\Sigma_1^{-1} - \\Sigma_2^{-1}) x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum likelihood estimation\n",
    "\n",
    "$L = \\prod_k \\big(\\prod_{y_i = k} p_k \\mathcal{N}(x_i | \\mu_k, \\Sigma_k) \\big)$\n",
    "\n",
    "$\\ell = \\sum_k \\sum_{y_i = k} \\log p_k + \\log \\mathcal{N}(x_i | \\mu_k, \\Sigma_k)$\n",
    "\n",
    "so we can treat each class separately and get separate MLEs for each class\n",
    "\n",
    "$\\hat{p}_k = \\frac{n_k}{n}$\n",
    "\n",
    "$\\hat{\\mu}_k = \\frac{1}{n_k} \\sum_{y_i = k} x_i$\n",
    "\n",
    "$\\hat{\\Sigma}_k =\\frac{1}{n_k} \\sum_{y_i = k} (x_i - \\hat{\\mu}_k) (x_i - \\hat{\\mu}_k)^\\top$\n",
    "\n",
    "if $\\Sigma_k = \\Sigma$ $\\forall k$, then we have the same $\\hat{p}_k$, $\\hat{\\mu}_k$, but we get $\\hat{\\Sigma} = \\frac{1}{n} \\sum_i (x_i - \\hat{\\mu}) (x_i - \\hat{\\mu})^\\top$ where $\\hat{\\mu} = \\frac{1}{n} \\sum_i x_i$  \n",
    "this is the same as $\\frac{1}{K} \\sum_k \\hat{\\Sigma}_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the generative model\n",
    "\n",
    "let score $y_i = w^\\top \\phi(x_i)$\n",
    "\n",
    "for now let $\\phi(x_i) = \\begin{bmatrix} x_{i1} \\\\ \\vdots \\\\ x_{id} \\end{bmatrix}$ (can also include nonlinear terms in general)\n",
    "\n",
    "$w^\\top = f(\\mu_i \\Sigma_i p_i)$\n",
    "\n",
    "so our estimation scheme looks like  \n",
    "data $\\to$ $p_k, \\mu_k, \\Sigma_k$ $\\to$ $w$ $\\to$ $\\hat{y}_i$ $\\to$ $\\hat{t}_i$\n",
    "\n",
    "we can skip some intermediate steps and just estimate $w$ from the data directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "$t_i \\stackrel{indep}{\\sim} Bernoulli(\\cdot)$\n",
    "\n",
    "where the parameter is some function of $w^\\top \\phi(x_i)$\n",
    "\n",
    "assumption on how the data are generated: $p_i = \\sigma(w^\\top \\phi(x_i))$\n",
    "\n",
    "note that we do not impose any distribution on the data matrix $\\Phi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood estimation\n",
    "\n",
    "$L = \\prod_i y_i^{t_i} (1 - y_i)^{1 - t_i}$\n",
    "\n",
    "where $y_i = \\sigma(w^\\top \\phi(x_i))$\n",
    "\n",
    "$\\ell = \\sum_i t_i \\log y_i + \\sum_i (1 - t_i) \\log (1 - y_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to take the derivative:  \n",
    "$\\sigma'(x) = \\frac{e^{-x}}{1 + e^{-x}} \\frac{1}{1 + e^{-x}}$\n",
    "$= \\sigma(x) (1 - \\sigma(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we have\n",
    "\n",
    "$\\partial_w \\ell = \\sum_i \\frac{t_i}{y_i} y_i (1 - y_i) \\phi(x_i) - \n",
    "\\sum_i (1 - t_i) \\frac{1}{1 - y_i} y_i (1 - y_i) \\phi(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$= \\sum_i \\phi(x_i) \\big( t_i (1 - y_i) - (1 - t_i) y_i \\big)$  \n",
    "$= \\sum_i \\phi(x_i) \\big( t_i - t_i y_i - y_i + y_i t_i \\big)$  \n",
    "$= \\sum_i \\phi(x_i) (t_i - y_i)$  \n",
    "$= \\Phi^\\top (t - y) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can't solve this analytically\n",
    "\n",
    "#### Gradient ascent\n",
    "\n",
    "1. initialize $w_0$\n",
    "2. choose step size $\\eta$\n",
    "3. until convergence, do $w_{i+1} = w_i + \\eta \\nabla_w f(w_i)$\n",
    "\n",
    "variants\n",
    "\n",
    "* stochastic gradient ascent (compute gradient from subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Newton-Raphson\n",
    "\n",
    "1. initialize $w_0$\n",
    "2. until convergence, do $w_{i+1} = w_i - H(w_i)^{-1} \\nabla f(w_i)$\n",
    "\n",
    "where $H$ is the hessian $H(w) = \\nabla \\nabla^\\top f(w)$  \n",
    "$H_{ij} = \\frac{\\partial^2}{\\partial w_i \\partial w_j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** \n",
    "\n",
    "let $f(x) = (x - 2) (x - 3)$  \n",
    "$= x^2 - 5x + 6$\n",
    "\n",
    "roots at $2$ and $3$\n",
    "\n",
    "$f'(x) = 2x - 5$  \n",
    "$f''(x) = 2$\n",
    "\n",
    "let $x_0 = 4$  \n",
    "then $x_1 = 4 - \\frac{2 (4) - 5}{2} = 2.5$  \n",
    "then $x_2 = 2.5 - \\frac{0}{2} = 2.5$\n",
    "\n",
    "so $f(x)$ has an optimum at 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.**\n",
    "\n",
    "let $f(x) = 5 x_1^2 + 6 x_1 x_2 + 3 x_2^2$\n",
    "\n",
    "then $\\nabla f = \\begin{bmatrix} 10 x_1 + 6 x_2 \\\\ 6 x_1 + 6 x_2 \\end{bmatrix}$\n",
    "\n",
    "then $H = \\begin{bmatrix} 10 & 6 \\\\ 6 & 6 \\end{bmatrix}$\n",
    "\n",
    "let $x^{(0)} = (1, 1)$\n",
    "\n",
    "then $x^{(1)} = \n",
    "\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} - \n",
    "\\begin{bmatrix} 10 & 6 \\\\ 6 & 6 \\end{bmatrix}^{-1} \n",
    "\\begin{bmatrix} 16 \\\\ 12 \\end{bmatrix} = \n",
    "\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "since $f$ is quadratic, we only need one step to get to the optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### back to logistic regression\n",
    "\n",
    "$\\nabla_w \\ell = \\Phi^\\top (t - y)$\n",
    "\n",
    "$H = -\\sum_i \\phi(x_i) y_i (1 - y_i) \\phi(x_i)^\\top$  \n",
    "$= -\\sum_i \\phi(x_i) \\sqrt{y_i (1 - y_i)} \\sqrt{y_i (1 - y_i)} \\phi(x_i)^\\top$  \n",
    "$= -\\Phi^\\top R \\Phi$\n",
    "\n",
    "where $R = diag(y_i (1 - y_i))$\n",
    "\n",
    "### Newton-Raphson for logistic regression\n",
    "\n",
    "1. initialize $w^{(0)}$\n",
    "2. until convergence, do $w^{(1)} = w^{(0)} + (\\Phi^\\top R \\Phi)^{-1} \\Phi^\\top (y - t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
