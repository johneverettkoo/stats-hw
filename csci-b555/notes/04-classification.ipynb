{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Models for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "Suppose responses $y$ are of the form \"yes\" or \"no\". Say $y \\in \\{+1, -1\\}$.\n",
    "\n",
    "Suppose further that with each response there is an observable $x \\in D$, some domain.\n",
    "\n",
    "We want to come up with some $g : D \\to \\{+1, -1\\}$ that predicts $y$ from $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The generative model\n",
    "\n",
    "Suppose each response $t_i$ is independent and can take on one of $c$ classes.  \n",
    "For each $t_i$, let $x_i$ have some distribution given its corresponding $t_i$.\n",
    "\n",
    "$t_i \\stackrel{iid}{\\sim} Discrete(p_1, ..., p_c)$\n",
    "\n",
    "$X_i \\mid t_i \\sim P(x_i | \\theta_{t_i})$\n",
    "\n",
    "**case 1**: we abstract away $\\theta_k$ and the domain of $X_i$  \n",
    "**case 2**: $X_i \\in \\mathbb{R}^d$, $\\theta_k = (\\mu_k, \\Sigma_k)$, $p(x_i | \\theta_k) = \\mathcal{N}_d(x_i | \\mu_k, \\Sigma_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1\n",
    "\n",
    "we want to find $p(y = k \\mid x)$  \n",
    "$p(y = k \\mid x) = \\frac{p(y = k) p(x | y = k)}{\\sum_j p(y = j) p(x | y = j)}$\n",
    "\n",
    "let $a_k = \\log p(y = k) p(x | y = k)$  \n",
    "then $p(y = k | x) = \\frac{e^{a_k}}{\\sum_j e^{a_j}}$,\n",
    "the softmax function\n",
    "\n",
    "in the case where $k \\in \\{1, 2\\}$, we have  \n",
    "$p(y = 1 | x) = \\frac{1}{1 + e^{-(a_1 - a_2)}}$  \n",
    "$= \\frac{1}{1 + e^{-a}}$  \n",
    "where $a = a_1 - a_2$, the log odds $\\log \\frac{p(y = 1 | x)}{p(y = 2 | x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2\n",
    "\n",
    "suppose we have two classes with identical $\\Sigma_1 = \\Sigma_2 = \\Sigma$\n",
    "\n",
    "then $a = \\log p_1 - \\log p_2 - \\frac{d}{2} \\log 2 \\pi - \\frac{1}{2} \\log |\\Sigma| - \\frac{1}{2} (x - \\mu_1)^\\top \\Sigma^{-1} (x - \\mu_1) + \\frac{d}{2} \\log 2 \\pi + \\frac{1}{2} \\log |\\Sigma| + \\frac{1}{2} (x - \\mu_2)^\\top \\Sigma^{-1} (x - \\mu_2)$  \n",
    "$= \\log p_1 - \\log p_2 - \\frac{1}{2} \\mu_1^\\top \\Sigma^{-1} \\mu_1 + \\frac{1}{2} \\mu_2^\\top \\Sigma^{-1} \\mu_2 + x^\\top \\Sigma^{-1} (\\mu_1 - \\mu_2)$\n",
    "\n",
    "let $w_0$ be the terms that do not depend on $x$ and $w_1$ be the coefficient of $x$  \n",
    "then we have $w_0 + w_1^\\top x$, which is a linear function of $x$\n",
    "\n",
    "if $\\Sigma_1 \\neq \\Sigma_2$, we then have a quadratic term $-\\frac{1}{2} x^\\top (\\Sigma_1^{-1} - \\Sigma_2^{-1}) x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum likelihood estimation\n",
    "\n",
    "$L = \\prod_k \\big(\\prod_{y_i = k} p_k \\mathcal{N}(x_i | \\mu_k, \\Sigma_k) \\big)$\n",
    "\n",
    "$\\ell = \\sum_k \\sum_{y_i = k} \\log p_k + \\log \\mathcal{N}(x_i | \\mu_k, \\Sigma_k)$\n",
    "\n",
    "so we can treat each class separately and get separate MLEs for each class\n",
    "\n",
    "$\\hat{p}_k = \\frac{n_k}{n}$\n",
    "\n",
    "$\\hat{\\mu}_k = \\frac{1}{n_k} \\sum_{y_i = k} x_i$\n",
    "\n",
    "$\\hat{\\Sigma}_k =\\frac{1}{n_k} \\sum_{y_i = k} (x_i - \\hat{\\mu}_k) (x_i - \\hat{\\mu}_k)^\\top$\n",
    "\n",
    "if $\\Sigma_k = \\Sigma$ $\\forall k$, then we have the same $\\hat{p}_k$, $\\hat{\\mu}_k$, but we get $\\hat{\\Sigma} = \\frac{1}{n} \\sum_i (x_i - \\hat{\\mu}) (x_i - \\hat{\\mu})^\\top$ where $\\hat{\\mu} = \\frac{1}{n} \\sum_i x_i$  \n",
    "this is the same as $\\frac{1}{K} \\sum_k \\hat{\\Sigma}_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the generative model\n",
    "\n",
    "let score $y_i = w^\\top \\phi(x_i)$\n",
    "\n",
    "for now let $\\phi(x_i) = \\begin{bmatrix} x_{i1} \\\\ \\vdots \\\\ x_{id} \\end{bmatrix}$ (can also include nonlinear terms in general)\n",
    "\n",
    "$w^\\top = f(\\mu_i \\Sigma_i p_i)$\n",
    "\n",
    "so our estimation scheme looks like  \n",
    "data $\\to$ $p_k, \\mu_k, \\Sigma_k$ $\\to$ $w$ $\\to$ $\\hat{y}_i$ $\\to$ $\\hat{t}_i$\n",
    "\n",
    "we can skip some intermediate steps and just estimate $w$ from the data directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "$t_i \\stackrel{indep}{\\sim} Bernoulli(\\cdot)$\n",
    "\n",
    "where the parameter is some function of $w^\\top \\phi(x_i)$\n",
    "\n",
    "assumption on how the data are generated: $p_i = \\sigma(w^\\top \\phi(x_i))$\n",
    "\n",
    "note that we do not impose any distribution on the data matrix $\\Phi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood estimation\n",
    "\n",
    "$L = \\prod_i y_i^{t_i} (1 - y_i)^{1 - t_i}$\n",
    "\n",
    "where $y_i = \\sigma(w^\\top \\phi(x_i))$\n",
    "\n",
    "$\\ell = \\sum_i t_i \\log y_i + \\sum_i (1 - t_i) \\log (1 - y_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to take the derivative:  \n",
    "$\\sigma'(x) = \\frac{e^{-x}}{1 + e^{-x}} \\frac{1}{1 + e^{-x}}$\n",
    "$= \\sigma(x) (1 - \\sigma(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we have\n",
    "\n",
    "$\\partial_w \\ell = \\sum_i \\frac{t_i}{y_i} y_i (1 - y_i) \\phi(x_i) - \n",
    "\\sum_i (1 - t_i) \\frac{1}{1 - y_i} y_i (1 - y_i) \\phi(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$= \\sum_i \\phi(x_i) \\big( t_i (1 - y_i) - (1 - t_i) y_i \\big)$  \n",
    "$= \\sum_i \\phi(x_i) \\big( t_i - t_i y_i - y_i + y_i t_i \\big)$  \n",
    "$= \\sum_i \\phi(x_i) (t_i - y_i)$  \n",
    "$= \\Phi^\\top (t - y) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can't solve this analytically\n",
    "\n",
    "#### Gradient ascent\n",
    "\n",
    "1. initialize $w_0$\n",
    "2. choose step size $\\eta$\n",
    "3. until convergence, do $w_{i+1} = w_i + \\eta \\nabla_w f(w_i)$\n",
    "\n",
    "variants\n",
    "\n",
    "* stochastic gradient ascent (compute gradient from subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Newton-Raphson\n",
    "\n",
    "1. initialize $w_0$\n",
    "2. until convergence, do $w_{i+1} = w_i - H(w_i)^{-1} \\nabla f(w_i)$\n",
    "\n",
    "where $H$ is the hessian $H(w) = \\nabla \\nabla^\\top f(w)$  \n",
    "$H_{ij} = \\frac{\\partial^2}{\\partial w_i \\partial w_j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** \n",
    "\n",
    "let $f(x) = (x - 2) (x - 3)$  \n",
    "$= x^2 - 5x + 6$\n",
    "\n",
    "roots at $2$ and $3$\n",
    "\n",
    "$f'(x) = 2x - 5$  \n",
    "$f''(x) = 2$\n",
    "\n",
    "let $x_0 = 4$  \n",
    "then $x_1 = 4 - \\frac{2 (4) - 5}{2} = 2.5$  \n",
    "then $x_2 = 2.5 - \\frac{0}{2} = 2.5$\n",
    "\n",
    "so $f(x)$ has an optimum at 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.**\n",
    "\n",
    "let $f(x) = 5 x_1^2 + 6 x_1 x_2 + 3 x_2^2$\n",
    "\n",
    "then $\\nabla f = \\begin{bmatrix} 10 x_1 + 6 x_2 \\\\ 6 x_1 + 6 x_2 \\end{bmatrix}$\n",
    "\n",
    "then $H = \\begin{bmatrix} 10 & 6 \\\\ 6 & 6 \\end{bmatrix}$\n",
    "\n",
    "let $x^{(0)} = (1, 1)$\n",
    "\n",
    "then $x^{(1)} = \n",
    "\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} - \n",
    "\\begin{bmatrix} 10 & 6 \\\\ 6 & 6 \\end{bmatrix}^{-1} \n",
    "\\begin{bmatrix} 16 \\\\ 12 \\end{bmatrix} = \n",
    "\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "since $f$ is quadratic, we only need one step to get to the optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### back to logistic regression\n",
    "\n",
    "$\\nabla_w \\ell = \\Phi^\\top (t - y)$\n",
    "\n",
    "$H = -\\sum_i \\phi(x_i) y_i (1 - y_i) \\phi(x_i)^\\top$  \n",
    "$= -\\sum_i \\phi(x_i) \\sqrt{y_i (1 - y_i)} \\sqrt{y_i (1 - y_i)} \\phi(x_i)^\\top$  \n",
    "$= -\\Phi^\\top R \\Phi$\n",
    "\n",
    "where $R = diag(y_i (1 - y_i))$\n",
    "\n",
    "### Newton-Raphson for logistic regression\n",
    "\n",
    "1. initialize $w^{(0)}$\n",
    "2. until convergence, do $w^{(i+1)} = w^{(i)} + (\\Phi^\\top R \\Phi)^{-1} \\Phi^\\top (y - t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other types of models\n",
    "\n",
    "* ordinal regression\n",
    "* count regression\n",
    "* proportion regression (e.g., beta regression)\n",
    "* robust regression\n",
    "* ...\n",
    "\n",
    "would be good to have one model/framework for all types of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential family of distributions\n",
    "\n",
    "$X \\mid \\eta \\sim P(x | \\eta) \\in \\text{exponential family}$ iff $p(x | \\eta) = h(x) g(\\eta) e^{\\eta^\\top u(x)}$\n",
    "\n",
    "**def** $\\eta \\in \\mathbb{R}^d$ are the *natural parameters*\n",
    "\n",
    "**def** $u(x) \\in \\mathbb{R}^d$ are the *sufficient statistics*\n",
    "\n",
    "**def** $\\theta = E[u(X)] \\in \\mathbb{R}^d$ are the *expectation parameters*\n",
    "\n",
    "**theorem** there exists a 1-1 function $\\psi$ such that $\\eta = \\psi(\\theta)$ and $\\theta = \\psi^{-1}(\\eta)$ if $u$ are linearly independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** bernoulli distribution\n",
    "\n",
    "$f(x) = p^x (1-p)^{1-x}$  \n",
    "$= e^{x \\log p} e^{(1-x) \\log (1-p)}$  \n",
    "$= e^{x \\log p} e^{\\log (1-p)} e^{-x \\log (1-p)}$  \n",
    "$= (1-p) e^{x \\log \\frac{p}{1-p}}$\n",
    "\n",
    "$u(x) = x$  \n",
    "$h(x) = 1$  \n",
    "$\\eta = \\log \\frac{p}{1-p}$\n",
    "\n",
    "we can see that $1 - p = \\frac{1}{1 + e^\\eta}$,  \n",
    "so $g(\\eta) = \\frac{1}{1 + e^\\eta}$\n",
    "\n",
    "$f(x) = \\frac{1}{1 + e^\\eta} e^{\\eta x}$\n",
    "\n",
    "$p$ is the *standard parameter*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** normal distribution\n",
    "\n",
    "$f(x) = (2 \\pi \\sigma^2)^{-1/2} e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}}$  \n",
    "$= (2 \\pi \\sigma^2)^{-1/2} e^{-\\frac{\\mu^2}{2 \\sigma^2} - \\frac{x^2}{2 \\sigma^2} + \\frac{\\mu x}{\\sigma^2}}$\n",
    "\n",
    "$h(x) = 1$  \n",
    "$\\eta_1 = \\frac{\\mu}{\\sigma^2}$  \n",
    "$\\eta_2 = -\\frac{1}{2 \\sigma^2}$  \n",
    "$u_1(x) = x$  \n",
    "$u_2(x) = x^2$  \n",
    "$g(\\eta_1, \\eta_2) = (-\\eta_2 / \\pi)^{1/2} e^{-\\frac{\\eta_1^2}{4 \\eta_2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** poisson distribution\n",
    "\n",
    "$f(x) = \\frac{1}{x!} \\lambda^x e^{-\\lambda x}$\n",
    "\n",
    "$\\eta = \\log \\lambda$  \n",
    "$\\lambda = e^{\\eta}$  \n",
    "$u(x) = x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**theorem**\n",
    "\n",
    "1. if $X_1, ..., X_n \\stackrel{iid}{\\sim} P(x)$ where $P$ is in the exponential family, then $L(\\eta) = \\Big(\\prod_i h(x_i)\\Big) g(\\eta)^n e^{\\eta^\\top \\sum_i u(x_i)}$\n",
    "\n",
    "2. $\\theta = E[u(X)] = -\\frac{\\partial}{\\partial \\eta} \\log g(\\eta)$  \n",
    "$Cov(X) = E\\big[ (X - E[X]) (X - E[X])^\\top \\big] = -\\frac{\\partial^2}{\\partial \\eta \\partial \\eta^\\top} \\log g(\\eta)$\n",
    "\n",
    "3. if the sample is of size 1, then the maximum likelihood estimate occurs when $u(x) = \\theta$  \n",
    "if sample is iid, then $\\frac{1}{n} \\sum_i u(X_i)$ is the maximum likelihood estimate for $\\theta$\n",
    "\n",
    "**corollary** $\\hat{\\theta}_{MLE} = \\frac{1}{n} \\sum u(X_i)$\n",
    "\n",
    "in the exponential family, the maximum likelihood estimate coincides with our intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** bernoulli distribution\n",
    "\n",
    "$u(X) = X$\n",
    "\n",
    "$E[X] = p$\n",
    "\n",
    "so $\\hat{p}_{MLE} = \\frac{1}{n} \\sum X_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**proof of (3)**\n",
    "\n",
    "recall that $L(\\eta) \\propto g(\\eta)^n e^{\\eta^\\top \\sum_i u(x_i)}$  \n",
    "$\\implies \\ell(\\eta) = n \\log g(\\eta) + \\eta^\\top \\sum_i u(x_i) + C$  \n",
    "$\\implies \\ell'(\\eta) = n \\partial_\\eta \\log g(\\eta) + \\sum_i u(x_i)$  \n",
    "$\\implies$ maximum likelihood estimate occurs when \n",
    "$\\frac{1}{n} \\sum_i u(X_i) = -\\partial_\\eta \\log g(\\eta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**proof of (2)**\n",
    "\n",
    "$\\int h(x) g(\\eta) e^{\\eta^\\top u(x)} dx = 1$ since this is a pdf\n",
    "\n",
    "then taking the derivative of both sides w.r.t. $\\eta$:  \n",
    "$(\\partial_\\eta g(\\eta)) \\int \\cdots dx + g(\\eta) \\int h(x) e^{\\eta^\\top u(x)} u(x) dx = 0$  \n",
    "$\\implies (\\partial_\\eta g(\\eta)) (g(\\eta))^{-1} + g(\\eta) E[u(X)] = 0$  \n",
    "$\\implies -E[u(X)] = \\partial_\\eta \\log g(\\eta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**proof of (2) cont'd**\n",
    "\n",
    "we have\n",
    "$\\partial_{\\eta_i} \\log g(\\eta) = -g(\\eta) \\int h(x) e^{\\eta^\\top u(x)} u_i(x) dx$\n",
    "\n",
    "taking the derivative w.r.t $\\eta_j$:\n",
    "\n",
    "$\\partial_{\\eta_i} \\partial_{\\eta_j} \\log g(\\eta) = -\\partial_{\\eta_j} g(\\eta) \\int h(x) e^{\\eta^\\top u(x)} u_i(x) dx - g(\\eta) \\int h(x) e^{\\eta^\\top u(x)} u_i(x) u_j(x) dx$  \n",
    "$= g(\\eta)E[u_j(X)] (g(\\eta))^{-1} E[u_i(X)] - E[u_i(X) u_j(X)]$  \n",
    "$= -Cov(X_i, X_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled exponential family\n",
    "\n",
    "$\\eta \\in \\mathbb{R}$, $u(X) = X$\n",
    "\n",
    "$p(x) = \\frac{1}{s} h(\\frac{x}{s}) g(\\eta) e^{\\frac{1}{s} \\eta x}$\n",
    "\n",
    "then $-\\partial_\\eta \\log g(\\eta) = \\frac{1}{s} E[X] = \\frac{1}{s} E[u(X)] = \\frac{\\theta}{s}$  \n",
    "and $-\\partial_\\eta^2 \\log g(\\eta) = \\frac{Var(X)}{s^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** Bernoulli\n",
    "\n",
    "$p(x) = \\sigma(-\\eta) e^{\\eta x}$  \n",
    "$\\theta = \\mu = E[X]$  \n",
    "$\\eta = \\log \\frac{\\mu}{1 - \\mu}$, $\\mu = \\sigma(\\eta)$\n",
    "\n",
    "so this is in the scale family with $s = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** Poisson\n",
    "\n",
    "$p(x) = \\frac{1}{x!} e^{-e^\\eta} e^{\\eta x}$  \n",
    "$\\eta = \\psi(\\lambda) = \\log \\lambda$  \n",
    "$\\lambda = \\theta = e^\\eta$\n",
    "\n",
    "so again $s=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** normal\n",
    "\n",
    "$p(x) = (2 \\pi \\sigma^2)^{-1/2} e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}}$\n",
    "\n",
    "let $s = \\sigma^2$  \n",
    "then $p(x) = (2 \\pi s)^{-1/2} e^{-\\frac{1}{2s} x^2} e^{-\\frac{\\mu^2}{2s}} e^{\\frac{1}{s} \\mu x}$  \n",
    "\n",
    "then $\\mu = \\theta = E[X] = \\eta = \\psi(\\eta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count/Poisson regression\n",
    "\n",
    "Since our responses $t_i$ are counts, a plausible model is $t_i \\sim Poisson(y_i)$ where $y_i > 0$. One function that forces positive values is $y_i = e^{a_i}$ where we can say $a_i$ is our linear combination $a_i = w^\\top \\phi(x_i)$. Then we get\n",
    "\n",
    "$p(t_i | y_i) = \\frac{y_i^{t_i} e^{-y_i}}{t_i!}$\n",
    "\n",
    "In order to find $\\hat{w}_{MLE}$, we use Newton-Raphson:\n",
    "\n",
    "$\\ell(w) = \\sum_i t_i \\log y_i - \\sum_i y_i - \\sum_i \\log t_i !$  \n",
    "$= \\sum_i t_i a_i - \\sum_i e^{a_i} - \\log t_i !$\n",
    "\n",
    "$\\implies \\nabla_w \\ell = \\sum_i t_i \\phi(x_i) - \\sum_i e^{a_i} \\phi(x_i)$  \n",
    "$= \\sum_i (t_i - y_i) \\phi(x_i)$  \n",
    "$= \\Phi^\\top (t - y)$\n",
    "\n",
    "$\\nabla_w \\times \\nabla_w \\ell = - \\sum_i \\phi(x_i) \\partial_w^\\top y_i$  \n",
    "$= -\\sum_i \\phi(x_i) y_i \\phi(x_i)^\\top$  \n",
    "$= -\\Phi^\\top diag(y (1 - y)) \\Phi$\n",
    "\n",
    "so the update step is:\n",
    "\n",
    "$w^{(i+1)} = w^{(i)} - (\\Phi^\\top diag(y^{(i)} (1 - y^{(i)})) \\Phi)^{-1} \\Phi^\\top (y^{(i)} - t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized linear model\n",
    "\n",
    "there is an underlying linear function: $a_i = w^\\top \\phi(x_i)$\n",
    "\n",
    "predictions are some function of the linear outputs: $y_i = f(a_i)$\n",
    "\n",
    "* $f$ is the activation function\n",
    "* $f^{-1}$ is the link function\n",
    "* $y_i$ is the mean parameter $y_i = \\theta_i = E[X_i]$\n",
    "\n",
    "$\\eta_i = \\psi(y_i)$\n",
    "\n",
    "$t_i \\mid \\eta_i \\sim P$ where $P$ is in the 1 dimensional scaled exponential family\n",
    "\n",
    "if we choose $f = \\psi^{-1}$, then $\\eta_i = a_i = w^\\top \\phi(x_i)$ (canonical link function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L(w) = \\prod_i \\frac{1}{s} h(\\frac{t_i}{s}) g(\\eta_i) e^{\\frac{1}{s} \\eta_i t_i}$\n",
    "\n",
    "$\\implies \\ell(w) = -n \\log s + \\sum_i \\log h(\\frac{t_i}{s}) + \\sum_i \\log g(\\eta_i) + \\frac{1}{s} \\sum_i \\eta_i t_i$  \n",
    "$\\implies \\nabla_w \\ell = \\sum_i (\\partial_{\\eta_i} \\log g(\\eta_i)) \\psi'(y_i) f'(a_i) \\phi(x_i) + \\frac{1}{s} \\sum_i t_i \\phi'(y_i) f'(a_i) \\phi(x_i)$  \n",
    "$= \\frac{1}{s} \\sum_i (t_i - y_i) \\psi'(y_i) f'(a_i) \\phi(x_i)$\n",
    "\n",
    "recall that $\\partial_\\eta \\log g(\\eta) = \\theta$ (which is $y_i$ in this case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if using canonical link function, then \n",
    "\n",
    "$= \\frac{1}{s} \\sum_i (t_i - y_i) \\phi(x_i)$\n",
    "$= \\frac{1}{s} \\Phi^\\top (t - y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the second derivative ...\n",
    "\n",
    "$\\nabla_w \\times \\nabla_w \\ell$  \n",
    "$= \\frac{1}{s} \\sum_i (t_i - y_i) \\psi'(y_i) f''(a_i) \\phi(x_i) \\phi(x_i)^\\top$  \n",
    "$+ \\frac{1}{s} \\sum_i (t_i - y_i) (f'(a_i))^2 \\psi''(y_i) \\phi(x_i) \\phi(x_i)^\\top$  \n",
    "$-\\frac{1}{s} \\sum_i \\psi'(y_i) (f'(a_i))^2 \\phi(x_i) \\phi(x_i)^\\top$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if using canonical link function, then the first two terms cancel out and third term simplifies to  \n",
    "$-\\frac{1}{s} \\sum_i f'(a_i) \\phi(x_i) \\phi(x_i)^\\top$\n",
    "$= -\\frac{1}{s} \\Phi^\\top diag(f'(a)) \\Phi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then the Newton-Raphson update step (in the canonical case) becomes\n",
    "\n",
    "$w^{(i+1)} = w^{(i)} + (\\Phi^\\top diag(f'(a^{(i)})) \\Phi)^{-1} \\Phi^\\top (t - y^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian logistic regression\n",
    "\n",
    "need a prior for weights $w$  \n",
    "since $w$ can take on any value in $\\mathbb{R}^d$, an appropriate prior might be normal  \n",
    "we also can't come up with a good conjugate prior, so ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w \\sim \\mathcal{N}_d(m_0, S_0)$\n",
    "\n",
    "choosing $m_0 = 0$ induces shrinkage  \n",
    "a simple choice of $S_0$ might be $S_0 = \\alpha^{-1} I$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(w | \\Phi, t) \\propto p(w) p(t | w, \\Phi)$  \n",
    "$\\propto (2 \\pi)^{-d/2} |S_0|^{-1/2} e^{-\\frac{1}{2} (w - m_0)^\\top S_0^{-1} (w - m_0)} \\prod_i y_i^{t_i} (1 - y_i)^{1 - t_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can't get a recognizable form for the posterior  \n",
    "approximate the posterior using a known form\n",
    "\n",
    "### MAP solution\n",
    "\n",
    "$\\hat{w}_{MAP} = \\arg\\max \\log p(w | \\Phi, t)$\n",
    "\n",
    "$\\log p(w | \\Phi, t) -\\frac{1}{2} w^\\top S_0^{-1} w + w^\\top S_0^{-1} m_0 + \\sum_i t_i \\log y_i + \\sum_i (1 - t_i) \\log (1 - y_i) + C$\n",
    "\n",
    "taking the gradient w.r.t. $w$ yields:\n",
    "\n",
    "$\\nabla_w p(w | \\cdot) = -S_0^{-1} w+ S^{-1} m_0 + \\Phi^\\top (t - y)$  \n",
    "$= \\Phi^\\top (t - y) - S_0^{-1} (w - m_0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_w \\times \\nabla_w p(w | \\cdot) = -\\Phi^\\top diag(y (1 - y)) \\Phi - S_0^{-1}$\n",
    "\n",
    "then we can use Newton-Raphson:\n",
    "\n",
    "$w^{(i+1)} = w^{(i)} + (\\Phi^\\top diag(y^{(i)} (1-y^{(i)})) \\Phi + S_0^{-1})^{-1} (\\Phi^\\top (t - y^{(i)}) - S_0^{-1} (w^{(i)} - m_0))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplace approximation\n",
    "\n",
    "set $\\hat{\\mu} = \\arg\\max p(w | \\cdot) = \\hat{w}_{MAP}$  \n",
    "and $\\hat{\\Sigma} =$ curvature of $p(w | \\cdot)$  \n",
    "and approximate $w \\mid \\cdot \\sim \\mathcal{N}(\\hat{\\mu}, \\hat{\\Sigma})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### method\n",
    "\n",
    "let $f(x)$ be an unnormalized posterior  \n",
    "$p(x) = \\frac{1}{z} f(x)$ for some normalizing term $z$\n",
    "($z = \\int f(x) dx$)  \n",
    "if $p(x)$ is normal pdf, then $\\log p(x)$ is quadratic  \n",
    "so we can get a normal pdf approximation of $p(x)$ with second order taylor series\n",
    "\n",
    "set $g(x) = \\log f(x)$\n",
    "\n",
    "$g(x) \\approx g(x_0) + g'(x_0) (x - x_0) + \\frac{1}{2} g''(x_0) (x - x_0)^2$\n",
    "\n",
    "if we choose $x_0 = \\arg\\max p(x)$ and $p(x)$ is differentiable, $g'(x_0) = 0$\n",
    "\n",
    "we can set $\\hat{\\sigma}^2 = -\\frac{1}{g''(x_0)}$  \n",
    "alternatively $\\beta = -g''(x_0)$\n",
    "\n",
    "in higher dimensions:\n",
    "\n",
    "* $g(x) \\approx g(x_0) + \\frac{1}{2} (x - x_0) H(x - x_0)$\n",
    "* again, choose $x_0 = \\arg\\max g(x)$\n",
    "* then approximate $x \\sim \\mathcal{N}_d(x_0, -H^{-1}|_{x_0})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### evidence function approximation\n",
    "\n",
    "$\\int f(x) dx \\approx f(x_0) e^{-\\frac{1}{2 \\sigma^2} (x - x_0)^2} dx$\n",
    "$= f(x_0) (2 \\pi \\sigma^2)^{1/2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace approximation for the distribution of $w$ in bayesian logistic regression:\n",
    "\n",
    "* $w | \\Phi, t \\sim \\mathcal{N} \\Big(\\hat{w}_{MAP}, \\hat{\\Sigma}_{MAP} \\Big)$\n",
    "* $\\hat{\\Sigma}_{MAP} = \\big(\\Phi^\\top diag(y (1-y)) \\Phi + S_0^{-1} \\big)^{-1}$\n",
    "* $y = \\sigma(\\Phi \\hat{w}_{MAP})$\n",
    "* evidence \n",
    "    * $\\log p(w | \\cdot) \\approx f(\\hat{w}_{MAP}) e^{-\\frac{1}{2} (w - \\hat{w})^\\top \\hat{\\Sigma} (w - \\hat{w})}$\n",
    "    * $f(w) = \\mathcal{N}(w | m_0, S_0) L(w)$\n",
    "    * evidence $= \\int f(w) dw = f(\\hat{w}) (2 \\pi)^{d/2} |\\hat{\\Sigma}|^{1/2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predictive distribution\n",
    "\n",
    "find $p(t_{n+1} | t, \\Phi, \\phi(x_{n+1}))$\n",
    "\n",
    "$p(t_{n+1} | \\cdot) = \\int p(w | \\cdot) p(t_{n+1} | w, \\cdot) dw$  \n",
    "$= \\int \\mathcal{N}(w | \\hat{w}, \\hat{\\Sigma}) \\sigma(w^\\top \\phi(x_{n+1})) dw$  \n",
    "\n",
    "no closed form solution\n",
    "\n",
    "approximate $\\sigma(z) \\approx \\Phi(\\sqrt{\\pi / 8} z)$\n",
    "\n",
    "$\\approx \\int \\mathcal{N}(w | \\cdot) \\Phi(\\sqrt{\\pi/8} w^\\top \\phi(x_{n+1})) dw$\n",
    "\n",
    "if $z = w^\\top \\phi(x_{n+1})$ and $w \\sim \\mathcal{N}(\\hat{w}, \\hat{\\Sigma})$  \n",
    "then $z \\sim \\mathcal{N}(\\hat{w}^\\top \\phi(x_{n+1}), \\phi(x_{n+1})^\\top \\hat{\\Sigma} \\phi(x_{n+1}))$\n",
    "\n",
    "then we get  \n",
    "$= \\int \\mathcal{N}(z | \\mu_z, \\sigma^2_z) \\Phi(\\sqrt{\\pi / 8} z) dz$\n",
    "\n",
    "where $\\mu_z$ and $\\sigma^2_z$ are the mean and variance of $z$ from above\n",
    "\n",
    "using $\\int \\Phi(\\alpha t) \\mathcal{N}(t | \\mu, \\sigma^2) = \\Phi(\\frac{\\mu}{\\alpha^{-2} + \\sigma^2})$, we get\n",
    "\n",
    "$= \\Phi \\bigg(\\frac{\\mu_z}{\\frac{8}{\\pi} + \\sigma_z^2} \\bigg)$\n",
    "\n",
    "we can approximate this back to the sigmoid function to get  \n",
    "$\\approx \\sigma \\bigg( \\sqrt{8 / \\pi} \\frac{\\mu_z}{\\frac{8}{\\pi} + \\sigma^2_z} \\bigg)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for predicting class, we might say if $p(t_{n+1} | \\cdot) > 0.5$ we predict $t_{n+1} = 1$ and $0$ otherwise  \n",
    "but the only value inside $\\sigma(\\cdot)$ that can be negative is $\\mu_z$, so we only care about $\\mu_z$ for predicting class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
