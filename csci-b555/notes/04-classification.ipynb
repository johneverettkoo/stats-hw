{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Models for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "Suppose responses $y$ are of the form \"yes\" or \"no\". Say $y \\in \\{+1, -1\\}$.\n",
    "\n",
    "Suppose further that with each response there is an observable $x \\in D$, some domain.\n",
    "\n",
    "We want to come up with some $g : D \\to \\{+1, -1\\}$ that predicts $y$ from $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The generative model\n",
    "\n",
    "Suppose each response $t_i$ is independent and can take on one of $c$ classes.  \n",
    "For each $t_i$, let $x_i$ have some distribution given its corresponding $t_i$.\n",
    "\n",
    "$t_i \\stackrel{iid}{\\sim} Discrete(p_1, ..., p_c)$\n",
    "\n",
    "$X_i \\mid t_i \\sim P(x_i | \\theta_{t_i})$\n",
    "\n",
    "**case 1**: we abstract away $\\theta_k$ and the domain of $X_i$  \n",
    "**case 2**: $X_i \\in \\mathbb{R}^d$, $\\theta_k = (\\mu_k, \\Sigma_k)$, $p(x_i | \\theta_k) = \\mathcal{N}_d(x_i | \\mu_k, \\Sigma_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1\n",
    "\n",
    "we want to find $p(y = k \\mid x)$  \n",
    "$p(y = k \\mid x) = \\frac{p(y = k) p(x | y = k)}{\\sum_j p(y = j) p(x | y = j)}$\n",
    "\n",
    "let $a_k = \\log p(y = k) p(x | y = k)$  \n",
    "then $p(y = k | x) = \\frac{e^{a_k}}{\\sum_j e^{a_j}}$,\n",
    "the softmax function\n",
    "\n",
    "in the case where $k \\in \\{1, 2\\}$, we have  \n",
    "$p(y = 1 | x) = \\frac{1}{1 + e^{-(a_1 - a_2)}}$  \n",
    "$= \\frac{1}{1 + e^{-a}}$  \n",
    "where $a = a_1 - a_2$, the log odds $\\log \\frac{p(y = 1 | x)}{p(y = 2 | x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2\n",
    "\n",
    "suppose we have two classes with identical $\\Sigma_1 = \\Sigma_2 = \\Sigma$\n",
    "\n",
    "then $a = \\log p_1 - \\log p_2 - \\frac{d}{2} \\log 2 \\pi - \\frac{1}{2} \\log |\\Sigma| - \\frac{1}{2} (x - \\mu_1)^\\top \\Sigma^{-1} (x - \\mu_1) + \\frac{d}{2} \\log 2 \\pi + \\frac{1}{2} \\log |\\Sigma| + \\frac{1}{2} (x - \\mu_2)^\\top \\Sigma^{-1} (x - \\mu_2)$  \n",
    "$= \\log p_1 - \\log p_2 - \\frac{1}{2} \\mu_1^\\top \\Sigma^{-1} \\mu_1 + \\frac{1}{2} \\mu_2^\\top \\Sigma^{-1} \\mu_2 + x^\\top \\Sigma^{-1} (\\mu_1 - \\mu_2)$\n",
    "\n",
    "let $w_0$ be the terms that do not depend on $x$ and $w_1$ be the coefficient of $x$  \n",
    "then we have $w_0 + w_1^\\top x$, which is a linear function of $x$\n",
    "\n",
    "if $\\Sigma_1 \\neq \\Sigma_2$, we then have a quadratic term $-\\frac{1}{2} x^\\top (\\Sigma_1^{-1} - \\Sigma_2^{-1}) x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum likelihood estimation\n",
    "\n",
    "$L = \\prod_k \\big(\\prod_{y_i = k} p_k \\mathcal{N}(x_i | \\mu_k, \\Sigma_k) \\big)$\n",
    "\n",
    "$\\ell = \\sum_k \\sum_{y_i = k} \\log p_k + \\log \\mathcal{N}(x_i | \\mu_k, \\Sigma_k)$\n",
    "\n",
    "so we can treat each class separately and get separate MLEs for each class\n",
    "\n",
    "$\\hat{p}_k = \\frac{n_k}{n}$\n",
    "\n",
    "$\\hat{\\mu}_k = \\frac{1}{n_k} \\sum_{y_i = k} x_i$\n",
    "\n",
    "$\\hat{\\Sigma}_k =\\frac{1}{n_k} \\sum_{y_i = k} (x_i - \\hat{\\mu}_k) (x_i - \\hat{\\mu}_k)^\\top$\n",
    "\n",
    "if $\\Sigma_k = \\Sigma$ $\\forall k$, then we have the same $\\hat{p}_k$, $\\hat{\\mu}_k$, but we get $\\hat{\\Sigma} = \\frac{1}{n} \\sum_i (x_i - \\hat{\\mu}) (x_i - \\hat{\\mu})^\\top$ where $\\hat{\\mu} = \\frac{1}{n} \\sum_i x_i$  \n",
    "this is the same as $\\frac{1}{K} \\sum_k \\hat{\\Sigma}_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
