{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recall ordinary least squares regression:\n",
    "\n",
    "* $t_i \\stackrel{iid}{\\sim} \\mathcal{N}(f_i, \\beta^{-1})$\n",
    "* $f_i = w^\\top \\phi(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "suppose we want a prior for $w$\n",
    "\n",
    "* preferably a *conjugate* prior\n",
    "* recall posterior $\\propto$ prior $\\times$ likelihood\n",
    "* then a multivariate normal prior results in a multivariate normal posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate normal distribution\n",
    "\n",
    "let $X \\in \\mathbb{R}^p$\n",
    "\n",
    "$X \\sim MVN(\\mu, \\Sigma)$ iff it has density function  \n",
    "$f(x) = (2 \\pi)^{-p/2} |\\Sigma|^{-1/2} \\exp \\big( -\\frac{1}{2} (x - \\mu)^\\top \\Sigma^{-1} (x - \\mu) \\big)$\n",
    "\n",
    "$\\Sigma_{ij} = Cov(X_i, X_j) = Cov(X_j, X_i) = \\Sigma_{ji}$\n",
    "\n",
    "alternative parameterization using precision $Q = \\Sigma^{-1}$:\n",
    "\n",
    "* $X \\sim MVN(\\mu, Q^{-1})$\n",
    "* $f(x) = \\big( \\frac{1}{2 \\pi} \\big)^{p/2} |Q| \\exp \\big( -\\frac{1}{2} (x - \\mu)^\\top Q (x - \\mu) \\big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**theorem**\n",
    "\n",
    "let \n",
    "\n",
    "* $X \\sim F_X$\n",
    "* $y = g(x)$, 1-1 and invertible\n",
    "\n",
    "then\n",
    "\n",
    "* $f_Y(y) = f_X \\big(g^{-1}(y) \\big) \\bigg|\\frac{dg^{-1}}{dy} \\bigg|$\n",
    "\n",
    "for multidimensional random variables\n",
    "\n",
    "* $f_Y(y) = f_X \\big( g^{-1}(y) \\big) \\big| J_{g^{-1}} (y) \\big|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**eigendecomposition of $\\Sigma = V \\Lambda V^\\top$**\n",
    "\n",
    "let $Y = V^\\top (X - \\mu)$ then $X = V Y + \\mu$  \n",
    "$\\Sigma^{-1} = V \\Lambda^{-1} V^\\top$  \n",
    "$|V| = 1$  \n",
    "$|\\Sigma| = |V^\\top \\Lambda V| = |V^\\top| |\\Lambda| |V| = |\\Lambda|$  \n",
    "so we get  \n",
    "$f(y) = (2 \\pi)^{-p/2} |\\Lambda|^{-1/2} \\exp( -\\frac{1}{2} y^\\top \\Lambda^{-1} y)$  \n",
    "where $\\Lambda$ is a diagonal matrix  \n",
    "then $Y_i$'s are iid normal  \n",
    "$f(y) = \\prod_i f(y_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Cov(x) = E[(x - \\mu) (x - \\mu)^\\top]$  \n",
    "$= E[z z^\\top]$ (where $z = x - \\mu$)  \n",
    "$= E[Vy (Vy)^\\top]$  \n",
    "$= E[V y y^\\top V^\\top]$  \n",
    "$= V E[y y^\\top] V^\\top$  \n",
    "$= V \\Lambda V^\\top$  \n",
    "$= \\Sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**corollary**\n",
    "\n",
    "suppose $f(x) \\propto e^{a^\\top x} e^{-\\frac{1}{2} x^\\top B x}$\n",
    "\n",
    "then $X \\sim MVN(B^{-1} a, B^{-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lemma**\n",
    "\n",
    "given $f(a) = a^\\top Q a$, $Q$ is symmetric\n",
    "\n",
    "if we want $\\frac{\\partial f}{\\partial a}$\n",
    "\n",
    "note that $f(a) = \\sum_i \\sum_j a_i a_j Q_{ij}$\n",
    "\n",
    "then $\\frac{\\partial f}{\\partial a_k} = 2 a_k Q_{kk} + \\sum_{i \\neq k} a_i Q_{ik} + \\sum_{i \\neq k} a_i Q_{ki}$\n",
    "\n",
    "but note that $Q$ is symmetric, so this just simplifies to \n",
    "$2 a_k Q_{kk} + 2 \\sum_{i \\neq k} a_i Q_{ik}$  \n",
    "$= 2 \\sum_i a_k Q_{ik}$  \n",
    "$= 2 Q a$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lemma**\n",
    "\n",
    "$\\frac{\\partial}{\\partial A} |A| = (A^{-1})^\\top$\n",
    "\n",
    "$\\frac{\\partial}{\\partial A} (a^\\top A^{-1} b) = -(A^{-1})^\\top a b^\\top (A^{-1})^\\top$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood estimation for MVN\n",
    "\n",
    "given\n",
    "\n",
    "* $X_i \\stackrel{iid}{\\sim} MVN(\\mu, \\Sigma)$\n",
    "    * $X_i, \\mu \\in \\mathbb{R}^d$\n",
    "    * $\\Sigma \\in \\mathbb{R}^{d \\times d}$\n",
    "* $\\mu, \\Sigma$ unknown, to be estimated from observed $X_i$s\n",
    "\n",
    "then the likelihood is:\n",
    "\n",
    "* $L(\\mu, \\Sigma) = \\prod (2 \\pi)^{d/2} |\\Sigma|^{-1/2} e^{-\\frac{1}{2} (x_i - \\mu)^\\top \\Sigma^{-1} (x_i - \\mu)}$\n",
    "* $\\ell(\\mu, \\Sigma) = -\\frac{d}{2} \\log 2 \\pi - \\frac{1}{2} \\log |\\Sigma| - \\frac{1}{2} \\sum_i (x_i - \\mu)^\\top \\Sigma^{-1} (x_i - \\mu)$\n",
    "\n",
    "to maximize, set partial derivatives to 0\n",
    "\n",
    "* $\\frac{\\partial \\ell}{\\partial \\mu} = \\frac{1}{2} \\sum_i 2 \\Sigma^{-1} (x_i - \\mu) = 0$  \n",
    "$\\implies \\sum_i x_i - \\mu = 0$  \n",
    "$\\implies \\sum_i x_i - n \\mu = 0$  \n",
    "$\\implies \\hat{\\mu}_{MLE} = \\frac{1}{n} \\sum X_i$  \n",
    "(note that $X_i$ are vectors, not scalars)\n",
    "\n",
    "* $\\frac{\\partial \\ell}{\\partial \\Sigma} = -\\frac{n}{2} (\\Sigma^{-1})^\\top + \\frac{1}{2} \\sum_i (\\Sigma^{-1})^\\top (x_i - \\mu) (x_i - \\mu)^\\top (\\Sigma^{-1})^\\top = 0$  \n",
    "$\\implies -\\frac{n}{2} I + \\frac{1}{2} \\sum_i (\\Sigma^{-1})^\\top (x_i - \\mu) (x_i - \\mu)^\\top = 0$  \n",
    "$\\implies -\\frac{n}{2} \\Sigma + \\frac{1}{2} \\sum_i (x_i - \\mu) (x_i - \\mu)^{\\top} = 0$  \n",
    "$\\implies \\hat{\\Sigma}_{MLE} = \\frac{1}{n} \\sum_i (X_i - \\hat{\\mu}) (X_i -\\hat{\\mu})^\\top$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lemma**\n",
    "\n",
    "$\\frac{\\partial}{\\partial x} (A B)$  \n",
    "note that $[A B]_{ij} = \\sum_k A_{ik} B_{kj}$  \n",
    "then taking the derivative w.r.t. $x$, we get \n",
    "$\\sum_k \\frac{\\partial A_{ik}}{\\partial x} B_{kj} + \\sum_k A_{ik} \\frac{\\partial B_{kj}}{\\partial x}$  \n",
    "$= A \\frac{\\partial B}{\\partial x} + \\frac{\\partial A}{\\partial x} B$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lemma**\n",
    "\n",
    "let $I = A A^{-1}$  \n",
    "then $0 = \\partial_x I = \\partial_x (A A^{-1}) = A \\partial_x A^{-1} + A^{-1} \\partial_x A$\n",
    "\n",
    "then $\\partial_x A^{-1} = -(A^{-1}) (\\partial_x A) (A^{-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lemma**\n",
    "\n",
    "$\\partial_{A_{ij}} A^{-1} = -A^{-1} \\mathbb{J}_{ij} A^{-1}$\n",
    "\n",
    "this is a matrix for which element $kl$ is -$A^{-1}_{ki} A^{-1}_{jl}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lemma**\n",
    "\n",
    "$\\partial_{A_{ij}} (a^\\top A^{-1} b)$\n",
    "$= -a^\\top A^{-1} \\mathbb{J}_{ij} A^{-1} b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lemma**\n",
    "\n",
    "$-(A^{-1})^\\top a b^\\top (A^{-1})^\\top = -a^\\top A^{-1} \\mathbb{J}_{ij} A^{-1} b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lemma**\n",
    "\n",
    "given\n",
    "\n",
    "* prior $X \\sim \\mathcal{N}(m, S)$\n",
    "* likelihood: $\\propto e^{a^\\top x} e^{-\\frac{1}{2} x^\\top B x}$\n",
    "\n",
    "then\n",
    "\n",
    "* posterior $\\propto e^{(a + S^{-1} m)^\\top x} e^{-\\frac{1}{2} x^\\top (S^{-1} + B) x}$  \n",
    "\n",
    "then\n",
    "\n",
    "* $\\Sigma = (S^{-1} + B)^{-1}$\n",
    "* $\\mu = (S^{-1} + B)^{-1} (S^{-1} m + a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**theorem: linear transformation**\n",
    "\n",
    "let\n",
    "\n",
    "* $x \\sim \\mathcal{N} (\\mu, \\Lambda^{-1}$\n",
    "* $y = Ax + b$\n",
    "\n",
    "then $y \\sim \\mathcal{N} (A \\mu + b, A \\Lambda^{-1} A^\\top)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**theorem: partitioned gaussians**\n",
    "\n",
    "let \n",
    "\n",
    "* $x = \\begin{bmatrix} x_a \\\\ x_b \\end{bmatrix}$\n",
    "* $\\mu = \\begin{bmatrix} \\mu_a \\\\ \\mu_b \\end{bmatrix}$\n",
    "* $\\Lambda = \\begin{bmatrix} \\Lambda_{aa} & \\Lambda_{ab} \\\\ \\Lambda_{ba} & \\Lambda_{bb} \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**theorem: linearly dependent gaussians**\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian linear regression\n",
    "\n",
    "* prior on $w$: $w \\sim \\mathcal{N}(m_0, S_0)$\n",
    "* likelihood: $\\prod_i \\mathcal{N}(t_i \\mid w^\\top \\phi(x_i), \\beta^{-1})$\n",
    "$= \\mathcal{N}(t \\mid \\Phi w, \\beta^{-1} I)$\n",
    "* then posterior $w \\mid t$ is also normally distributed:  \n",
    "$w \\mid t \\sim \\mathcal{N}(m_n, S_n)$\n",
    "    * $S_n = (S_0^{-1} + \\beta \\Phi^\\top \\Phi)^{-1}$\n",
    "    * $m_n = S_n (\\beta \\Phi^\\top t + S_0^{-1} m_0)$\n",
    "* posterior predictive:  \n",
    "$p(t_{n+1} | x, x_{n+1}) \\propto p(t_{n+1} | w, x, x_{n+1}) p(w | x)$\n",
    "    * $w \\mid x \\sim \\mathcal{N}(m_n, S_n)$\n",
    "    * $t_{n+1} \\mid w, x_{n+1} \\sim \\mathcal{N}(w^\\top \\phi(x_{n+1}), \\beta^{-1})$\n",
    "    * therefore $t_{n+1} \\mid x, x_{n+1} \\sim \\mathcal{N}(\\phi(x_{n+1})^\\top m_n, \\beta^{-1} + \\phi(x_{n+1})^\\top S_n \\phi(x_{n+1}))$\n",
    "* typical choice of prior: $w \\sim \\mathcal{N}(0, \\alpha^{-1} I)$\n",
    "    * then $S_n = (\\alpha I + \\beta \\Phi^\\top \\Phi)^{-1}$\n",
    "    * $m_n = (\\alpha I + \\beta \\Phi^\\top \\Phi)^{-1} \\beta \\Phi^\\top t$\n",
    "    $= (\\frac{\\alpha}{\\beta} I + \\Phi^\\top \\Phi)^{-1} \\Phi^\\top t$\n",
    "        * same as regularized linear regression where \n",
    "        $\\lambda = \\alpha / \\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**def: gamma distribution**\n",
    "\n",
    "$\\lambda \\mid a, b \\sim Gamma(a, b)$ iff \n",
    "$f(\\lambda) = \\frac{1}{\\Gamma(a) b^a} \\lambda^{a-1} e^{-b \\lambda}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior for $\\beta$\n",
    "\n",
    "$L = (\\frac{\\beta}{2 \\pi})^{n/2} e^{-\\frac{\\beta}{2} t^\\top t} e^{-\\frac{\\beta}{2} w^\\top \\Phi^\\top \\Phi w} e^{\\beta w^\\top \\Phi^\\top t}$  \n",
    "$\\propto \\beta^{n/2} e^{-(t^\\top t + w^\\top \\Phi^\\top \\Phi w)\\beta}$\n",
    "\n",
    "this suggests that a gamma prior is conjugate\n",
    "\n",
    "if we set prior \n",
    "$p(w, \\beta) = \\mathcal{N}(w| m_0, S_0) \\times g(\\beta | a_0, b_0)$\n",
    "then the posterior for $w$ depends on the distribution of $\\beta$\n",
    "\n",
    "a better prior might be:\n",
    "\n",
    "* $p(w, \\beta) = g(\\beta | a_0, b_0) \\mathcal{N}(m_0, \\beta^{-1} S_0)$\n",
    "* $p(\\beta | a_0, b_0) = g(\\beta | a_0, b_0)$\n",
    "\n",
    "then the posterior is \n",
    "$g(\\beta | a_n, b_n) \\times \\mathcal{N}(w | m_n, \\beta^{-1} S_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(w, \\beta | t, x) \\propto p(\\beta) \\times p(w | \\beta) \\times p(t | w, \\beta, x)$  \n",
    "$\\propto \\beta^{a_0 - 1} e^{-b_0 \\beta} \\times \\beta^{n/2} |S_0 / \\beta|^{-1/2} e^{-\\frac{1}{2} (w - m_0)^\\top (\\beta S_0^{-1}) (w - m_0)} \\times \\beta^{n/2} e^{-\\frac{\\beta}{2} t^\\top t} e^{-\\frac{\\beta}{2} w^\\top \\Phi^\\top \\Phi w} e^{\\beta w^\\top \\Phi^\\top t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after some math, we get the following result:\n",
    "\n",
    "$w \\mid t, x, \\beta \\sim \\mathcal{N}(\\mu, \\Sigma)$  \n",
    "where $\\Sigma = \\beta^{-1} (S_0^{-1} \\Phi^\\top \\Phi)^{-1}$  \n",
    "and $\\mu = (S_0^{-1} \\Phi^\\top \\Phi)^{-1} (\\Phi^\\top + S_0^{-1} m_0)$\n",
    "\n",
    "$\\beta \\mid t, x \\sim Gamma(a_n, b_n)$  \n",
    "where $a_n = a_0 + n/2$  \n",
    "and $b_n = b_0 + \\frac{1}{2} t^\\top t + \\frac{1}{2} m_0^\\top S_0^{-1} m_0 - \\frac{1}{2} m_n^\\top S_n^{-1} m_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection\n",
    "\n",
    "* how should we go about choosing model parameters (e.g., size of search space, priors, etc.)\n",
    "* as our search space increases, the model fits better to the training set\n",
    "* as our search space increases, the less certain we are that we didn't happen to come across a good fit by chance\n",
    "* balancing methods\n",
    "    * structural risk minimium (SRM)\n",
    "    * bayes information criterion (BIC)\n",
    "    * Akaike information criterion (AIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recall $p(\\theta | y) \\propto p(\\theta) p(y | \\theta)$\n",
    "\n",
    "suppose that we say that the prior is uninformative and choose model $\\theta$ s.t. $p(y | \\theta)$ is maximized (second level maximum likelihood or evidence maximization)\n",
    "\n",
    "e.g., for linear regression,  \n",
    "$p(y | \\theta) = p(t | \\alpha, \\beta)$  \n",
    "$= \\int p(w | \\alpha) p(t | \\Phi, w, \\beta) dw$\n",
    "\n",
    "where\n",
    "\n",
    "* $w \\sim \\mathcal{N}(0, \\alpha^{-1} I)$\n",
    "* $p(t | w) \\sim \\mathcal{N}(\\Phi w, \\beta^{-1} I)$\n",
    "\n",
    "so \n",
    "\n",
    "* $w | t \\sim \\mathcal{N}(m_n, S_n)$\n",
    "* $m_n = \\beta S_n \\Phi^\\top t$\n",
    "* $S_n = (\\alpha I + \\beta \\Phi^\\top \\Phi)^{-1}$\n",
    "\n",
    "then we get  \n",
    "$= \\int (\\frac{\\alpha}{2 \\pi})^{d/2} e^{-\\frac{\\alpha}{2} w^\\top w} (\\frac{\\beta}{2 \\pi})^{n/2} e^{-\\frac{\\beta}{2} (t - \\Phi w)^\\top (t - \\Phi w)} dw$  \n",
    "completing the square, we get  \n",
    "$= (\\frac{\\beta}{2 \\pi})^{n/2} (\\frac{\\alpha}{2 \\pi})^{d/2} |S_n|^{1/2} e^{-\\frac{\\beta}{2} |\\Phi m_n - t|^2} e^{-\\frac{\\alpha}{2} |m_n|^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then the log-likelihood (really, log-evidence) is\n",
    "\n",
    "$\\ell = \\frac{n}{2} \\log \\beta + \\frac{\\alpha}{2} \\log \\alpha + \\frac{1}{2} |S_n| - \\frac{\\beta}{2} |\\Phi m_n - t|^2 - \\frac{\\alpha}{2} |m_n|^2 + C$\n",
    "\n",
    "and we can maximize this by taking the derivatives w.r.t. $\\alpha$ and $\\beta$, setting to 0, and solving\n",
    "\n",
    "some linear algebra:\n",
    "\n",
    "* let the eigenvalues of $\\Phi^\\top \\Phi$ be $\\hat{\\lambda}_i$\n",
    "* then the eigenvalues of $\\beta \\Phi^\\top \\Phi$ are $\\lambda_i = \\beta \\hat{\\lambda}_i$\n",
    "* then the eigenvalues of $S_n^{-1} = \\alpha I + \\beta \\Phi^\\top \\Phi$ are $\\lambda_i + \\alpha$\n",
    "* then the eigenvalues of $S_n$ are $(\\lambda_i + \\alpha)^{-1}$\n",
    "* $\\log |S_n| = \\log \\prod \\frac{1}{\\lambda_i + \\alpha} = \\sum \\log \\frac{1}{\\lambda_i + \\alpha}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$0 = \\partial_\\alpha \\ell = \n",
    "(\\partial_{m_n} \\ell)^\\top (\\partial_\\alpha m_n) + \\frac{d}{2 \\alpha} + \\partial_\\alpha (\\frac{1}{2} \\sum \\log \\frac{1}{\\lambda_i + \\alpha}) - \\frac{1}{2} |m_n|^2$  \n",
    "$\\implies \\frac{d}{\\alpha} - \\sum_i \\frac{1}{\\lambda_i + \\alpha} - |m_n|^2 = 0$  \n",
    "$\\implies d - \\sum \\frac{\\alpha}{\\lambda_i + \\alpha} = \\alpha |m_n|^2$  \n",
    "$\\implies \\sum \\frac{\\lambda}{\\lambda_i + \\alpha} = \\alpha |m_n|^2$  \n",
    "$\\implies \\hat{\\alpha} = \\frac{\\gamma}{|m_n|^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\partial_{m_n} \\ell = \\partial_{m_n} \\big(-\\frac{\\beta}{2} (\\Phi m_n - t)^\\top (\\Phi m_n - t) - \\frac{\\alpha}{2} m_n^\\top m_n \\big)$  \n",
    "$= \\partial_{m_n} \\big(-\\frac{\\beta}{2} m_n^\\top \\Phi^\\top \\Phi m_n - \\frac{\\beta}{2} t^\\top t + \\beta m_n^\\top \\Phi^\\top t - \\frac{\\alpha}{2} m_n^\\top m_n \\big)$  \n",
    "$= -\\frac{\\beta}{2} 2 \\Phi^\\top \\Phi m_n + \\beta \\Phi^\\top t - \\frac{\\alpha}{2} 2 m_n$  \n",
    "$= -(\\frac{\\beta} \\Phi^\\top \\Phi + \\alpha I) m_n + \\beta \\Phi^\\top t$  \n",
    "$= -S_n^{-1} m_n + \\beta \\Phi^\\top t$  \n",
    "$= -S_n^{-1} \\beta S_n \\Phi^\\top t + \\beta \\Phi^\\top t$  \n",
    "$= 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$0 = \\partial_{\\beta} \\ell$\n",
    "$= (\\partial_{m_n} \\ell)^\\top \\partial_\\beta m_n + \\frac{n}{2} \\beta^{-1} + \\frac{1}{2} \\partial_\\beta \\log \\prod \\frac{1}{\\lambda_i + \\alpha} - \\frac{1}{2} |\\Phi m_n - t|^2$  \n",
    "\n",
    "$\\partial_\\beta \\log \\prod \\frac{1}{\\lambda_i + \\alpha}$  \n",
    "$= \\partial_\\beta (- \\sum \\log (\\lambda_i + \\alpha))$  \n",
    "$= -\\sum \\frac{1}{\\lambda_i + \\alpha} \\partial_\\beta \\lambda_i$  \n",
    "$= -\\sum \\frac{\\lambda_i}{\\lambda_i + \\alpha} \\beta^{-1}$\n",
    "\n",
    "so the above becomes  \n",
    "$\\frac{n}{\\beta} - \\frac{\\gamma}{\\beta} |\\Phi m_n - t|^2 = 0$  \n",
    "$\\implies \\hat{\\beta}^{-1} = \\frac{1}{n - \\gamma} |\\Phi m_n - t|^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we get \n",
    "\n",
    "* $\\hat{\\alpha} = \\frac{\\gamma}{|m_n|^2}$\n",
    "* $\\hat{\\beta}^{-1} = \\frac{1}{n - \\gamma} |\\Phi m_n - t|^2$\n",
    "* $\\gamma = \\sum_i \\frac{\\lambda_i}{\\lambda_i + \\alpha}$\n",
    "\n",
    "note that:\n",
    "\n",
    "* if $\\lambda_i$'s are large, then the eigenvalues of the variance $S_n$ are close to 0, suggesting that we have high confidence in the results\n",
    "* if $\\lambda_i$'s are small, then all of the eigenvalues of $S_n$ are $\\alpha^{-1}$, so the prior dominates\n",
    "* if $\\alpha$ is small, then $\\gamma \\approx d$, so we get a result similar to linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we get a model selection algorithm:\n",
    "\n",
    "1. initialize $\\alpha$ and $\\beta$\n",
    "2. do until convergence:\n",
    "    1. calculate $m_n$ and $S_n$ based on current values of $\\alpha$ and $\\beta$\n",
    "    2. calculate $\\alpha$ and $\\beta$ based on current values of $m_n$ and $S_n$\n",
    "    \n",
    "this actually won't converge but instead provide a sample for $\\alpha$ and $\\beta$, and we might choose from this sample $\\alpha$ and $\\beta$ that maximizes $\\ell$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
