{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Framework\n",
    "\n",
    "### Concepts\n",
    "\n",
    "* (probabolistic) model\n",
    "* data/sample\n",
    "* likelihood\n",
    "* maximum likelihood\n",
    "* prior\n",
    "* posterior\n",
    "* maximum a posteriori (MAP)\n",
    "* predictive distribution\n",
    "* conjugate priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example: independent coin toss\n",
    "\n",
    "* $P(\\text{heads}) = p$\n",
    "* then $P(HHT) = p \\times p \\times (1 - p)$\n",
    "\n",
    "let \n",
    "\n",
    "$x_i = \\begin{cases}\n",
    "    1 & \\text{if } i^{th} \\text{ toss is heads} \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "consider 3 cases\n",
    "\n",
    "1. 300 heads, 200 tails\n",
    "2. 3 heads, 2 tails\n",
    "3. 5 heads, 0 tails\n",
    "\n",
    "### maximum likelihood principle\n",
    "\n",
    "pick the model (and parameters) that has the highest likelihood given the sample/data\n",
    "\n",
    "$L(\\theta) = P(\\text{data} \\mid \\theta)$  \n",
    "$\\hat{\\theta} = argmax_\\theta P(\\text{data} \\mid \\theta)$\n",
    "\n",
    "let $\\ell(\\theta) = \\log L(\\theta)$. Then $argmax_\\theta L(\\theta) = argmax_{\\theta} \\ell(\\theta)$\n",
    "\n",
    "### back to the coin toss example\n",
    "\n",
    "$L(p) = \\binom{n}{x} p^x (1-p)^{n-x}$  \n",
    "$\\ell(p) = \\log \\binom{n}{x} + x \\log p + (n-x) \\log (1-p)$\n",
    "\n",
    "where $x$ is the number of heads and $n$ is the total number of coin tosses\n",
    "\n",
    "To maximize w.r.t. $p$, take the derivative and set to 0:\n",
    "\n",
    "$0 = \\frac{x}{p} - \\frac{n-x}{1-p}$  \n",
    "$\\implies \\hat{p} = \\frac{x}{n}$\n",
    "\n",
    "alternatively, consider the prior:\n",
    "\n",
    "* $P(p=.5) = .9$\n",
    "* $P(p=.6) = .1$\n",
    "* $P(p \\not\\in \\{.5, .6\\}) = 0$\n",
    "\n",
    "Another prior:\n",
    "\n",
    "* probability density function $f(p) \\propto p^2 (1-p)^2$ when $p \\in (0, 1)$ and 0 otherwise\n",
    "* $f(p) = 30 p^2 (1-p)^2$\n",
    "\n",
    "### prior\n",
    "\n",
    "distribution over models\n",
    "\n",
    "oftentimes we choose a type of model and a family of distributions for the parameters of the model\n",
    "\n",
    "### back to the coin toss example\n",
    "\n",
    "given a prior and no data, we can compute the probability of heads:  \n",
    "$P(H) = \\sum_\\theta P(H | \\theta) P(\\theta)$  \n",
    "\n",
    "for the first prior, we have $.9 \\times .5 + .1 \\times .6 = .51$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### posterior distribution\n",
    "\n",
    "if we have data, then we can update our prior belief\n",
    "\n",
    "$P(\\theta \\mid \\text{data})$\n",
    "\n",
    "To compute, we use ...\n",
    "\n",
    "### Bayes' rule\n",
    "\n",
    "$P(\\theta \\mid x) = \\frac{P(x \\mid \\theta) P(\\theta)}{P(x)}$\n",
    "\n",
    "note that $P(x \\mid \\theta) = L(\\theta)$, the likelihood\n",
    "\n",
    "to compute the denominator:\n",
    "\n",
    "$P(x) = \\sum_\\theta P(x | \\theta) P(\\theta)$\n",
    "\n",
    "oftentimes we can just use $P(\\theta | x) \\propto P(x | \\theta) P(\\theta)$  \n",
    "i.e., posterior $\\propto$ prior $\\times$ likelihood\n",
    "\n",
    "### back to the coin toss example ...\n",
    "\n",
    "using the first prior, we can compute \n",
    "$P(p=.5 | x) = \\frac{.9 \\times .5^x .5^{n-x}}{.9 \\times .5^x .5^{n-x} + .1 \\times .6^x .4^{n-x}}$\n",
    "\n",
    "for the second prior ...  \n",
    "$f(p | x) = \\frac{f(p) p^x (1-p)^{n-x}}{\\int f(p) p^x (1-p)^{n-x} dp}$  \n",
    "\n",
    "we can avoid doing the integral in the denominator by saying ...  \n",
    "$\\propto p^{x+2} (1-p)^{n-x+2}$  \n",
    "and then noting that probabilities must sum up to or integrate to 1\n",
    "\n",
    "### maximum a posteriori principle\n",
    "\n",
    "choose the model that maximizes the posterior $P(\\theta | x)$\n",
    "\n",
    "note that this often doesn't require normalizing the posterior distribution (just need to compute the argmax)\n",
    "\n",
    "### back to the coin toss example ...\n",
    "\n",
    "using the prior $f(p) \\propto p^2 (1-p)^2$, we have  \n",
    "$f(p|x) = p^{x+2} (1-p)^{n-x+2}$  \n",
    "and taking the derivative w.r.t. $p$ and setting to 0, we get  \n",
    "$\\hat{p} = \\frac{x+1}{n+2}$\n",
    "\n",
    "### conjugate prior\n",
    "\n",
    "a prior distribution such that the the posterior distribution is of the same family\n",
    "\n",
    "### back to the coin toss example ...\n",
    "\n",
    "we started with\n",
    "\n",
    "* $x \\mid p \\sim Binomial(n, p)$\n",
    "* $p \\sim Beta(2, 2)$\n",
    "\n",
    "then we get\n",
    "\n",
    "* $p \\mid x \\sim Beta(x + 2, n - x + 2)$\n",
    "\n",
    "#### beta distribution\n",
    "\n",
    "$\\theta \\sim Beta(a, b)$ iff \n",
    "$f(\\theta) = \\frac{\\Gamma(a+b)}{\\Gamma(a) \\Gamma(b)} \\theta^{a-1} (1-\\theta)^{b-1}$\n",
    "\n",
    "$\\theta \\sim Beta(a, b) \\implies$\n",
    "\n",
    "* $E[\\theta] = \\frac{a}{a+b}$\n",
    "* $argmax_\\theta f(\\theta) = \\frac{a-1}{a+b-2}$\n",
    "\n",
    "### predictive distribution\n",
    "\n",
    "the distribution of a new observation given the posterior\n",
    "\n",
    "computed by integrating out the posterior to get an \"aggregate prediction\"\n",
    "\n",
    "$f(\\tilde{x} | x) = \\int_\\Theta f(\\tilde{x} | \\theta, x) f(\\theta | x) d\\theta$\n",
    "\n",
    "where $x$ is the sample and $\\tilde{x}$ is a new observation\n",
    "\n",
    "### back to the coin toss example ...\n",
    "\n",
    "given our data of $x$ heads out of $n$ tosses and prior $Beta(2, 2)$, we have $P(H) = f(\\tilde{x} | x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example: normal distribution\n",
    "\n",
    "given\n",
    "\n",
    "* model $X_i \\stackrel{iid}{\\sim} \\mathcal{N}(\\mu, \\beta)$ where $\\beta$ is the precision ($\\beta = \\sigma^{-2}$)\n",
    "* data $X_1, ..., X_n$\n",
    "\n",
    "then\n",
    "\n",
    "* $L(\\mu, \\beta) = \\prod_i f(x_i | \\mu, \\beta) = (\\frac{\\beta}{2 \\pi})^{n/2} \\exp(-\\frac{\\beta}{2} \\sum_i (x_i - \\mu)^2)$\n",
    "* $\\ell(\\mu, \\beta) = -\\frac{n}{2} \\log 2 \\pi + \\frac{n}{2} \\log \\beta - \\frac{\\beta}{2} \\sum_i (x_i - \\mu)^2$\n",
    "\n",
    "the MLE is found by taking partial derivatives w.r.t. $\\mu$ and $\\beta$, setting them to $0$, and solving\n",
    "\n",
    "* $\\hat{\\mu} = \\frac{\\sum_i X_i}{n}$\n",
    "* $\\hat{\\beta} = \\frac{n}{\\sum_i (X_i - \\hat{\\mu})^2}$\n",
    "\n",
    "note that estimators $\\hat{\\mu}$ and $\\hat{\\beta}$ are *random variables*\n",
    "\n",
    "* $\\hat{\\mu}$ and $\\hat{\\beta}$ are functions of random variables $X_1, ..., X_n$ and so must also be random variables\n",
    "\n",
    "properties of our MLEs:\n",
    "\n",
    "* $E[\\hat{\\mu}] = E[\\frac{\\sum_i X_i}{n}] = \\mu$\n",
    "$\\implies \\hat{\\mu}$ is unbiased\n",
    "* $Var(\\hat{\\mu}) = Var(\\frac{\\sum_i X_i}{n}) = n^{-2} \\sum_i Var(X_i)\n",
    "= \\frac{1}{n \\beta}$\n",
    "\n",
    "* $E[\\hat{\\beta}] = \\frac{n-1}{n} \\frac{1}{\\beta}$\n",
    "$\\implies \\hat{\\beta}$ is biased\n",
    "\n",
    "### example: normal distribution with known variance/precision\n",
    "\n",
    "conjugate prior for $\\mu$: normal\n",
    "\n",
    "* $\\mu \\sim \\mathcal{N}(m, b)$ (again, $b$ is precision)\n",
    "\n",
    "then we get the posterior  \n",
    "$f(\\mu | x) \\propto f(x | \\mu) f(\\mu)$  \n",
    "$\\propto e^{-\\frac{b}{2} \\beta^2} e^{b m \\mu} e{-\\frac{n \\beta}{2} \\mu^2} e^{\\beta \\sum x_i \\mu}$  \n",
    "and we can compute the parameters for $\\mu \\mid x$ by completing the square\n",
    "\n",
    "$\\mu \\mid x \\sim \\mathcal{N}(\\frac{mb + \\beta \\sum x_i}{b + n \\beta}, b + n \\beta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
