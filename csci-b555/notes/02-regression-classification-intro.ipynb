{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notation\n",
    "\n",
    "* **data/design matrix** - $\\Phi \\in \\mathbb{R}^{n \\times p}$\n",
    "    * typically $n > p$\n",
    "* **response vector** - $t \\in \\mathbb{R}^n$\n",
    "* $\\phi(x_i)^\\top$ is the $i^{th}$ row/observation\n",
    "* $\\phi_j(X)$ is the $j^{th}$ column/feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assumptions\n",
    "\n",
    "* there is a **true function** $f(x_i) = w^\\top \\phi(x_i) = \\sum_k w_k \\phi_k(x_i)$ where $w$ is a vector of (unknown) weights/coefficients\n",
    "* we observe noise/uncertainty in the form of $t(x_i) \\stackrel{iid}{\\sim} \\mathcal{N}(f(x_i), \\beta^{-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "likelihood\n",
    "\n",
    "* $L(w, \\beta) = \\prod_i \\mathcal{N}(t_i \\mid w^\\top x_i, \\beta^{-1})$\n",
    "$= (\\frac{\\beta}{2 \\pi})^{n/2} e^{-\\frac{\\beta}{2} \\sum_i (t_i - w^\\top \\phi(x_i))^2}$\n",
    "* $\\ell(w, \\beta) = -\\frac{n}{2} \\log 2 \\pi + \\frac{n}{2} \\log \\beta - \\frac{\\beta}{2} \\sum_i (t_i - w^\\top \\phi(x_i))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to solve for $\\hat{w}$, maximize $\\ell$ w.r.t. $w$ $\\iff$ minimize sum of squares $\\sum_i (t_i - w^\\top \\phi(x_i))^2$  \n",
    "this is equivalent to minimizing $|t - \\Phi w|^2 = (t - \\Phi w)^\\top (t - \\Phi w) = t^\\top t - t^\\top \\Phi w - w^\\top \\Phi^\\top t + w^\\top \\Phi^\\top \\Phi w$  \n",
    "taking the derivative w.r.t. $w$ and setting to 0 yields  \n",
    "$0 = -2 \\Phi^\\top t + 2 \\Phi^\\top \\Phi w$\n",
    "$\\implies \\hat{w} = (\\Phi^\\top \\Phi)^{-1} \\Phi^\\top t$\n",
    "\n",
    "note that $\\Phi^\\top \\Phi$ is invertible iff $n > p$ and the columns of $\\Phi$ are independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "geometric intuition\n",
    "\n",
    "* we want to minimize $|t - \\Phi \\hat{w}|^2$ w.r.t. $\\hat{w}$\n",
    "* defining $\\hat{t} = \\Phi \\hat{w}$ implies $\\hat{t}$ is in the column span of $\\Phi$\n",
    "* then $t - \\hat{t}$ is orthogonal to every column of $\\Phi$\n",
    "* then we have $\\Phi^\\top (t - \\hat{t}) = 0$  \n",
    "$\\implies \\Phi^\\top (t - \\Phi \\hat{w}) = 0$  \n",
    "$\\implies \\hat{w} = (\\Phi^\\top \\Phi)^{-1} \\Phi^\\top t$\n",
    "* $\\Phi (\\Phi^\\top \\Phi)^{-1} \\Phi^\\top$ is the projection matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**regularized linear regression** - minimize $|t - \\Phi w|^2 + \\lambda |w|^2$\n",
    "\n",
    "* $t^\\top t - 2 w^\\top \\Phi^\\top t + w^\\top \\Phi^\\top \\Phi w + \\lambda w^\\top w$\n",
    "* to solve for $w$, take derivative and set to 0\n",
    "* $0 = -2 \\Phi^\\top t + 2 \\Phi^2 \\Phi w + 2 \\lambda w$  \n",
    "$\\implies \\hat{w} = (\\Phi^\\top \\Phi + \\lambda I)^{-1} \\Phi^\\top t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Classification\n",
    "\n",
    "now suppose that $t \\in \\{0, 1\\}^n$  \n",
    "i.e., $t_i \\in \\{0, 1\\}$\n",
    "\n",
    "a plausible model for this might be:  \n",
    "$t_i \\stackrel{indep}{\\sim} Bernoulli \\big(g(\\phi(x_i)^\\top )\\big)$\n",
    "\n",
    "If we want to keep the linear structure, then a plausible way is to say \n",
    "$g(\\cdot) = \\sigma(f_i)$ where $f_i = w^\\top \\phi(x_i)$ as before\n",
    "\n",
    "$\\sigma(y) = \\frac{1}{1 - e^{-y}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to find $\\hat{w}_{MLE}$, we again set $\\ell(w) = 0$ and solve \n",
    "\n",
    "no closed-form solution--solve via numerical optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generative model for continuous features\n",
    "\n",
    "for each example $i$\n",
    "\n",
    "* pick label for example $i$: $t_i$ usin $P(y_i = 1) = p$\n",
    "* if $y_i = 0$ then pick $\\phi(x_i) \\sim P(x_i \\mid y_i = 0)$\n",
    "    * e.g., $\\phi(x_i) \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$\n",
    "* if $y_i = 1$ then pick $\\phi(x_i) \\sim P(x_i \\mid y_i = 1)$\n",
    "    * e.g., $\\phi(x_i) \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)$\n",
    "    \n",
    "model has parameters $\\{p, \\mu_0, \\sigma_0^2, \\mu_1, \\sigma_1^2\\}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generative model for discrete features\n",
    "\n",
    "suppose two features each with three labels (e.g., S, M. L)\n",
    "\n",
    "then there are $3^2 = 9$ possible permutations of the two features\n",
    "\n",
    "$3^2 - 1 = 8$ degrees of freedom\n",
    "\n",
    "to control for the size of the design matrix, assume that the features are conditionally independent given the response (naive bayes)\n",
    "\n",
    "$P(X_i \\mid y_i) = \\prod_{l=1}^L P(X_{il} \\mid y_i)$\n",
    "\n",
    "for each example $i$\n",
    "\n",
    "* pick $y_i \\in \\{1, 2, ..., C\\}$ from $Discrete(\\{p_1, ..., p_{C}\\}$\n",
    "* for $l = 1$ to $L$\n",
    "    * let $j = y_l$\n",
    "    * pick $x_{il}$ from $Discrete(\\{q_{jl}, ..., q_{jlD}\\})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** \n",
    "\n",
    "* let \n",
    "    * $C = 2$, $L = 2$, $D = 3$\n",
    "    * each $X$ can take on values $\\{a, b, c\\}$\n",
    "    * $p_1 = .9$, $p_2 = .1$\n",
    "    * $j = 1$, $l = 1$ $\\implies$ $q = \\{.1, .8, .1\\}$\n",
    "    * $j = 1$, $l = 2$ $\\implies$ $q = \\{.2, .4, .4\\}$\n",
    "    * $j = 2$, $l = 1$ $\\implies$ $q = \\{.3, .4, .3\\}$\n",
    "    * $j = 2$, $l = 2$ $\\implies$ $q = \\{.2, .7, .1\\}$\n",
    "    * $i$ is the observation index (row of $\\Phi$)\n",
    "    * $j$ is the class index\n",
    "    * $l$ is the feature index (column of $\\Phi$)\n",
    "    * $k$ is the feature value index\n",
    "\n",
    "* then\n",
    "    * $P(X_i \\mid t_i = j) = \\prod_l P(X_{il} \\mid t_i = j)$\n",
    "    * let $P(X_{il} = k \\mid t_i = j) = q_{jlk}$, \n",
    "    * for every $j, l$ set of parameters that sum to 1 : $\\forall j, l$, $\\sum_k q_{jlk} = 1$\n",
    "\n",
    "* suppose we observe\n",
    "    * $C=1$ and $X = [a, b, a]$\n",
    "    * $C=1$ and $X = [c, a, c]$\n",
    "    * $C=2$ and $X = [a, a, b]$\n",
    "    * where the possible values are of $\\{a, b, c\\}$\n",
    "\n",
    "* then\n",
    "    * $L = p_1 q_{111} q_{122} q_{131} \\times p_1 q_{113} q_{121} q_{133} \\times p_2 q_{211} q_{221} q_{232}$  \n",
    "    $= \\prod_i \\prod_j \\big( p_j \\prod_l ( \\prod_k q_{jlk}^{x_{ilk}} )\\big)$\n",
    "    * $x_{ilk}$ is a binary value such that $x_{ilk} = 1 \\iff$ feature $l$ has value $k$\n",
    "    * define $t_{ij} = 1 \\iff t_{i} = j$\n",
    "    * $\\ell = \\sum_i \\sum_j \\big( t_{ij} \\log p_j + t_{ij} \\sum_l \\sum_k x_{ilk} \\log q_{jlk} \\big)$  \n",
    "    $= \\sum_i \\sum_j t_{ij} \\log p_j + \\sum_j \\sum_l \\sum_i t_{ij} \\sum_k x_{ilk} \\log q_{jlk}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lagrange multipliers**\n",
    "\n",
    "* want to maximize $f(x)$\n",
    "* constraint: $g(x) = 0$\n",
    "* then we optimize for $\\mathcal{L} = f(x) + \\lambda g(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.**\n",
    "\n",
    "* $X_i \\sim Multinomial(\\{A, B, C\\}, \\{p_1, p_2, p_3\\})$\n",
    "* sample of 10 iid observations: 6 As, 2 Bs, 2 Cs\n",
    "* $L = p_1^6 p_2^2 p_3^2$\n",
    "* $\\ell = 6 \\log p_1 + 2 \\log p_2 + 2 \\log p_3$\n",
    "* constraint $p_1 + p_2 + p_3 = 1$\n",
    "* using lagrange multipliers  \n",
    "$\\mathcal{L} = 6 \\log p_1 + 2 \\log p_2 + 2 \\log p_3 + \\lambda (1 - \\sum p_i)$\n",
    "* taking derivatives w.r.t. each $p_i$ and $\\lambda$ and setting to 0, we get\n",
    "    * $\\lambda = 10$\n",
    "    * $p_1 = 3/5$\n",
    "    * $p_2 = 1/5$\n",
    "    * $p_3 = 1/5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** naive bayes for text\n",
    "\n",
    "* given collection of documents with sentiment labels $\\in \\{0, 1\\}$\n",
    "* $q_{jk}$ is probability of using word $k$ when sentiment is $j$\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
