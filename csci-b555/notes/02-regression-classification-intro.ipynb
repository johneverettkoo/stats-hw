{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notation\n",
    "\n",
    "* **data/design matrix** - $\\Phi \\in \\mathbb{R}^{n \\times p}$\n",
    "    * typically $n > p$\n",
    "* **response vector** - $t \\in \\mathbb{R}^n$\n",
    "* $\\phi(x_i)^\\top$ is the $i^{th}$ row/observation\n",
    "* $\\phi_j(X)$ is the $j^{th}$ column/feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assumptions\n",
    "\n",
    "* there is a **true function** $f(x_i) = w^\\top \\phi(x_i) = \\sum_k w_k \\phi_k(x_i)$ where $w$ is a vector of (unknown) weights/coefficients\n",
    "* we observe noise/uncertainty in the form of $t(x_i) \\stackrel{iid}{\\sim} \\mathcal{N}(f(x_i), \\beta^{-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "likelihood\n",
    "\n",
    "* $L(w, \\beta) = \\prod_i \\mathcal{N}(t_i \\mid w^\\top x_i, \\beta^{-1})$\n",
    "$= (\\frac{\\beta}{2 \\pi})^{n/2} e^{-\\frac{\\beta}{2} \\sum_i (t_i - w^\\top \\phi(x_i))^2}$\n",
    "* $\\ell(w, \\beta) = -\\frac{n}{2} \\log 2 \\pi + \\frac{n}{2} \\log \\beta - \\frac{\\beta}{2} \\sum_i (t_i - w^\\top \\phi(x_i))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to solve for $\\hat{w}$, maximize $\\ell$ w.r.t. $w$ $\\iff$ minimize sum of squares $\\sum_i (t_i - w^\\top \\phi(x_i))^2$  \n",
    "this is equivalent to minimizing $|t - \\Phi w|^2 = (t - \\Phi w)^\\top (t - \\Phi w) = t^\\top t - t^\\top \\Phi w - w^\\top \\Phi^\\top t + w^\\top \\Phi^\\top \\Phi w$  \n",
    "taking the derivative w.r.t. $w$ and setting to 0 yields  \n",
    "$0 = -2 \\Phi^\\top t + 2 \\Phi^\\top \\Phi w$\n",
    "$\\implies \\hat{w} = (\\Phi^\\top \\Phi)^{-1} \\Phi^\\top t$\n",
    "\n",
    "note that $\\Phi^\\top \\Phi$ is invertible iff $n > p$ and the columns of $\\Phi$ are independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "geometric intuition\n",
    "\n",
    "* we want to minimize $|t - \\Phi \\hat{w}|^2$ w.r.t. $\\hat{w}$\n",
    "* defining $\\hat{t} = \\Phi \\hat{w}$ implies $\\hat{t}$ is in the column span of $\\Phi$\n",
    "* then $t - \\hat{t}$ is orthogonal to every column of $\\Phi$\n",
    "* then we have $\\Phi^\\top (t - \\hat{t}) = 0$  \n",
    "$\\implies \\Phi^\\top (t - \\Phi \\hat{w}) = 0$  \n",
    "$\\implies \\hat{w} = (\\Phi^\\top \\Phi)^{-1} \\Phi^\\top t$\n",
    "* $\\Phi (\\Phi^\\top \\Phi)^{-1} \\Phi^\\top$ is the projection matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**regularized linear regression** - minimize $|t - \\Phi w|^2 + \\lambda |w|^2$\n",
    "\n",
    "* $t^\\top t - 2 w^\\top \\Phi^\\top t + w^\\top \\Phi^\\top \\Phi w + \\lambda w^\\top w$\n",
    "* to solve for $w$, take derivative and set to 0\n",
    "* $0 = -2 \\Phi^\\top t + 2 \\Phi^2 \\Phi w + 2 \\lambda w$  \n",
    "$\\implies \\hat{w} = (\\Phi^\\top \\Phi + \\lambda I)^{-1} \\Phi^\\top t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Classification\n",
    "\n",
    "now suppose that $t \\in \\{0, 1\\}^n$  \n",
    "i.e., $t_i \\in \\{0, 1\\}$\n",
    "\n",
    "a plausible model for this might be:  \n",
    "$t_i \\stackrel{indep}{\\sim} Bernoulli \\big(g(\\phi(x_i)^\\top )\\big)$\n",
    "\n",
    "If we want to keep the linear structure, then a plausible way is to say \n",
    "$g(\\cdot) = \\sigma(f_i)$ where $f_i = w^\\top \\phi(x_i)$ as before\n",
    "\n",
    "$\\sigma(y) = \\frac{1}{1 - e^{-y}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to find $\\hat{w}_{MLE}$, we again set $\\ell(w) = 0$ and solve \n",
    "\n",
    "no closed-form solution--solve via numerical optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generative model for continuous features\n",
    "\n",
    "for each example $i$\n",
    "\n",
    "* pick label for example $i$: $t_i$ usin $P(y_i = 1) = p$\n",
    "* if $y_i = 0$ then pick $\\phi(x_i) \\sim P(x_i \\mid y_i = 0)$\n",
    "    * e.g., $\\phi(x_i) \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$\n",
    "* if $y_i = 1$ then pick $\\phi(x_i) \\sim P(x_i \\mid y_i = 1)$\n",
    "    * e.g., $\\phi(x_i) \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)$\n",
    "    \n",
    "model has parameters $\\{p, \\mu_0, \\sigma_0^2, \\mu_1, \\sigma_1^2\\}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generative model for discrete features\n",
    "\n",
    "suppose two features each with three labels (e.g., S, M. L)\n",
    "\n",
    "then there are $3^2 = 9$ possible permutations of the two features\n",
    "\n",
    "$3^2 - 1 = 8$ degrees of freedom\n",
    "\n",
    "to control for the size of the design matrix, assume that the features are conditionally independent given the response (Naive Bayes)\n",
    "\n",
    "$P(X_i \\mid y_i) = \\prod_{l=1}^L P(X_{il} \\mid y_i)$\n",
    "\n",
    "for each example $i$\n",
    "\n",
    "* pick $y_i \\in \\{1, 2, ..., C\\}$ from $Discrete(\\{p_1, ..., p_{C}\\}$\n",
    "* for $l = 1$ to $L$\n",
    "    * let $j = y_l$\n",
    "    * pick $x_{il}$ from $Discrete(\\{q_{jl}, ..., q_{jlD}\\})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.g.** \n",
    "\n",
    "* let \n",
    "    * $C = 2$, $L = 2$, $D = 3$\n",
    "    * each $X$ can take on values $\\{a, b, c\\}$\n",
    "    * $p_1 = .9$, $p_2 = .1$\n",
    "    * $j = 1$, $l = 1$ $\\implies$ $q = \\{.1, .8, .1\\}$\n",
    "    * $j = 1$, $l = 2$ $\\implies$ $q = \\{.2, .4, .4\\}$\n",
    "    * $j = 2$, $l = 1$ $\\implies$ $q = \\{.3, .4, .3\\}$\n",
    "    * $j = 2$, $l = 2$ $\\implies$ $q = \\{.2, .7, .1\\}$\n",
    "    * $i$ is the observation index\n",
    "    * $j$ is the class index\n",
    "    * $l$ is the feature index\n",
    "    * $k$ is the feature value index$\n",
    "\n",
    "* suppose we observe\n",
    "    * $C=1$ and $X = [a, b]$\n",
    "    * $C=1$ and $X = [c, a]$\n",
    "    * $C=2$ and $X = [a, a]$\n",
    "\n",
    "* then\n",
    "    * $L = \\prod_i \\prod_j (p_j \\prod_l \\prod_k q_{jkl}^{x_ilk})^{j_ij}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
