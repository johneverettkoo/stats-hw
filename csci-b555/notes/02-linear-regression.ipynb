{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notation\n",
    "\n",
    "* **data matrix** - $\\Phi \\in \\mathbb{R}^{n \\times p}$\n",
    "* **response vector** - $t \\in \\mathbb{R}^n$\n",
    "* $\\phi(x_i)^\\top$ is the $i^{th}$ row/observation\n",
    "* $\\phi_j(X)$ is the $j^{th}$ column/feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assumptions\n",
    "\n",
    "* there is a **true function** $f(x_i) = w^\\top \\phi(x_i) = \\sum_k w_k \\phi_k(x_i)$ where $w$ is a vector of (unknown) weights/coefficients\n",
    "* we observe noise/uncertainty in the form of $t(x_i) \\stackrel{iid}{\\sim} \\mathcal{N}(f(x_i), \\beta^{-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "likelihood\n",
    "\n",
    "* $L(w, \\beta) = \\prod_i \\mathcal{N}(t_i \\mid w^\\top x_i, \\beta^{-1})$\n",
    "$= (\\frac{\\beta}{2 \\pi})^{n/2} e^{-\\frac{\\beta}{2} \\sum_i (t_i - w^\\top \\phi(x_i))^2}$\n",
    "* $\\ell(w, \\beta) = -\\frac{n}{2} \\log 2 \\pi + \\frac{n}{2} \\log \\beta - \\frac{\\beta}{2} \\sum_i (t_i - w^\\top \\phi(x_i))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to solve for $\\hat{w}$, maximize $\\ell$ w.r.t. $w$ $\\iff$ minimize sum of squares $\\sum_i (t_i - w^\\top \\phi(x_i))^2$  \n",
    "this is equivalent to minimizing $|t - \\Phi w|^2 = (t - \\Phi w)^\\top (t - \\Phi w) = t^\\top t - t^\\top \\Phi w - w^\\top \\Phi^\\top t + w^\\top \\Phi^\\top \\Phi w$  \n",
    "taking the derivative w.r.t. $w$ and setting to 0 yields  \n",
    "$0 = -2 \\Phi^\\top t + 2 \\Phi^\\top \\Phi w$\n",
    "$\\implies \\hat{w} = (\\Phi^\\top \\Phi)^{-1} \\Phi^\\top t$\n",
    "\n",
    "note that $\\Phi^\\top \\Phi$ is invertible iff $n > p$ and the columns of $\\Phi$ are independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "geometric intuition\n",
    "\n",
    "* we want to minimize $|t - \\Phi \\hat{w}|^2$ w.r.t. $\\hat{w}$\n",
    "* defining $\\hat{t} = \\Phi \\hat{w}$ implies $\\hat{t}$ is in the column span of $\\Phi$\n",
    "* then $t - \\hat{t}$ is orthogonal to every column of $\\Phi$\n",
    "* then we have $\\Phi^\\top (t - \\hat{t}) = 0$  \n",
    "$\\implies \\Phi^\\top (t - \\Phi \\hat{w}) = 0$  \n",
    "$\\implies \\hat{w} = (\\Phi^\\top \\Phi)^{-1} \\Phi^\\top t$\n",
    "* $\\Phi (\\Phi^\\top \\Phi)^{-1} \\Phi^\\top$ is the projection matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**regularized linear regression** - minimize $|t - \\Phi w|^2 + \\lambda |w|^2$\n",
    "\n",
    "* $t^\\top t - 2 w^\\top \\Phi^\\top t + w^\\top \\Phi^\\top \\Phi w + \\lambda w^\\top w$\n",
    "* to solve for $w$, take derivative and set to 0\n",
    "* $0 = -2 \\Phi^\\top t + 2 \\Phi^2 \\Phi w + 2 \\lambda w$  \n",
    "$\\implies \\hat{w} = (\\Phi^\\top \\Phi + \\lambda I)^{-1} \\Phi^\\top t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
