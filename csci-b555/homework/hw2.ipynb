{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI-B555\n",
    "\n",
    "## Written Assignment 2\n",
    "\n",
    "#### John Koo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "(3.3 from Bishop)\n",
    "\n",
    "Let $R$ be a matrix such that $R_{ii} = r_i$ and $R_{ij} = 0$ $\\forall i \\neq j$.  \n",
    "Then we can rewrite $\\frac{1}{2} \\sum_n r_n (t_n - w^\\top \\phi(x_n))^2$ \n",
    "$= \\frac{1}{2} (t - \\Phi w)^\\top R (t - \\Phi w)$.\n",
    "\n",
    "As before, we take the derivative w.r.t. $w$ and set to zero to obtain  \n",
    "$0 = -\\Phi^\\top R t + \\Phi^\\top R \\Phi w$.\n",
    "\n",
    "Solving for $w$ yields  \n",
    "$\\hat{w} = (\\Phi^\\top R \\Phi)^{-1} \\Phi^\\top R w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "#### Part 1\n",
    "\n",
    "We have $\\begin{bmatrix} Y_1 \\\\ Y_2 \\end{bmatrix} = \\begin{bmatrix} X_2^2 \\\\ X_1 + 5 X_2 \\end{bmatrix}$\n",
    "\n",
    "If we invert this, we get \n",
    "$\\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix} = \\begin{bmatrix} \n",
    "-5 Y_1^{1/2} + Y_2 \\\\ Y_1^{1/2} \\end{bmatrix}$\n",
    "\n",
    "The Jacobian is\n",
    "$J = \\begin{bmatrix}\n",
    "    \\frac{5}{2} Y_1^{-1/2} & 1 \\\\\n",
    "    \\frac{1}{2} Y_1^{-1/2} & 0\n",
    "\\end{bmatrix}$\n",
    "\n",
    "So $|J| = |0 - \\frac{1}{2} Y_1^{-1/2}| = \\frac{1}{2} Y_1^{-1/2}$\n",
    "\n",
    "Since the support of $X$ has an area of 1, the density of both $X_1$ and $X_2$ is $1$ on the support. So on the support of $Y$, the density is:\n",
    "\n",
    "$f(y_1, y_2) = \\frac{1}{2} y_1^{-1/2}$\n",
    "\n",
    "#### Part 2\n",
    "\n",
    "For the support of $Y$, we can see that since $X_2$ has support $[1, 2]$, $Y_1 = X_2^2$ must have support $[1, 4]$. Since $Y_2 = X_1 - 5 X_2 = X_1 - 5 \\sqrt{Y_1}$ and $X_1$ also has support $[1, 2]$, the support of $Y_2$ is $[1 - 5 \\sqrt{Y_1}, 2 - 5 \\sqrt{Y_2}]$.\n",
    "\n",
    "#### Part 3\n",
    "\n",
    "We need to show that $\\int_{y_1 = 1 - 5 \\sqrt{y_2}}^{2 - 5 \\sqrt{y_2}} \\int_{y_2=1}^4 \\frac{1}{2} y_2^{-1/2} dy_2 dy_1 = 1$.  \n",
    "Since $(2 - 5 \\sqrt{y_2}) - (1 - 5 \\sqrt{y_2}) = 1$, we can ignore the integration w.r.t. $y_1$, so we are left with:  \n",
    "$\\int_1^4 \\frac{1}{2} y_2^{-1/2} dy_2$  \n",
    "$= \\frac{1}{2} (2) y_2^{1/2} \\bigg|_1^4$  \n",
    "$= (\\sqrt{4} - \\sqrt{1}) = (2 - 1) = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "$X \\sim \\mathcal{N}_2(0, I)$  \n",
    "$Y \\mid x_1, x_2 \\sim \\mathcal{N}(3 x_1 + 2 x_2 + 5, 25)$\n",
    "\n",
    "We can rewrite: $Y \\mid \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\sim \\mathcal{N} \\big( \\begin{bmatrix} 3 & 2 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} + 5, (\\frac{1}{50})^{-1} \\big)$\n",
    "\n",
    "Then $X \\mid y$ is normal with variance\n",
    "$\\big( \n",
    "    I + \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} \\frac{1}{50} \\begin{bmatrix} 3 & 2 \\end{bmatrix} \n",
    "\\big)^{-1}$, \n",
    "which, after some matrix arithmetic, reduces to \n",
    "$\\begin{bmatrix} \n",
    "    6/7 & -2/21 \\\\\n",
    "    -2/21 & 59/63\n",
    "\\end{bmatrix}$.\n",
    "\n",
    "Then $mean(X \\mid y) = Var(X | y) \\big( \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} \\frac{1}{50} (y - 5) \\big)$, and when $y = 4$, this reduces to $\\begin{bmatrix} -1/21 \\\\ -2/63 \\end{bmatrix}$.\n",
    "\n",
    "Then \n",
    "$X \\mid y = 4 \\sim \n",
    "\\mathcal{N}_2 \\bigg(\n",
    "    \\begin{bmatrix} -1/21 \\\\ -2/63 \\end{bmatrix}, \n",
    "    \\begin{bmatrix} 6/7 & -2/21 \\\\ -2/21 & 59/63 \\end{bmatrix}\n",
    "\\bigg)$.\n",
    "\n",
    "$X_1$ and $X_2$ are not independent conditional to $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "\n",
    "#### Case 1: $S$ is negative definite\n",
    "\n",
    "$x^\\top S x < 0$ $\\forall x \\neq 0$, so this takes the maximum value at $x = 0$.\n",
    "\n",
    "#### Case 2: $S$ is negative semi-definite\n",
    "\n",
    "$x^\\top S x$ can be at most 0, so we need to find $x$ such that $x^\\top S x = 0$. One such solution (but not necessarily the only solution) is again, $x = 0$.\n",
    "\n",
    "#### Case 3: $S$ is positive semi-definite or positive definite\n",
    "\n",
    "Suppose there exists at least one $x \\neq 0$ such that $x^\\top S x > 0$.\n",
    "\n",
    "Since $x^\\top S x \\neq 0$, $x$ cannot be orthogonal to every eigenvector of $S$ (i.e., it's not orthogonal to some of them). Therefore, $x = \\sum_i c_i v_i + \\sum_j d_j + w_j$ where $c_i, d_j \\in \\mathbb{R}$, $v_i$ are the columns of $V$, and $w_j$ are vectors that are orthogonal to every column of $V$. Since $|x| \\leq 1$, $\\sum c_i^2 + \\sum d_j^2 \\leq 1$.\n",
    "\n",
    "Note that if $v_i$ is an eigenvector of $S$, then $v_i^\\top S v_i = \\sigma_i$, its corresponding eigenvalue. Therefore, $x^\\top S x = \\sum_i \\sigma_i c_i^2$. From here, it's straightforward to see that $c_1 = 1$ and  $c_i, d_j = 0$ $\\forall i \\neq 1$ and $\\forall j$, so $x = v_1$, the first eigenvector of $S$ (the one that corresponds to the largest eigenvalue)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5\n",
    "\n",
    "(3.7 from Bishop)\n",
    "\n",
    "$p(w | t) \\propto p(t | w) p(w)$  \n",
    "$\\propto \\exp \\Big( -\\frac{\\beta}{2} (t - \\Phi w)^\\top (t - \\Phi w) \\Big) \\exp \\Big( -\\frac{1}{2} (w - m_0)^\\top S_0^{-1} (w - m_0) \\Big)$  \n",
    "$\\propto \\exp \\Big( -\\frac{\\beta}{2} w^\\top \\Phi^\\top \\Phi w + \\beta t^\\top \\Phi w - \\frac{1}{2} w^\\top S_0^{-1} w + w^\\top S_0^{-1} m_0 \\Big)$  \n",
    "$= \\exp \\Big( -\\frac{1}{2} w^\\top (\\beta \\Phi^\\top \\Phi + S_0^{-1}) w + (\\beta t^\\top \\Phi + m_0^\\top S_0^{-1}) w \\Big)$\n",
    "\n",
    "Then we can see that the variance is the inverse of twice the quadratic term, and the mean is the inverse of twice the quadratic term multiplied by the linear term:\n",
    "\n",
    "$S_N = \\Big( \\beta \\Phi^\\top \\Phi + S_0^{-1} \\Big)^{-1}$  \n",
    "$m_N = S_N \\Big( \\beta \\Phi^\\top t + S_0^{-1} m_0 \\Big)$\n",
    "\n",
    "(I also had to take the transpose of the linear term using the fact that $x^\\top y = y^\\top x$ to get a column vector for the mean. Also note that since $S_0$ is symmetric, so is $S_0^{-1}$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6\n",
    "\n",
    "(3.11 from Bishop)\n",
    "\n",
    "We are given that $\\sigma_N^2 (x) = \\frac{1}{\\beta} \\phi(x)^\\top S_N \\phi(x)$.  \n",
    "Then $\\sigma_{N+1}^2(x) = \\frac{1}{\\beta} \\phi(x)^\\top S_{N+1} \\phi(x)$\n",
    "\n",
    "The solutions for 3.8 (from the book website, which has solutions for a select subset of exercises) show:  \n",
    "$S_{N+1} = \\big( \\beta \\phi_{N+1} \\phi_{N+1}^\\top + S_N^{-1} \\big)^{-1}$,  \n",
    "which can be rewritten as \n",
    "$= ((\\beta^{1/2} \\phi_{N+1}) (\\beta^{1/2} \\phi_{N+1})^\\top + S_N^{-1})^{-1}$\n",
    "\n",
    "Using the template, we can see that  \n",
    "$S_{N+1} = S_N - \\frac{\\beta S_N \\phi_{N+1} \\phi_{N+1}^\\top S_N}{1 + \\beta \\phi^\\top S_N \\phi}$\n",
    "\n",
    "$S_N$ is a covariance matrix, so it must be positive definite. Furthermore, $\\phi_{N+1} \\phi_{N+1}^\\top$ is a sort of sample covariance matrix (times a factor), so it must be positive semidefinite. Then the numerator, $S_N \\phi_{N+1} \\phi_{N+1}^\\top S_N$ is positive semidefinite. And since $S_N$ is positive definite, $\\phi_{N+1}^\\top S_N \\phi_{N+1}$ is positive, so the denomiator must be positive. So $S_{N+1} + S_N - S'$ where $S'$ is a positive semidefinite matrix.\n",
    "\n",
    "Plugging this into the expression yields  \n",
    "$\\sigma_{N+1}^2 = \\beta^{-1} \\phi(x)^\\top (S_N - S') \\phi(x)$  \n",
    "$= \\big(\\beta^{-1} + \\phi(x)^\\top S_N \\phi(x) \\big) - \\big(\\phi(x)^\\top S' \\phi(x) \\big)$\n",
    "\n",
    "The first term is just $\\sigma_N^2$, and the second term is nonnegative since $S'$ is positive semidefinite. So $\\sigma_{N+1}^2 \\leq \\sigma_N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7\n",
    "\n",
    "(Proof of C.28 from Bishop)\n",
    "\n",
    "Prove $\\frac{\\partial}{\\partial A} \\log |A| = (A^{-1})^\\top$\n",
    "\n",
    "Given $\\frac{\\partial}{\\partial x} \\log |A| = \\text{Tr}(A^{-1} \\frac{\\partial A}{\\partial x})$ and $\\frac{\\partial}{\\partial A} \\text{Tr}(A) = I$\n",
    "\n",
    "We can see that the $ij^{th}$ element of this expression is given by:  \n",
    "$\\frac{\\partial}{\\partial A_{ij}} \\log |A|$\n",
    "$= \\text{Tr}(A^{-1} \\frac{\\partial A}{\\partial A_{ij}})$\n",
    "$= \\text{tr}(\\frac{\\partial A}{\\partial A_{ij}} A^{-1})$\n",
    "\n",
    "$\\frac{\\partial A}{\\partial A_{ij}} = 1^{(ij)}$, the matrix of all zeros except for 1 at the $ij^{th}$ place. Then we can see that $1^{ij} A^{-1}$ is all zero except for the $i^{th}$ row which contains the $j^{th}$ column of $A^{-1}$. Then $\\text{tr}(\\frac{\\partial A}{\\partial A_{ij}} A^{-1}) = A^{-1}_{ji}$\n",
    "\n",
    "So the $ij^{th}$ element of $\\frac{\\partial}{\\partial A} \\log |A|$ is $A^{-1}_{ji}$, i.e., the full expression is $(A^{-1})^\\top$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
