---
title: 'S721 HW7'
author: 'John Koo'
output: pdf_document
# output: html_document
# geometry: "left=1cm,right=1cm,top=1cm,bottom=1.5cm"
urlcolor: blue
header-includes:
- \usepackage{float}
- \usepackage{mathtools}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      comment = NA, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.pos = 'H', 
                      fig.align = 'center', 
                      fig.height = 3, 
                      fig.width = 6, 
                      fig.dpi = 300)
options(xtable.comment = FALSE, 
        xtable.table.placement = 'H')
```

# From text (I)

## 7.11

## Part a

From HW6, we obtained $\hat{\theta}_{MLE} = -\frac{n}{\sum_i^n \log X_i}$.

In class, we noted that if $X_i \sim Beta(\theta, 1)$, 
$-\log X_i \sim Exp(1 / \theta)$. The sum of exponentially distributed random 
variables is gamma distributed, so $-\sum_i^n \log X_i = Gamma(n, 1 / \theta)$.
Therefore, $\hat{\theta}_{MLE} \sim -n \times InvGamma(n, 1 / \theta)$. 

If $Y \sim InvGamma(n, 1 / \theta)$, then the density function is
$f(y \mid n, 1 / \theta) = 
\frac{\theta^n}{\Gamma(n)} (\frac{1}{y})^{n+1} \exp(-\frac{\theta}{y})$.

$E[Y] = 
\int_0^\infty \frac{\theta^n}{\Gamma(n)} y y^{-n - 1} \exp(-\theta / y) dy$  
$= \frac{\theta^n}{\Gamma(n)} \int y^{-n} \exp(-\theta / y) dy$

Letting $u = y^{-1}$, we get $du = -y^{-2} dy$, so the expectation goes to:  
$-\frac{\theta^n}{\Gamma(n)} \int u^{n-2} \exp(-\theta u) du$
$= -\frac{\theta^n}{\Gamma(n)} \frac{\Gamma(n-1)}{\theta^{n-1}} 
\int \frac{\theta^{n-1}}{\Gamma(n-1)} u^{(n-1) - 1} \exp(-\theta u) du$
$= -\frac{\theta}{n-1}$ since the term inside the integral is just the density 
function for a gamma-distributed $U$.

Then $E[\hat{\theta}_{MLE}] = E[-n Y] = -n E[Y] = n \frac{\theta}{n-1}$.

$E[Y^2] = \frac{\theta^n}{\Gamma(n)} \int y^{-n+1} \exp(-\theta / y) dy$.  
Then again letting $u = y^{-1}$, $du = -y^{-2}$, we get:  
$-\frac{\theta^n}{\Gamma(n)} \int u^{n-3} \exp(-\theta u) du$  
$= -\frac{\theta^n}{\Gamma(n)} \frac{\Gamma(n-2)}{\theta^{n-2}} 
\int \frac{\theta^{n-2}}{\Gamma(n-2)} u^{(n-2) - 1} \exp(-\theta u) du$  
$= -\frac{\theta^n}{\Gamma(n)} \frac{\Gamma(n-2)}{\theta^{n-2}}$  
$= -\frac{\theta^2}{(n-1) (n-2)}$.

Then $E[\hat{\theta}_{MLE}^2] = -n^2 E[Y^2] = n^2 \frac{\theta^2}{(n-1) (n-2)}$.

Then $Var(\hat{\theta}_{MLE}) = 
\frac{n^2}{(n-1) (n-2)} \theta^2 - \frac{n^2}{(n-1)^2} \theta^2$  
$= \theta^2 (\frac{n^2 (n-1) - n^2 (n-2)}{(n-1)^2 (n-2)})$  
$= \theta^2 (\frac{n^3 - n^2 - n^3 + 2 n^2}{(n-1)^2 (n-2)})$  
$= \frac{n^2}{(n-1)^2 (n-2)} \theta^2$.

Then $Var(\hat{\theta}_{MLE}) \sim \frac{1}{n} \rightarrow 0$ as $n \to \infty$.

### Part b

We saw from previous homework that $E[X_i] = \frac{\theta}{\theta + 1}$, so 
setting this equal to $\bar{X}$, we get 
$\hat{\theta}_{MOM} = \frac{\bar{X}}{1 - \bar{X}}$.

## 7.12

### Part a

$E[X] = \theta$, so $\hat{\theta}_{MOM} = \bar{X}$.

The likelihood function is 
$L(\theta \mid x) = \prod_i \theta^{x_i} (1 - \theta)^{1 - x_i}$  
$\implies \ell(\theta \mid x) = 
\log \theta \sum_i x_i + \log(1 - \theta) \sum_i (1 - x_i)$  
$\implies \ell'(\theta) = 
\frac{\sum_i x_i}{\theta} - \frac{\sum_i 1 - x_i}{1 - \theta}$, and setting this
to zero, we get  
$\hat{\theta}_{MLE} = \bar{X}$. 

But this is only true if $\bar{X} \leq 1/2$. When $\bar{X} > 1/2$, we note that
$\ell'(\theta) > 0$, so $L$ is an increasing function. We just set it at the
highest possible value, $\hat{\theta}_{MLE} = 1/2$.

### Part b

The MSE can be broken up into bias and variance. The bias in this case is 0, so 
we just have the variance of 
$\bar{X} = Var(X) / n = \frac{\theta (1 - \theta)}{n}$.

### Part c

When $\bar{X} \leq 1/2$, the estimators are equivalent. When $\bar{X} > 1/2$, 
we get an invalid estimate from the method of moments. So the MLE estimator is 
preferred.

## 7.15

### Part a

Writing down the likelihood function, we get  
$L(\mu, \lambda \mid x) = 
\prod_i (\frac{\lambda}{2 \pi x_i^3})^{1/2} 
\exp(-\lambda (x_i - \mu)^2 / (2 \mu^2 x_i))$, 
but it's easier to deal with the log-likelihood, which is  
$\ell(\mu, \lambda \mid x) = 
\frac{n}{2} \log \lambda - \frac{n}{2} \log 2 \pi - 
\frac{3}{2} \sum_i \log x_i - \sum_i \frac{\lambda (x_i - \mu)^2}{2 \mu^2 x_i}$.

When considering $\mu$, we can ignore everything that doesn't have $\mu$ in it, 
so we only need to consider  
$-\frac{\lambda}{2} \sum_i \frac{(x_i - \mu)^2}{\mu^2 x_i}$  
$= -\frac{\lambda}{2} (\sum_i x_i / \mu^2 - 2 n / \mu + \sum_i x_i^{-1})$,  
and if we differentiate w.r.t. $\mu$, we get  
$-\frac{\lambda}{2} (-\mu^{-3} \sum_i x_i + 2 n \mu^{-2})$.  
Setting this to 0, we get  
$\mu = \sum_i x_i / n$, so $\hat{\mu}_{MLE} = \bar{X}$.

For $\lambda$, we can plug in our estimate for $\mu = \bar{x}$, so, considering 
only the terms that depend on $\lambda$, we get  
$\frac{n}{2} \log \lambda - 
\frac{\lambda}{2 \bar{x}^2} \sum_i \frac{(x_i - \bar{x})^2}{x_i}$,  
and if we find the derivative w.r.t. $\lambda$ and set it to zero, we get  
$\frac{n}{2 \lambda} - 
\frac{1}{2 \bar{x}^2} \sum_i \frac{(x_i - \bar{x})^2}{x_i} = 0$  
$\frac{n}{\lambda} =
\frac{1}{\bar{x}^2} (\sum_i x_i - 2 n \bar{x} + \bar{x}^2 \sum_i x_i^{-1})$  
$\frac{n}{\lambda} = 
\frac{1}{\bar{x}^2} (n \bar{x} - 2 n \bar{x} + \bar{x}^2 \sum_i x_i^{-1})$  
$\frac{n}{\lambda} = \frac{1}{\bar{x}} (\bar{x} \sum_i x_i^{-1} - n)$  
$\frac{n}{\lambda} = \sum_i x_i^{-1} - n \bar{x}^{-1}$  
$\frac{n}{\lambda} = \sum_i (x_i^{-1} - \bar{x}^{-1})$  
$\implies \lambda = n \bigg( \sum_i (x_i^{-1} - \bar{x}^{-1}) \bigg)^{-1}$  
So $\hat{\lambda}_{MLE} = n \bigg(\sum_i (X_i^{-1} - \bar{X}^{-1}) \bigg)^{-1}$

## 2.34

Since the density functions for both $X$ and $Y$ are even, any odd-numbered 
moment for either must be 0. 

If $r$ is even, $E[Y] = \frac{1}{6} 3^{r/2} \times 2 = 3^{r/2 - 1}$, so the 
first five moments of $Y$ are 0, 1, 0, 3, 0.

From S620 notes, the moment generating function for the standard normal is 
$M_X(t) = \exp(t^2 / 2)$.  
$M_X'(t) = t \exp(t^2 / 2)$  
$M_X''(t) = \exp(t^2 / 2) + t \exp(t^2 / 2) = (1 + t^2) \exp(t^2 / 2)$  
$M_X'''(t) = 2t \exp(t^2 / 2) + (1 + t^2) t \exp(t^2 / 2) = 
(t^3 + 3t) \exp(t^2 / 2)$
$M_X^{(4)}(t) = (3t + 3) \exp(...) + (t^3 + 3t) t \exp(...) = 
(t^4 + 3t^2 + et + 3) \exp(...)$

Then setting each of these to 0 (and noting that any odd-numbered moment is 0), 
we get 0, 1, 0, 3, 0.

## 3.16

### Part a

$\Gamma(\alpha + 1) = \int_0^\infty x^\alpha e^{-x} dx$  
Letting $u = x^\alpha$ and $dv = e^{-x}$, we get 
$du = \alpha x^{\alpha - 1} dx$ and $v = -e^{-x}$. Then the above integral 
is equal to  
$-x^\alpha e^{-x} |_0^\infty + \alpha \int x^{\alpha - 1} e^{-x} dx$  
$= \alpha \int_0^\infty x^{\alpha - 1} e^{-x} dx$  
$= \alpha \Gamma(\alpha - 1)$

### Part b

$\Gamma(1/2) = \int_0^\infty x^{-1/2} \exp(-x) dx$  
Let $u = x^{1/2}$. Then $du = \frac{1}{2} x^{-1/2} dx$. Then the above integral 
becomes  
$\int_0^\infty 2 \exp(-u^2) du$  
$= 2 \sqrt{\pi} \int_0^\infty\frac{1}{\sqrt{\pi}} \exp(-u^2) du$  
$= 2 \sqrt{\pi} \frac{1}{2}$  
$= \sqrt{\pi}$

## 3.17

$E[X^\nu] = 
\int_0^\infty \frac{1}{\Gamma(\alpha) \beta^\alpha} 
x^{\alpha - 1 + \nu} \exp(-x / \beta) dx$  
$= \frac{\Gamma(\alpha + \nu) \beta^{\alpha + \nu}}{\Gamma(\alpha) \beta^\alpha}
\int \frac{1}{\Gamma(\alpha + \nu) \beta^{\alpha + \nu}} 
x^{(\alpha + \nu) - 1} \exp(-x / \beta) dx$  
$= \frac{\Gamma(\alpha + \nu) \beta^{\alpha + \nu}}
{\Gamma(\alpha) \beta^\alpha}$  
$= \frac{\beta^\nu \Gamma(\alpha + \nu)}{\Gamma(\alpha)}$

## 3.20

### Part a

$E[X] = \frac{2}{\sqrt{2 \pi}} \int_0^\infty x \exp(-x^2 / 2) dx$.  
Let $u = x^2 / 2 \implies du = x dx$. Then the above integral becomes  
$= \sqrt{\frac{2}{\pi}} \int_0^\infty \exp(-u) du$  
$= \sqrt{\frac{2}{\pi}}$.

$E[X^2] = \sqrt{\frac{2}{\pi}} \int_0^\infty x^2 \exp(-x^2 / 2) dx$  
Let $u = x \implies du = dx$ and 
$dv = x \exp(-x^2 / 2) dx \implies v = -\exp(-x^2 / 2)$. Then the above 
becomes  
$\sqrt{\frac{2}{\pi}} (-x \exp(-x^2 / 2) |_0^\infty + 
\int_0^\infty \exp(-x^2 / 2) dx)$  
$= \sqrt{\frac{2}{\pi}} (0 + \sqrt{\frac{\pi}{2}})$  
$= 1$.  
Then $Var(X) = 1 - \frac{2}{\pi}$.

### Part b

The gamma distribution has the density function 
$f(y) = \frac{1}{\Gamma(\alpha) \beta^\alpha} y^{\alpha - 1} \exp(-y / \beta)$.

Let $Y = g(X) = X^2$. Then $X = g^{-1}(Y) = Y^{1/2}$, so 
$\frac{d}{dy} g^{-1} (y) = \frac{1}{2} y^{-1/2}$. Then the density function for 
$Y$ is  
$f_Y(y) = \sqrt{\frac{2}{\pi}} \exp(-y / 2) \frac{1}{2} y^{-1/2}$  
$= \frac{1}{\sqrt{\pi} 2^{1/2}} y^{1/2 - 1} \exp(-y / 2)$.  
Then since $\Gamma(1/2) = \sqrt{\pi}$, we can see that $\alpha = 1/2$ and 
$\beta = 2$.

## 3.24

### Part a

$f_X(x) = \frac{1}{\beta} \exp (-x / \beta)$

If $Y = X^{1 / \gamma}$, then $X = Y^\gamma$ and $X' = \gamma Y^{\gamma - 1}$.

Plugging this into $f_X$, we get  
$\frac{\gamma}{\beta} y^{\gamma - 1} \exp(-y^\gamma / \beta)$

To evaluate 
$\int_0^\infty \frac{\gamma}{\beta} 
y^{\gamma - 1} \exp(-y^\gamma / \beta) dy$,  
we set $x = y^\gamma \implies dx = \gamma y^{\gamma - 1} dy$ to obtain  
$\int_0^\infty \frac{1}{\beta} \exp(-x / \beta) dx = 1$  
since it is just the density function for an exponential random variable.

$E[Y] = \int \frac{\gamma}{\beta} y^\gamma \exp(-y^\gamma / \beta) dy$  
Letting $x = y^\gamma \implies dx = \gamma y^{\gamma - 1} dy$ as before, we 
get  
$\int \frac{1}{\beta} y \exp(-x / \beta) dx$  
$= \frac{1}{\beta} \int x^{1 / \gamma} \exp(-x / \beta) dx$
$= \frac{1}{\beta} \int x^{(1 / \gamma + 1) - 1} \exp(-x / \beta) dx$  
$= \beta^{-1} \Gamma(1 + 1 / \gamma) \beta^{1 + 1 / \gamma}$  
$= \beta^{1 / \gamma} \Gamma(1 + 1 / \gamma)$

$E[Y^2] = \int \frac{\gamma}{\beta} y^{\gamma + 1} \exp(-y^\gamma / \beta) dy$  
Letting $x = y^\gamma \implies dx = \gamma y^{\gamma - 1} dy$ as before, we 
get  
$\beta^{-1} \int y^2 \exp(-x / \beta) dx$  
$= \beta^{-1} \int x^{2 / \gamma} \exp(-x / \beta) dx$  
$= \beta^{-1} \int x^{(1 + 2 / \gamma) - 1} \exp(-x / \beta) dx$  
$= \beta^{-1} \Gamma(1 + 2 / \gamma) \beta^{1 + 2 / \gamma}$  
$= \beta^{2 / \gamma} \Gamma(1 + 2 / \gamma)$  
Then $Var(Y) = \beta^{2 / \gamma} \Gamma(1 + 2 / \gamma) - 
\beta^{2 / \gamma} \Gamma(1 + 1 / \gamma)^2$  
$= \beta^{2 / \gamma} (\Gamma(1 + 2 / \gamma) - \Gamma(1 + 1 / \gamma)^2)$

### Part c

$Y = 1 / X \implies X = 1 / Y \implies X' = -Y^{-2}$

Then $f(y) = f(x(y)) |x'(y)| = 
\frac{1}{\Gamma(a) b^a} y^{1-a} \exp(-\frac{1}{by}) y^{-2}$  
$= \frac{1}{\Gamma(a) b^a} \frac{1}{y^{a+1}} \exp(-\frac{1}{by})$

$\int_0^\infty 
\frac{1}{\Gamma(a) b^a} \frac{1}{y^{a+1}} \exp(-\frac{1}{by}) dy$  
Let $x = y^{-1} \implies dx = -y^{-2} dy$. Then the above becomes  
$\int_\infty^0 \frac{1}{\Gamma(a) b^a} y^{-a - 1} (-y^2) \exp(-x / b) dx$  
$= \int_0^\infty \frac{1}{\Gamma(a) b^a} y^{-a + 1} \exp(-x / b) dx$  
$= \int_0^\infty \frac{1}{\Gamma(a) b^a} x^{a-1} \exp(-x / b) dx$  
$= 1$ since this is just the integral of $Gamma(a, b)$.

$E[Y] = \int_0^\infty \frac{1}{\Gamma(a) b^a} y^{-a} \exp(-\frac{1}{by}) dy$  
Again, let $x = y^{-1} \implies dx = -y^{-2} dy$. Then the above becomes  
$\int_\infty^0 \frac{1}{\Gamma(a) b^a} x^a (-x^{-2}) \exp(-x / b) dx$  
$= \int_0^\infty \frac{1}{\Gamma(a) b^a} x^{a-2} \exp(-x / b) dx$  
$= \frac{\Gamma(a-1) b^{a-1}}{\Gamma(a) b^a} 
\int_0^\infty \frac{1}{\Gamma(a-1) b^{a-1}} x^{(a-1)-1} \exp(-x / b) dx$  
$= \frac{\Gamma(a-1) b^{a-1}}{\Gamma(a) b^a}$  
$= \frac{1}{(a-1) b}$

For $E[Y^2]$, we can extrapolate the results for $E[Y]$ to obtain:  
$E[Y^2] = \int_0^\infty \frac{1}{\Gamma(a) b^a} x^{a-3} \exp(-x / b) dx$  
$= \frac{\Gamma(a-2) b^{a-2}}{\Gamma(a) b^a} 
\int_0^\infty \frac{1}{\Gamma(a-2) b^{a-2}} x^{(a-2)-1} \exp(-x / b) dx$  
$= \frac{\Gamma(a-2) b^{a-2}}{\Gamma(a) b^a}$  
$= \frac{1}{(a-1) (a-2) b^2}$.  
Then $Var(Y) = \frac{1}{(a-1) (a-2) b^2} - \frac{1}{(a-1)^2 b^2}$  
$= \frac{(a-1) - (a-2)}{(a-1)^2 (a-2) b^2}$  
$= \frac{1}{(a-1)^2 (a-2) b^2}$

# Not from text (II)

## Problem 1

$R(\theta, W) = E[(\theta - W)^2]$  
$= E[\theta^2 - 2 \theta W + W^2]$  
$= \theta^2 - 2 \theta E[W] + E[W^2]$ (since $\theta$ is a constant)  
$= \theta^2 - 2 \theta E[W] + Var(W) + E[W]^2$ 
(since $Var(W) = E[W^2] - E[W]^2$)  
$= Var(W) + (\theta^2 - 2 \theta E[W] + E[W]^2)$  
$= Var(W) + (\theta - E[W])^2$  
$= Var(W) + (Bias(W))^2$

## Problem 2

### Part a

$\frac{1}{2 n (n-1)} \sum_i \sum_j (X_i - X_j)^2$  
$= \frac{1}{2 n (n-1)} 
\sum_i \sum_j \big( (X_i - \bar{X}) - (X_j - \bar{X}) \big)^2$  
$= \frac{1}{2 n (n-1)} \bigg(
n \sum_i (X_i - \bar{X})^2 - 
2 \sum_i (X_i - \bar{X}) \sum_j (X_j - \bar{X}) + 
n \sum_j (X_j - \bar{X})
\bigg)$  
Note that the middle term is 0 since $\sum_i (X_i - \bar{X}) = 0$. We also note 
that $\sum_i (X_i - \bar{X}) = (n-1) S^2$. Then we get:  
$\frac{1}{2 n (n-1)} \bigg( n (n-1) S^2 + n (n-1) S^2 \bigg)$  
$= \frac{1}{2 n (n-1)} 2 n (n-1) S^2$  
$= S^2$

### Part b

We can find this by computing $E[(S^2)^2] - (E[S^2])^2$, which means we hae to 
first compute $E[S^2]$ and $E[(S^2)^2]$.

First, we can rewrite:  
$S^2 = \frac{1}{n-1} \sum_i (X_i - \bar{X})^2$  
$= \frac{1}{n-1} \sum_i (X_i - \frac{1}{n} \sum_j X_j)^2$  
$= \frac{1}{n-1} 
\sum_i (X_i^2 - \frac{2}{n} X_i \sum_j X_j + \frac{1}{n^2} (\sum_j X_j)^2)$  
$= \frac{1}{n-1} 
(\sum_i X_i^2 - \frac{2}{n} \sum_i X_i \sum_j X_j + 
n \frac{1}{n^2} (\sum_j X_j)^2)$  
$= \frac{1}{n-1} (\sum_i X_i^2 - \frac{1}{n} (\sum_i X_i)^2)$  
$= \frac{1}{n (n-1)} (n \sum_i X_i - (\sum_i X_i)^2)$

We will, w.l.o.g., take $\theta_1 = E[X_i] = 0$.

Then $E[S^2] = \frac{1}{n (n-1)} (n \sum_i E[X_i^2] - E[(\sum_i X_i)^2])$  
$= \frac{1}{n (n-1)} (n^2 \theta_2 - n \theta_2)$  
$= \frac{1}{n-1} (n \theta_2 - \theta_2)$  
$= \theta_2$

Then $E[(S^2)^2] = 
\frac{1}{n^2 (n-1)^2} E[(n \sum_i X_i^2 - (\sum_i X_i)^2)^2]$  
$= \frac{1}{n^2 (n-1)^2} 
E[n^2 (\sum_i X_i^2)^2 - 2 n (\sum_i X_i^2) (\sum_i X_i)^2 + (\sum_i X_i)^4]$  
which we can break down into three components inside the expectation.

First, $E[(\sum_i X_i^2)^2] = E[(X_1^2 + \cdots + X_n^2)^2]$  
$= E[\sum_i X_i^4 + \sum_{i \neq j} X_i^2 X_j^2]$  
$= \sum_i E[X_i^4] + \sum_{i \neq j} E[X_i^2] E[X_j^2]$ (since this is an iid  sample)  
$= n \theta_4 + n (n-1) \theta_2^2$ (since there are $n$ possible $i$'s and then 
$n-1$ possible $j$'s since $i \neq j$).

Second, $E[(\sum_i X_i^2) (\sum_i X_i)^2]$
$= E[(X_1^2 + \cdots + X_n^2) 
(X_1^2 + \cdots + X_n^2 + \sum_{i \neq j} X_i X_j)]$  
$= E[(\sum_i X_i^2)^2] + E[(\sum_i X_i^2) (\sum_{i \neq j} X_i X_j)]$  
The second term is zero since $E[X_i X_j] = E[X_i] E[X_j]$, and we set 
$\theta_1 = 0$. Then we are left with  
$E[(\sum_i X_i^2)^2] = n \theta_4 + n (n-1) \theta_2^2$ as in the previous part.

Third, $E[(\sum_i X_i)^4] = E[(X_1 + \cdots + X_n)^4]$  
$= E[\sum_i X_i^4 + 3 \sum_{i \neq j} X_i^2 X_j^2]$ (noting that each pair of 
$X_i$ and $X_j$ are independent so we can separate out the expectation, and 
$E[X_i] = 0$)  
$= n \theta_4 + 3 n (n-1) \theta_2^2$

Putting it all together, we get:  
$\frac{n^2 (n \theta_4 + n (n-1) \theta_2^2) - 2n (n \theta_4 + 
n (n-1) \theta_2^2) + n \theta_4 + 3n (n-1) \theta_2^2}{n^2 (n-1)^2}$  
$= \frac{n^3 \theta_4 + n^4 \theta_2^2 - n^3 \theta_2^2 - 2n^2 \theta_4 - 
2n^3 \theta_2^2 + 2n \theta_2^2 + n \theta_4 + 3n^2 \theta_2^2 - 
3n \theta_2^2}{n^2 (n-1)^2}$  
$= \frac{(n^3 - 2n^2 + n) \theta_4 + (n^4 - 3n^3 + 5n^2 - n) \theta_2^2}
{n^2 (n-1)^2}$  
$= \frac{n (n-1)^2 \theta_4 + n (n-1) (n^2 - 2n + 3) \theta_2^2}{n^2 (n-1)^2}$  
$= \frac{(n-1) \theta_4 + (n^2 - 2n + 3) \theta_2^2}{n (n-1)}$

Then $E[(S^2)^2] - (E[S^2])^2$  
$= \frac{(n-1) \theta_4 + (n^2 - 2n + 3) \theta_2^2}{n (n-1)} - \theta_2^2$  
$= \frac{(n-1) \theta_4 + (n^2 - 2n + 3) \theta_2^2 - (n^2 - n) \theta_2^2}
{n (n-1)}$  
$= \frac{(n-1) \theta_4 - (n-3) \theta_2^2}{n (n-1)}$  
$= \frac{1}{n} (\theta_4 - \frac{n-3}{n-1} \theta_2^2)$

### Part c

$Cov(\bar{X}, S^2) = \frac{1}{2n^2 (n-1)} 
E[\sum_i X_i \sum_j \sum_k (X_j - X_k)^2]$  
$= \frac{1}{2n^2 (n-1)} E[\sum_i X_i \sum_{j \neq k} (X_j - X_k)^2]$  
$= \frac{1}{2n^2 (n-1)} E[\sum_{\substack{i \\ j \neq k}} (
X_i X_j^2 - 2 X_i X_j X_k + X_i X_k^2)]$  
Note that the middle term, $2 E[X_i X_j X_k]$, is zero since $E[X_j] = 0$ and we 
force $X_j \neq X_k$. Similarly, the first and third terms are nonzero only when 
$X_i = X_j$ and $X_i = X_k$ respectively. So we are left with:  
$\frac{1}{2n^2 (n-1)} \sum_{j \neq k} (E[X_j^3] + E[X_k^3])$
$= \frac{2 n (n-1)}{2 n^2 (n-1)} E[X_i^3]$ (since there are $n (n-1)$ nonzero 
terms in each sum, and there are two sums)  
$= \frac{1}{n} \theta_3$

This is nonzero when the third moment is nonzero.

## Problem 3

### Part 1

$W_1 = \bar{X}$

Then we have:  
$E[\frac{(\theta - \bar{X})^2}{1 + \theta^2}]$  
$= \frac{1}{1 + \theta^2} E[(\theta - \bar{X})^2]$  
$= \frac{1}{1 + \theta^2} (\theta^2 - 2 \theta E[\bar{X}] + E[\bar{X}^2])$
$= \frac{1}{1 + \theta^2} (\theta^2 - 2 \theta^2 + Var(\bar{X}) + \theta^2)$  
$= \frac{Var(\bar{X})}{1 + \theta^2}$  
$= \frac{Var(X)}{n (1 + \theta^2)}$  
$= \frac{\theta}{n (1 + \theta^2)}$

### Part 2

$W_2 = \frac{\sum_i X_i + \sqrt{n/2}}{n + \sqrt{n/2}}$

Then we have:  
$(1 + \theta^2)^{-1} (n + \sqrt{n/2})^{-2} 
E[(n + \sqrt{n/2}) \theta - n \bar{X} + \sqrt{n/2})^2]$  
The part inside the expectation is, if we expand:  
$n^2 \theta^2 + \theta (n/2) + n^2 \bar{X}^2 + n/2 + 2 n \theta^2 \sqrt{n/2} - 
2 n^2 \theta \bar{X} + 2 n \theta \sqrt{n/2} - 2 \theta \sqrt{n/2} n \bar{X} + 
2 \theta (n/2) - 2 n \bar{X} \sqrt{n/2}$  
And taking the expectation of this (noting that 
$E[\bar{X}^2] = Var(\bar{X}) + \theta^2 = \theta / 2 + \theta^2$), we get:  
$n^2 \theta^2 + \theta^2 (n/2) + n^2 (\theta / n) + n^2 \theta^2 + n/2 + 
2 n \theta^2 \sqrt{n/2} - 2 n^2 \theta^2 + 2 n \theta \sqrt{n/2} - 
2 \theta^2 \sqrt{n/2} n + 2 \theta (n/2) - 2 n \theta \sqrt{n/2}$  
$= (n^2 + n/2 + n^2 + 2 n \sqrt{n/2} - 2 n^2 - 2 \sqrt{n/2} n) \theta^2 + 
(n + 2 n \sqrt{n/2} + n - 2 n \sqrt{n/2}) \theta + n/2$  
$= (n/2) \theta^2 + n/2$

Then, adding in the coefficients, we get:  
$(1 + \theta^2)^{-1} (n + \sqrt{n/2})^{-2} (n/2) (\theta^2 + 1)$  
$= \frac{n/2}{(n + \sqrt{n/2})^2}$  
$= \frac{1}{(\sqrt{2/n} n + \sqrt{2/n} \sqrt{n/2})^2}$  
$= \frac{1}{(1 + \sqrt{2n})^2}$

### Part 3

$W_3 = 1$

This is straightforward. The risk is:  
$\frac{(\theta - 1)^2}{\theta^2 + 1}$

### Part 4

$W_4 = S_n^2$

First, we note that $E[S^2] = \theta$, $E[(\theta - S^2)^2] = Var(S^2)$.

From a previous problem, we saw that 
$Var(S^2) = \frac{1}{n} (\theta_4 - \frac{n-3}{n-1} \theta_2^2)$ 
$= \frac{1}{n} (\theta_4 - \frac{n-3}{n-1} \theta^2)$ (since 
$\theta_2 = \theta$).

From S620 notes, $\theta_4 = \theta (1 + 3 \theta)$, so plugging this in, we 
get:  
$\frac{1}{n} (\theta (1 + 3 \theta) - \frac{n-3}{n-1} \theta^2)$  
$= \frac{1}{n} (\theta + 
\frac{\theta^2}{n-1} (3n \theta^2 - 3 \theta^2 - n \theta^2 + 3 \theta^2))$  
$= \frac{1}{n} (\theta + \frac{2 n \theta^2}{n-1})$  
$= \frac{2 \theta^2}{n-1} + \frac{\theta}{n}$

Then the risk becomes:  
$\frac{1}{\theta^2 + 1} (\frac{2 \theta^2}{n-1} + \frac{\theta}{n})$

### Plot

```{r}
library(ggplot2)
theme_set(theme_bw())

# sample size (arbitrary?)
n <- 5

# values of theta
theta <- seq(0, 5, .1)

# risk values
r1 <- theta / (n * (1 + theta ** 2))
r2 <- (1 + sqrt(2 * n)) ** -2
r3 <- (theta - 1) ** 2 / (theta ** 2 + 1)
r4 <- (theta ** 2 + 1) ** -1 * (2 * theta ** 2 / (n - 1) + theta / n)

ggplot() + 
  geom_line(aes(x = theta, y = r1, colour = 'w1')) + 
  geom_line(aes(x = theta, y = r2, colour = 'w2')) + 
  geom_line(aes(x = theta, y = r3, colour = 'w3')) + 
  geom_line(aes(x = theta, y = r4, colour = 'w4')) + 
  scale_colour_discrete(labels = c(expression(W[1]), 
                                   expression(W[2]), 
                                   expression(W[3]), 
                                   expression(W[4]))) + 
  labs(x = expression(theta), y = expression(R(theta, W)), colour = NULL) + 
  scale_colour_brewer(palette = 'Set1')
```

The first estimator has minima at 0 and infinity.  
The second estimator is flat, so it doesn't have a preference for any estimate.  
The third estimator is minimized at $\theta = 1$. Note that this one does 
not depend on $n$ (or any type of sample).  
The fourth estimator is minimized at 0. 

# Not from text (III)

## Problem 1

From class, we know that for a simple random variable $X$, 

$$E[X] = \sum_i^m x_i P(A_i)$$

If $E[X] \leq E[Y]$, then $E[X] - E[Y] \leq 0$. And again from class, we know:

$$E[X] - E[Y] = E[X - Y]$$

and

$$E[X - Y] = \sum_i^m \sum_j^n (x_i - y_j) P(A_i \cap B_j)$$

Each $P(A_i\cap B_j \geq 0$ since $P$ is a probability measure.

Since $X \leq Y$, each $x_i - y_j \leq 0$.

Therefore, this is a sum of negative numbers, which must be negative.

$$\sum_i^m \sum_j^n (x_i - y_j) P(A_i \cap B_j) \leq 0$$

So $E[X] - E[Y] \leq 0 \implies E[X] \leq E[Y]$.

## Problem 2

We can write:

$$E[X Y] = \sum_i^m \sum_j^n x_i y_j P(A_i \cap B_j)$$

Since $X$ and $Y$ are independent, each $A_i$ and $B_j$ are independent. 
Therefore, $P(A_i \cap B_j) = P(A_i) P(B_j)$, so the above becomes:

$$E[X Y] = \sum_i^m \sum_j^n x_i j_y P(A_i) P(B_j)$$
$$= \sum_i^m x_i P(A_i) \sum_j^n P(B_j)$$
$$= E[X] E[Y]$$