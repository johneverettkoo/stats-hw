---
title: "STAT-S620"
subtitle: 'Assignment 7'
author: "John Koo"
output: pdf_document
# output: html_document
urlcolor: blue
header-includes:
- \usepackage{float}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      comment = NA, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.pos = 'H', 
                      fig.align = 'center', 
                      fig.height = 5, 
                      fig.width = 5)
options(xtable.comment = FALSE, 
        xtable.table.placement = 'H')
```

# 4.4.7

$\psi(t) = \frac{1}{4} (3 e^t + e^{-t})$

$\psi'(t) = \frac{1}{4} (3 e^t - e^{-t})$

$\psi''(t) = \frac{1}{4} (3 et + e^{-t})$

Then $\mu = \mu_1 = \psi'(0) = 2/4 = \boxed{1/2}$.

Since $\psi''(t) = \psi(t)$, we know that $\mu_2 = \psi''(0) = \psi(0) = 1$. 
Therefore, $var(X) = 1 - 1/4 = \boxed{3/4}$.

# 4.4.11

For the pmf of a discrete $X$, the mgf is $\psi(t) = \sum_x P(X = x) e^{t x}$. 
Then $f(x)$ has support $\{1, 4, 8\}$, and 
$\boxed{f(1) = 1/5, f(4) = 2/5, f(8) = 2/5}$ (and 0 otherwise).

# 4.4.12

Using the same idea as exercise 4.4.11, we can say $X$ is discrete and 
$\boxed{f(0) = 2/3, f(1) = 1/6, f(-1) = 1/6}$ (and 0 otherwise).

# 4.6.13

## Part a

$var(X + Y) = var(X) + var(Y) + 2 cov(X, Y)$. We know that 
$cov(X, Y) = (-1/6)(3)(2) = -1$. Then $var(X + Y) = 9 + 4 - 2 = \boxed{11}$.

## Part b

$var(X - 3Y + 4) = var(X) + 9 var(Y) + (2)(1)(-3) cov(X, Y) = \boxed{51}$.

# 4.7.7

$f_X(x) = \int_0^1 (x + y) dy = xy + \frac{y^2}{2} |_{y=0}^1 = x + 1/2$. Then 
$g(y|x) = \frac{x + y}{x + 1/2}$ (for $y \in [0, 1]$ and 0 otherwise).

$E[Y|X] = \int_0^1 \frac{xy + y^2}{x + 1/2} dy =$ 
$\boxed{\frac{x/2 + 1/3}{x + 1/2}}$.

$E[Y^2|X] = \int_0^1 \frac{xy^2 + y^3}{x + 1/2} dy = \frac{x/3 + 1/4}{x + 1/2}$. 

Then $var(Y|X) = \frac{x/3 + 1/4}{x + 1/2} - \big(\frac{x/2 + 1/3}{x + 1/2}\big)^2$
$= \boxed{\frac{1}{12} - \frac{1}{36 (2x + 1)^2}}$ (via WolframAlpha).

# mgf of the normal distribution

## Part 1

By definition:

$$\psi(t) = \int e^{tx} \frac{1}{\sqrt{2 \pi \sigma^2}} 
e^{-\frac{(x - \mu)^2}{2 \sigma^2}} dx$$

Substituting $z = \frac{x - \mu}{\sigma}$, we get that $x = \mu + \sigma z$ and 
$dz = \frac{1}{\sqrt{\sigma^2}} dx$, so we are left with:

$$\psi(t) = \int e^{t(\mu + \sigma z)} \frac{1}{\sqrt{2 \pi}} e^{-z^2 / 2} dz$$
$$= \int \frac{1}{\sqrt{2 \pi}} e^{-z^2 / 2 + \sigma t z + \mu t} dz$$
$$= \int \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2} (z^2 - 2 \sigma t z - 2\mu t)} dz$$
$$= e^{\mu t + \frac{\sigma^2 t^2}{2}} \int \frac{1}{\sqrt{2 \pi}} 
e^{-\frac{1}{2}(z - \sigma t)^2} dz$$
$$= e^{\mu t + \frac{\sigma^2 t^2}{2}}$$

## Part 2

$\psi'(t) = (\mu + \sigma^2 t) \psi(t)$

Then $E[X] = \psi'(0) = \mu$

## Part 3

$\psi''(t) = (\mu + \sigma^2 t)^2 \psi(t) + \sigma^2 \psi(t)$

Then $E[X^2] = \psi''(0) = \mu^2 + \sigma^2$, and 
$var(X) = E[X^2] - E[X]^2 = \mu^2 + \sigma^2 - \mu^2 = \sigma^2$.

## Part 4

We know that the mgf of $Z$ is $\psi_Z(t) = \psi_X(t) \psi_Y(t)$ 
$= e^{\mu_1 t + \frac{\sigma_1^2 t^2}{2}} \times 
e^{\mu_2 t + \frac{\sigma_2^2 t^2}{2}}$ 
$= e^{(\mu_1 + \mu_2) t + \frac{(\sigma_1^2 + \sigma_2^2) t^2}{2}}$. 

# Not from text

## Part 1

If, for when $n=1$, $r=1/2$, then the possible values for $R$ when $n=2$ are 
$1/2 * 1/2 = 1/4$ and $1/2 * 2 = 1$. On the other hand, if when $n=1$, $r=2$, 
then the possible values for $R$ when $n=2$ are $2 * 1/2 = 1$ and $2 * 2 = 4$.

For a given $n$ and $i$, $r = 2^{2i - n}$.

## Part 2

We can see this is a binomial distribution. $P(R = r) = \binom{n}{i} 2^{-n}$.

## Part 3

$\psi(t) = E[e^{tr}] = \sum_r e^{tr} \binom{n}{i} 2^{-n}$ where $r = 2^{2i - n}$. 

## Part 4

$E[R] = \sum_{i=1}^n r \binom{n}{i} 2^{-n}$  
$= \sum_i^n \binom{n}{i} 2^{2i - 2n}$  
$= \sum_i^n \binom{n}{i} 4^{i-n}$  
$= \frac{1}{4^n} \sum_i^n \binom{n}{i} 4^i$  
$= \frac{1}{4^n} (4 + 1)^n = (\frac{5}{4})^n$

## Part 5

Since $E[R] = (\frac{5}{4})^n$ and $C = cR$, $E[C] = c (\frac{5}{4})^n$.